{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"unibox","text":"<p>A unified interface for seamless file operations across local, S3, and Hugging Face ecosystems.</p> <p><code>unibox</code> simplifies loading, saving, and exploring data\u2014whether it's a local CSV, an S3-hosted image, or an entire Hugging Face dataset. With a single API, you can handle diverse file types and storage backends effortlessly.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install unibox\n</code></pre> <p>Or with <code>uv</code>:</p> <pre><code>uv tool install unibox\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Load anything, anywhere:</p> <pre><code>import unibox as ub\n\n# Local parquet file\ndf = ub.loads(\"data/sample.parquet\")\n\n# S3-hosted text file\nlines = ub.loads(\"s3://my-bucket/notes.txt\")\n\n# Hugging Face dataset\ndataset = ub.loads(\"hf://user/repo\")\n</code></pre> <p>Save with ease:</p> <pre><code>ub.saves(df, \"s3://my-bucket/processed.parquet\")\nub.saves(dataset, \"hf://my-org/new-dataset\")\n</code></pre> <p>List files or peek at data:</p> <pre><code># List all JPGs in an S3 folder\nimages = ub.ls(\"s3://bucket/images\", exts=[\".jpg\"])\n\n# Preview a dataset\nub.peeks(dataset)\n</code></pre>"},{"location":"#why-unibox","title":"Why unibox?","text":"<ul> <li>Versatile: Handles CSVs, images, datasets, and more\u2014locally or remotely.</li> <li>Simple: One function call to load or save, no matter the source.</li> <li>Transformative: From quick data peeks to concurrent downloads, it scales with your needs.</li> </ul> <p>Explore the full power in our documentation.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>Love unibox? Join us! Check out CONTRIBUTING.md to get started.</p> <p>Extra dev notes: see README_dev.md.</p>"},{"location":"#next-steps","title":"Next steps","text":"<ul> <li>Get up and running in minutes. \u2192 Quickstart</li> <li>Learn S3, Hugging Face, and supported formats. \u2192 Guides</li> <li>Notebook helpers and image tools. \u2192 Utilities</li> <li>Task\u2011oriented snippets for common workflows. \u2192 Recipes</li> </ul>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#0120-2026-01-10","title":"0.12.0 - 2026-01-10","text":"<p>Compare with 0.11.2</p>"},{"location":"changelog/#features","title":"Features","text":"<ul> <li>Enhance dataset summary generation with additional profiling options and improved memory usage calculations (6f31bdf by trojblue).</li> <li>US-001 - Parse and normalize hf:// URIs (3db5b51 by Ubuntu).</li> </ul>"},{"location":"changelog/#0112-2026-01-07","title":"0.11.2 - 2026-01-07","text":"<p>Compare with 0.11.1</p>"},{"location":"changelog/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>incorrect huggingface memory table generation (28383e1 by trojblue).</li> <li>docs adding correct format (6c13d50 by trojblue).</li> <li>mkdocs missing components (5e0c111 by trojblue).</li> </ul>"},{"location":"changelog/#0111-2026-01-06","title":"0.11.1 - 2026-01-06","text":"<p>Compare with 0.11.0</p>"},{"location":"changelog/#bug-fixes_1","title":"Bug Fixes","text":"<ul> <li>update release dependencies and fix mkdocs configuration (563f2b0 by trojblue).</li> </ul>"},{"location":"changelog/#0110-2026-01-06","title":"0.11.0 - 2026-01-06","text":"<p>Compare with 0.10.0</p>"},{"location":"changelog/#features_1","title":"Features","text":"<ul> <li>add JSON-like input conversion to DataFrame in HFDatasetLoader (7e5b810 by trojblue).</li> </ul>"},{"location":"changelog/#0100-2025-10-31","title":"0.10.0 - 2025-10-31","text":"<p>Compare with 0.9.0</p>"},{"location":"changelog/#features_2","title":"Features","text":"<ul> <li>adding plotting utils from dataproc4 (36f990e by yada).</li> </ul>"},{"location":"changelog/#bug-fixes_2","title":"Bug Fixes","text":"<ul> <li>prevent writing logs to user-unwritable locations (fab3dc3 by yada).</li> <li>image load / save logic (a50781b by yada).</li> <li>change README update error handling to non-fatal warning (4ac4edc by yada).</li> <li>local mklink path resolving for hugginface data (1846cfc by yada).</li> </ul>"},{"location":"changelog/#code-refactoring","title":"Code Refactoring","text":"<ul> <li>update pyproject.toml for dynamic versioning and improve import order in multiple files (3601958 by yada).</li> </ul>"},{"location":"changelog/#090-2025-07-29","title":"0.9.0 - 2025-07-29","text":"<p>Compare with 0.8.3</p>"},{"location":"changelog/#features_3","title":"Features","text":"<ul> <li>adding http loader (e6525a5 by yada).</li> <li>adding credenial manager (f55c63d by yada).</li> </ul>"},{"location":"changelog/#bug-fixes_3","title":"Bug Fixes","text":"<ul> <li>cli error (fffc2b6 by yada).</li> <li>case sensitivity for ls (a2fd4de by yada).</li> <li>adding pre-flight check for existing index_level_0 (5a01a7f by yada).</li> <li>robust handling for dataset card generation (33a9d72 by yada).</li> <li>adding robust handling for df utils (85e61e5 by yada).</li> <li>removed imports (41313bd by yada).</li> </ul>"},{"location":"changelog/#code-refactoring_1","title":"Code Refactoring","text":"<ul> <li>removing old implementatinos of df (585c9fc by yada).</li> </ul>"},{"location":"changelog/#083-2025-04-14","title":"0.8.3 - 2025-04-14","text":"<p>Compare with 0.8.2</p>"},{"location":"changelog/#bug-fixes_4","title":"Bug Fixes","text":"<ul> <li>adding presigns() to init (b4764e6 by yada).</li> <li>adding more robust readme handling (61d0cd4 by yada).</li> <li>openai generation broken (5e3cece by yada).</li> </ul>"},{"location":"changelog/#082-2025-02-22","title":"0.8.2 - 2025-02-22","text":"<p>Compare with 0.8.1</p>"},{"location":"changelog/#bug-fixes_5","title":"Bug Fixes","text":"<ul> <li>blocking var breaks mac compatability (cb38b48 by yada).</li> </ul>"},{"location":"changelog/#081-2025-02-22","title":"0.8.1 - 2025-02-22","text":"<p>Compare with 0.8.0</p>"},{"location":"changelog/#080-2025-02-22","title":"0.8.0 - 2025-02-22","text":"<p>Compare with 0.7.0</p>"},{"location":"changelog/#features_4","title":"Features","text":"<ul> <li>adding dataset-card generation tool (a410353 by yada).</li> </ul>"},{"location":"changelog/#bug-fixes_6","title":"Bug Fixes","text":"<ul> <li>modifying file protect config (b26b632 by yada).</li> <li>local or s3 backends: prevent overwriting important system files (39867dc by yada).</li> <li>s3 backend: variable unbound when specifying path (7e97559 by yada).</li> <li>ub.ls(): working on both huggingface model repo and datasets (70b070e by yada).</li> <li>huggingface dataset: using proper saves() to work (and update) huggingface repo (7d382af by yada).</li> <li>huggingface dataset: shortcircuit to download directly instead of double save (c0ef5e0 by yada).</li> <li>broken s3 loading | broken: huggingface loading after backend downloads (36aaae2 by yada).</li> <li>hf backends loading | broken: s3 loading (fbf3240 by yada).</li> <li>use correct testing script (1984a08 by yada).</li> <li>missing credentials / dependencies for  test cases (672330a by yada).</li> </ul>"},{"location":"changelog/#code-refactoring_2","title":"Code Refactoring","text":"<ul> <li>allow loading from huggingface datasets as a file (aa3dc50 by yada).</li> <li>let loader handle huggingface loads (400f7e1 by yada).</li> </ul>"},{"location":"changelog/#070-2025-02-20","title":"0.7.0 - 2025-02-20","text":"<p>Compare with 0.6.0</p>"},{"location":"changelog/#features_5","title":"Features","text":"<ul> <li>adding dataset split control when using ub.saves(\"hf://org/dataset_repo\") (20735f9 by yada).</li> <li>adding toml / yaml loaders (3955e49 by yada).</li> </ul>"},{"location":"changelog/#bug-fixes_7","title":"Bug Fixes","text":"<ul> <li>missing type checks after update (f001f15 by yada).</li> <li>s3 backend: missing arguments for ls (7c1d8e5 by yada).</li> <li>incorrect huggingface data upload behavior when using datasets (bc5d773 by yada).</li> </ul>"},{"location":"changelog/#060-2025-02-13","title":"0.6.0 - 2025-02-13","text":"<p>Compare with 0.5.2</p>"},{"location":"changelog/#features_6","title":"Features","text":"<ul> <li>adding improved huggingface dataset (with hfapi / hf datasets mixed backend) (96486d2 by yada).</li> </ul>"},{"location":"changelog/#052-2025-02-05","title":"0.5.2 - 2025-02-05","text":""},{"location":"changelog/#features_7","title":"Features","text":"<ul> <li>Add LLM API utility (b0bc83d by openhands-agent)</li> </ul>"},{"location":"changelog/#bug-fixes_8","title":"Bug Fixes","text":"<ul> <li>HuggingFace URI loading incorrectly (ef8c9757f95 by openhands-agent)</li> <li>gallery() to handle None values and display images properly (6f2cb39 by openhands-agent)</li> </ul> <p>Compare with 0.5.1</p>"},{"location":"changelog/#051-2025-01-28","title":"0.5.1 - 2025-01-28","text":"<p>Compare with 0.5.0</p>"},{"location":"changelog/#050-2025-01-04","title":"0.5.0 - 2025-01-04","text":"<p>Compare with v0.4.13</p>"},{"location":"changelog/#features_8","title":"Features","text":"<ul> <li>adding back ub.peeks() support (ac76e3a by yada).</li> <li>adding proper colorized logger (9e0d758 by yada).</li> <li>adding basic huggingface upload tools (f412a18 by yada).</li> <li>adding huggingface storage backend (4c93076 by yada).</li> <li>adding basic test suite and txt loader (bc16177 by yada).</li> <li>adding basic working loader and tests (aa65789 by yada).</li> <li>adding skeleton loader classes (a1e299f by yada).</li> </ul>"},{"location":"changelog/#bug-fixes_9","title":"Bug Fixes","text":"<ul> <li>adding colorama dep (1a9dbe4 by trojblue).</li> <li>missing init in code (2799f19 by yada).</li> <li>huggingface uploading an datasets object; s3 incorrect uri passed in (e5bb8d2 by yada).</li> <li>adding colorlog dependency (a746230 by yada).</li> <li>adding datasets dependency (071e5cc by yada).</li> <li>color control characters getting written to logs (ecf1781 by yada).</li> <li>image_loader: properly handling image loaders (cf5e535 by yada).</li> <li>double write penalty at ub.saves() (ec07b9b by yada).</li> <li>adding convert to rgb when using gallery (0e5826d by yada).</li> </ul>"},{"location":"changelog/#code-refactoring_3","title":"Code Refactoring","text":"<ul> <li>remoivng old files (5e59240 by yada).</li> <li>adding project template (fd95d45 by yada).</li> </ul>"},{"location":"changelog/#v0413-2024-11-17","title":"v0.4.13 - 2024-11-17","text":"<p>Compare with v0.4.12</p>"},{"location":"changelog/#features_9","title":"Features","text":"<ul> <li>adding ub.label_gallery() tool for data labelling (0fdac23 by yada).</li> </ul>"},{"location":"changelog/#v0412-2024-09-30","title":"v0.4.12 - 2024-09-30","text":"<p>Compare with v0.4.11</p>"},{"location":"changelog/#features_10","title":"Features","text":"<ul> <li>allowing human-readable date in presigns() expiration (1381b9a by yada).</li> </ul>"},{"location":"changelog/#v0411-2024-09-30","title":"v0.4.11 - 2024-09-30","text":"<p>Compare with v0.4.10</p>"},{"location":"changelog/#features_11","title":"Features","text":"<ul> <li>s3_client: adding generate_presigned_uri function; removing unused code (ccfcbc8 by yada).</li> </ul>"},{"location":"changelog/#v0410-2024-07-22","title":"v0.4.10 - 2024-07-22","text":"<p>Compare with v0.4.9</p>"},{"location":"changelog/#bug-fixes_10","title":"Bug Fixes","text":"<ul> <li>further ipython import fix (faac26f by yada).</li> </ul>"},{"location":"changelog/#v049-2024-07-22","title":"v0.4.9 - 2024-07-22","text":"<p>Compare with v0.4.8</p>"},{"location":"changelog/#bug-fixes_11","title":"Bug Fixes","text":"<ul> <li>missing ipython dependency when calling ub.peeks() (989b471 by yada).</li> </ul>"},{"location":"changelog/#v048-2024-07-18","title":"v0.4.8 - 2024-07-18","text":"<p>Compare with v0.4.7</p>"},{"location":"changelog/#bug-fixes_12","title":"Bug Fixes","text":"<ul> <li>concurrent_loads: fixing load order (74ac31f by yada).</li> </ul>"},{"location":"changelog/#v047-2024-07-18","title":"v0.4.7 - 2024-07-18","text":"<p>Compare with v0.4.6</p>"},{"location":"changelog/#features_12","title":"Features","text":"<ul> <li>ub.gallery(): adding notebook gallery (4f577a3 by yada).</li> <li>adding ub.ls() wrapper for shorter ub.traverses() (e991c05 by yada).</li> <li>uni_peeker: adding peek_df functionality (ed78393 by yada).</li> <li>adding concurrent_loads() function (c3201d3 by yada).</li> </ul>"},{"location":"changelog/#bug-fixes_13","title":"Bug Fixes","text":"<ul> <li>ub.loads: tempfile naming error on windows (1d454d5 by yada).</li> </ul>"},{"location":"changelog/#v046-2024-06-28","title":"v0.4.6 - 2024-06-28","text":"<p>Compare with v0.4.5</p>"},{"location":"changelog/#bug-fixes_14","title":"Bug Fixes","text":"<ul> <li>UniSaver: replace NaN with null if saving to dict or jsonl (5babfb8 by yada).</li> </ul>"},{"location":"changelog/#v045-2024-06-28","title":"v0.4.5 - 2024-06-28","text":"<p>Compare with v0.4.4</p>"},{"location":"changelog/#bug-fixes_15","title":"Bug Fixes","text":"<ul> <li>adding graceful handling for errors when a line is unbale to be read (0965249 by yada).</li> </ul>"},{"location":"changelog/#v044-2024-06-14","title":"v0.4.4 - 2024-06-14","text":"<p>Compare with v0.4.3</p>"},{"location":"changelog/#bug-fixes_16","title":"Bug Fixes","text":"<ul> <li>ub.saves(); a bug where ub.saves(list[str]) won't correctly save (3849e63 by yada).</li> </ul>"},{"location":"changelog/#v043-2024-06-13","title":"v0.4.3 - 2024-06-13","text":"<p>Compare with v0.4.2</p>"},{"location":"changelog/#features_13","title":"Features","text":"<ul> <li>add ability to save various formatted image files as png file (8142695 by yada).</li> </ul>"},{"location":"changelog/#bug-fixes_17","title":"Bug Fixes","text":"<ul> <li>resolved bug that prevents loading files from url (c9cb709 by yada).</li> </ul>"},{"location":"changelog/#v042-2024-05-30","title":"v0.4.2 - 2024-05-30","text":"<p>Compare with v0.4.1</p>"},{"location":"changelog/#features_14","title":"Features","text":"<ul> <li>extend include_extensions at ub.traverses() to take more than extensions (593b69c by yada).</li> </ul>"},{"location":"changelog/#v041-2024-05-18","title":"v0.4.1 - 2024-05-18","text":"<p>Compare with v0.4.0</p>"},{"location":"changelog/#bug-fixes_18","title":"Bug Fixes","text":"<ul> <li>ub.traverses(): traversing a s3 directory will not return the dir itself (f0d85db by yada).</li> </ul>"},{"location":"changelog/#v040-2024-05-08","title":"v0.4.0 - 2024-05-08","text":"<p>Compare with v0.3.20</p>"},{"location":"changelog/#bug-fixes_19","title":"Bug Fixes","text":"<ul> <li>resizer hangs when handling large number of images to be resized (9627894 by yada).</li> </ul>"},{"location":"changelog/#v0320-2024-03-13","title":"v0.3.20 - 2024-03-13","text":"<p>Compare with v0.3.19</p>"},{"location":"changelog/#features_15","title":"Features","text":"<ul> <li>adding load feather support on uniloader (fee98ff by yada).</li> </ul>"},{"location":"changelog/#v0319-2024-03-09","title":"v0.3.19 - 2024-03-09","text":"<p>Compare with v0.3.18</p>"},{"location":"changelog/#bug-fixes_20","title":"Bug Fixes","text":"<ul> <li>incorrect behavior signature on ub.loads() on s3 uri (ca00b40 by yada).</li> </ul>"},{"location":"changelog/#v0318-2024-03-06","title":"v0.3.18 - 2024-03-06","text":"<p>Compare with v0.3.17</p>"},{"location":"changelog/#v0317-2024-03-06","title":"v0.3.17 - 2024-03-06","text":"<p>Compare with v0.3.16</p>"},{"location":"changelog/#bug-fixes_21","title":"Bug Fixes","text":"<ul> <li>unclosed image at uniresizer (b554630 by yada).</li> </ul>"},{"location":"changelog/#v0316-2024-03-05","title":"v0.3.16 - 2024-03-05","text":"<p>Compare with v0.3.15</p>"},{"location":"changelog/#bug-fixes_22","title":"Bug Fixes","text":"<ul> <li>adding back max_workers in uni_resizer (7c533f6 by yada).</li> </ul>"},{"location":"changelog/#v0315-2024-02-12","title":"v0.3.15 - 2024-02-12","text":"<p>Compare with v0.3.14</p>"},{"location":"changelog/#features_16","title":"Features","text":"<ul> <li>adding debug_print argument to unibox.traverses() (66d1c02 by yada).</li> </ul>"},{"location":"changelog/#v0314-2023-12-18","title":"v0.3.14 - 2023-12-18","text":"<p>Compare with v0.3.13</p>"},{"location":"changelog/#bug-fixes_23","title":"Bug Fixes","text":"<ul> <li>using unibox.loads() with s3 (2ba1a57 by yada).</li> </ul>"},{"location":"changelog/#v0313-2023-12-14","title":"v0.3.13 - 2023-12-14","text":"<p>Compare with v0.3.12</p>"},{"location":"changelog/#bug-fixes_24","title":"Bug Fixes","text":"<ul> <li>include ipykernel version to avoid tqdm issues (a081b70 by yada).</li> <li>traverses() with folder: incomplete s3 uri (3b4d25b by yada).</li> <li>adding unit for traverse s3 (c7d00d7 by yada).</li> </ul>"},{"location":"changelog/#v0312-2023-12-12","title":"v0.3.12 - 2023-12-12","text":"<p>Compare with v0.3.11</p>"},{"location":"changelog/#bug-fixes_25","title":"Bug Fixes","text":"<ul> <li>traverses(s3): allowing traverses() to return dir info (0af5fdb by yada).</li> </ul>"},{"location":"changelog/#v0311-2023-12-11","title":"v0.3.11 - 2023-12-11","text":"<p>Compare with v0.3.10</p>"},{"location":"changelog/#features_17","title":"Features","text":"<ul> <li>unibox.peeks(): adding list peek support &amp; proper command use (c346f99 by yada).</li> </ul>"},{"location":"changelog/#v0310-2023-12-11","title":"v0.3.10 - 2023-12-11","text":"<p>Compare with v0.3.9</p>"},{"location":"changelog/#features_18","title":"Features","text":"<ul> <li>adding support for s3 dir in unibox.traverses() (b90a97a by yada).</li> <li>adding unipeeker and unibox.peeks() method for previewing data (7b7a7cd by yada).</li> <li>adding traverse() in s3 client (3d786b3 by yada).</li> </ul>"},{"location":"changelog/#v039-2023-12-10","title":"v0.3.9 - 2023-12-10","text":"<p>Compare with v0.3.8</p>"},{"location":"changelog/#bug-fixes_26","title":"Bug Fixes","text":"<ul> <li>unibox.loads(): add ability to properly load files from url (32c5593 by yada).</li> </ul>"},{"location":"changelog/#v038-2023-11-21","title":"v0.3.8 - 2023-11-21","text":"<p>Compare with v0.3.6</p>"},{"location":"changelog/#features_19","title":"Features","text":"<ul> <li>support saving string as a txt file (5748e96 by yada).</li> <li>adding url support for unibox.loads() (9cc351b by yada).</li> </ul>"},{"location":"changelog/#v036-2023-11-12","title":"v0.3.6 - 2023-11-12","text":"<p>Compare with v0.3.5</p>"},{"location":"changelog/#features_20","title":"Features","text":"<ul> <li>support s3 uri in unibox.saves() (7e0db63 by yada).</li> <li>support s3 uri in unibox.loads() (7850ed4 by yada).</li> </ul>"},{"location":"changelog/#bug-fixes_27","title":"Bug Fixes","text":"<ul> <li>incorrect filename when using unibox.saves() on s3 (27a4121 by yada).</li> </ul>"},{"location":"changelog/#v035-2023-11-02","title":"v0.3.5 - 2023-11-02","text":"<p>Compare with v0.3.4</p>"},{"location":"changelog/#features_21","title":"Features","text":"<ul> <li>s3client | bump version to 0.3.5 (a0db95a by yada).</li> </ul>"},{"location":"changelog/#v034-2023-11-01","title":"v0.3.4 - 2023-11-01","text":"<p>Compare with v0.3.3</p>"},{"location":"changelog/#bug-fixes_28","title":"Bug Fixes","text":"<ul> <li>merges at init.py (ca658ed by yada).</li> </ul>"},{"location":"changelog/#v033-2023-11-01","title":"v0.3.3 - 2023-11-01","text":"<p>Compare with v0.3.2</p>"},{"location":"changelog/#v032-2023-11-01","title":"v0.3.2 - 2023-11-01","text":"<p>Compare with v0.3.1</p>"},{"location":"changelog/#features_22","title":"Features","text":"<ul> <li>UniMerger: unibox.merges(data1, data2) (d156e29 by yada).</li> </ul>"},{"location":"changelog/#v031-2023-09-17","title":"v0.3.1 - 2023-09-17","text":"<p>Compare with v0.3.0</p>"},{"location":"changelog/#v030-2023-09-03","title":"v0.3.0 - 2023-09-03","text":"<p>Compare with v0.2.14</p>"},{"location":"changelog/#features_23","title":"Features","text":"<ul> <li>image resizer: adding debug prints (bcb55df by yada).</li> <li>image resizer: adding ability to skip existing images (c745646 by yada).</li> <li>adding resizer-next (2ee732c by yada).</li> </ul>"},{"location":"changelog/#bug-fixes_29","title":"Bug Fixes","text":"<ul> <li>find existing images and remove them from jobs list (4ba16d9 by yada).</li> <li>adding lower() to suffix before doing checks (cc17d5c by yada).</li> </ul>"},{"location":"changelog/#v0214-2023-09-02","title":"v0.2.14 - 2023-09-02","text":"<p>Compare with v0.2.13</p>"},{"location":"changelog/#features_24","title":"Features","text":"<ul> <li>adding traverses() method for unibox (fe997a2 by yada).</li> </ul>"},{"location":"changelog/#v0213-2023-08-26","title":"v0.2.13 - 2023-08-26","text":"<p>Compare with v0.2.12</p>"},{"location":"changelog/#v0212-2023-08-23","title":"v0.2.12 - 2023-08-23","text":"<p>Compare with v0.2.11</p>"},{"location":"changelog/#features_25","title":"Features","text":"<ul> <li>adding wip file renameer (148280c by yada).</li> </ul>"},{"location":"changelog/#v0211-2023-08-21","title":"v0.2.11 - 2023-08-21","text":"<p>Compare with v0.2.10</p>"},{"location":"changelog/#bug-fixes_30","title":"Bug Fixes","text":"<ul> <li>typing alias issue in python38 (6179f3e by yada).</li> </ul>"},{"location":"changelog/#v0210-2023-08-20","title":"v0.2.10 - 2023-08-20","text":"<p>Compare with v0.2.9</p>"},{"location":"changelog/#features_26","title":"Features","text":"<ul> <li>reducing minimum  python dependency from 3.10 to 3.8 (bcf96e6 by yada).</li> </ul>"},{"location":"changelog/#v029-2023-08-19","title":"v0.2.9 - 2023-08-19","text":"<p>Compare with v0.2.8</p>"},{"location":"changelog/#v028-2023-08-18","title":"v0.2.8 - 2023-08-18","text":"<p>Compare with v0.2.7</p>"},{"location":"changelog/#v027-2023-08-17","title":"v0.2.7 - 2023-08-17","text":"<p>Compare with v0.2.6</p>"},{"location":"changelog/#v026-2023-08-15","title":"v0.2.6 - 2023-08-15","text":"<p>Compare with v0.2.5</p>"},{"location":"changelog/#v025-2023-08-15","title":"v0.2.5 - 2023-08-15","text":"<p>Compare with v0.2.3</p>"},{"location":"changelog/#features_27","title":"Features","text":"<ul> <li>updating UniTraverser for stateful calls and filepath store (0e385e9 by yada).</li> <li>adding UniTraverser class: code that traverses trhough directory (2e6ebff by yada).</li> </ul>"},{"location":"changelog/#v023-2023-08-14","title":"v0.2.3 - 2023-08-14","text":"<p>Compare with v0.2.2</p>"},{"location":"changelog/#features_28","title":"Features","text":"<ul> <li>remove pandas / pyarrow dep version (fe83df4 by yada).</li> </ul>"},{"location":"changelog/#v022-2023-08-13","title":"v0.2.2 - 2023-08-13","text":"<p>Compare with v0.1.4.3</p>"},{"location":"changelog/#features_29","title":"Features","text":"<ul> <li>adding unisaver (62d2eb2 by yada).</li> <li>adding UniSaver and unibox.saves() method; bump version number to 0.2.0 (4eec7b2 by yada).</li> </ul>"},{"location":"changelog/#v0143-2023-07-16","title":"v0.1.4.3 - 2023-07-16","text":"<p>Compare with v0.1.4</p>"},{"location":"changelog/#features_30","title":"Features","text":"<ul> <li>updating loads() for jsonl files (ccf54f3 by yada).</li> </ul>"},{"location":"changelog/#v014-2023-07-14","title":"v0.1.4 - 2023-07-14","text":"<p>Compare with v0.1.3.5</p>"},{"location":"changelog/#features_31","title":"Features","text":"<ul> <li>optimizing UniLoader class for csv &amp; parquet (383215f by yada).</li> </ul>"},{"location":"changelog/#v0135-2023-07-14","title":"v0.1.3.5 - 2023-07-14","text":"<p>Compare with v0.1.3.4</p>"},{"location":"changelog/#features_32","title":"Features","text":"<ul> <li>adding file mover; update image resizer (4d46099 by yada).</li> </ul>"},{"location":"changelog/#v0134-2023-07-10","title":"v0.1.3.4 - 2023-07-10","text":"<p>Compare with v0.1.3.3</p>"},{"location":"changelog/#features_33","title":"Features","text":"<ul> <li>using ProcessPool instead of ThreadPool; before: 80it/s -&gt; now: 105it/s (0180ad1 by yada).</li> </ul>"},{"location":"changelog/#v0133-2023-07-10","title":"v0.1.3.3 - 2023-07-10","text":"<p>Compare with v0.1.3.2</p>"},{"location":"changelog/#bug-fixes_31","title":"Bug Fixes","text":"<ul> <li>missing _resize (4d5fa90 by yada).</li> </ul>"},{"location":"changelog/#v0132-2023-07-10","title":"v0.1.3.2 - 2023-07-10","text":"<p>Compare with v0.1.3</p>"},{"location":"changelog/#features_34","title":"Features","text":"<ul> <li>updating version number (3c25239 by yada).</li> </ul>"},{"location":"changelog/#bug-fixes_32","title":"Bug Fixes","text":"<ul> <li>not resizing image when min_size &gt; actual size (85acf5d by yada).</li> </ul>"},{"location":"changelog/#v013-2023-07-10","title":"v0.1.3 - 2023-07-10","text":"<p>Compare with 0.1.21</p>"},{"location":"changelog/#features_35","title":"Features","text":"<ul> <li>updating cli &amp; click requirement version (cd324bd by yada).</li> <li>adding image resizer; refactor dir (8586022 by yada).</li> </ul>"},{"location":"changelog/#0121-2023-07-06","title":"0.1.21 - 2023-07-06","text":"<p>Compare with v0.1.2.1</p>"},{"location":"changelog/#v0121-2023-07-06","title":"v0.1.2.1 - 2023-07-06","text":"<p>Compare with v0.1.2</p>"},{"location":"changelog/#v012-2023-07-06","title":"v0.1.2 - 2023-07-06","text":"<p>Compare with v0.1</p>"},{"location":"changelog/#bug-fixes_33","title":"Bug Fixes","text":"<ul> <li>update version number (d5dc53a by yada).</li> </ul>"},{"location":"changelog/#v01-2023-07-06","title":"v0.1 - 2023-07-06","text":"<p>Compare with first commit</p>"},{"location":"changelog/#features_36","title":"Features","text":"<ul> <li>adding pipeline logger &amp; loader (56043e4 by yada).</li> <li>adding basic functionality (42d83cb by yada).</li> </ul>"},{"location":"changelog/#v050-2025-01-04","title":"v0.5.0 - 2025-01-04","text":"<p>Compare with v0.4.13</p>"},{"location":"changelog/#features_37","title":"Features","text":"<ul> <li>adding proper colorized logger (9e0d758 by yada).</li> <li>adding basic huggingface upload tools (f412a18 by yada).</li> <li>adding huggingface storage backend (4c93076 by yada).</li> <li>adding basic test suite and txt loader (bc16177 by yada).</li> <li>adding basic working loader and tests (aa65789 by yada).</li> <li>adding skeleton loader classes (a1e299f by yada).</li> </ul>"},{"location":"changelog/#bug-fixes_34","title":"Bug Fixes","text":"<ul> <li>huggingface uploading an datasets object; s3 incorrect uri passed in (e5bb8d2 by yada).</li> <li>adding colorlog dependency (a746230 by yada).</li> <li>adding datasets dependency (071e5cc by yada).</li> <li>color control characters getting written to logs (ecf1781 by yada).</li> <li>image_loader: properly handling image loaders (cf5e535 by yada).</li> <li>double write penalty at ub.saves() (ec07b9b by yada).</li> <li>adding convert to rgb when using gallery (0e5826d by yada).</li> </ul>"},{"location":"changelog/#code-refactoring_4","title":"Code Refactoring","text":"<ul> <li>remoivng old files (5e59240 by yada).</li> <li>adding project template (fd95d45 by yada).</li> </ul>"},{"location":"changelog/#v0413-2024-11-17_1","title":"v0.4.13 - 2024-11-17","text":"<p>Compare with v0.4.12</p>"},{"location":"changelog/#features_38","title":"Features","text":"<ul> <li>adding ub.label_gallery() tool for data labelling (0fdac23 by yada).</li> </ul>"},{"location":"changelog/#v0412-2024-09-30_1","title":"v0.4.12 - 2024-09-30","text":"<p>Compare with v0.4.11</p>"},{"location":"changelog/#features_39","title":"Features","text":"<ul> <li>allowing human-readable date in presigns() expiration (1381b9a by yada).</li> </ul>"},{"location":"changelog/#v0411-2024-09-30_1","title":"v0.4.11 - 2024-09-30","text":"<p>Compare with v0.4.10</p>"},{"location":"changelog/#features_40","title":"Features","text":"<ul> <li>s3_client: adding generate_presigned_uri function; removing unused code (ccfcbc8 by yada).</li> </ul>"},{"location":"changelog/#v0410-2024-07-22_1","title":"v0.4.10 - 2024-07-22","text":"<p>Compare with v0.4.9</p>"},{"location":"changelog/#bug-fixes_35","title":"Bug Fixes","text":"<ul> <li>further ipython import fix (faac26f by yada).</li> </ul>"},{"location":"changelog/#v049-2024-07-22_1","title":"v0.4.9 - 2024-07-22","text":"<p>Compare with v0.4.8</p>"},{"location":"changelog/#bug-fixes_36","title":"Bug Fixes","text":"<ul> <li>missing ipython dependency when calling ub.peeks() (989b471 by yada).</li> </ul>"},{"location":"changelog/#v048-2024-07-18_1","title":"v0.4.8 - 2024-07-18","text":"<p>Compare with v0.4.7</p>"},{"location":"changelog/#bug-fixes_37","title":"Bug Fixes","text":"<ul> <li>concurrent_loads: fixing load order (74ac31f by yada).</li> </ul>"},{"location":"changelog/#v047-2024-07-18_1","title":"v0.4.7 - 2024-07-18","text":"<p>Compare with v0.4.6</p>"},{"location":"changelog/#features_41","title":"Features","text":"<ul> <li>ub.gallery(): adding notebook gallery (4f577a3 by yada).</li> <li>adding ub.ls() wrapper for shorter ub.traverses() (e991c05 by yada).</li> <li>uni_peeker: adding peek_df functionality (ed78393 by yada).</li> <li>adding concurrent_loads() function (c3201d3 by yada).</li> </ul>"},{"location":"changelog/#bug-fixes_38","title":"Bug Fixes","text":"<ul> <li>ub.loads: tempfile naming error on windows (1d454d5 by yada).</li> </ul>"},{"location":"changelog/#v046-2024-06-28_1","title":"v0.4.6 - 2024-06-28","text":"<p>Compare with v0.4.5</p>"},{"location":"changelog/#bug-fixes_39","title":"Bug Fixes","text":"<ul> <li>UniSaver: replace NaN with null if saving to dict or jsonl (5babfb8 by yada).</li> </ul>"},{"location":"changelog/#v045-2024-06-28_1","title":"v0.4.5 - 2024-06-28","text":"<p>Compare with v0.4.4</p>"},{"location":"changelog/#bug-fixes_40","title":"Bug Fixes","text":"<ul> <li>adding graceful handling for errors when a line is unbale to be read (0965249 by yada).</li> </ul>"},{"location":"changelog/#v044-2024-06-14_1","title":"v0.4.4 - 2024-06-14","text":"<p>Compare with v0.4.3</p>"},{"location":"changelog/#bug-fixes_41","title":"Bug Fixes","text":"<ul> <li>ub.saves(); a bug where ub.saves(list[str]) won't correctly save (3849e63 by yada).</li> </ul>"},{"location":"changelog/#v043-2024-06-13_1","title":"v0.4.3 - 2024-06-13","text":"<p>Compare with v0.4.2</p>"},{"location":"changelog/#features_42","title":"Features","text":"<ul> <li>add ability to save various formatted image files as png file (8142695 by yada).</li> </ul>"},{"location":"changelog/#bug-fixes_42","title":"Bug Fixes","text":"<ul> <li>resolved bug that prevents loading files from url (c9cb709 by yada).</li> </ul>"},{"location":"changelog/#v042-2024-05-30_1","title":"v0.4.2 - 2024-05-30","text":"<p>Compare with v0.4.1</p>"},{"location":"changelog/#features_43","title":"Features","text":"<ul> <li>extend include_extensions at ub.traverses() to take more than extensions (593b69c by yada).</li> </ul>"},{"location":"changelog/#v041-2024-05-18_1","title":"v0.4.1 - 2024-05-18","text":"<p>Compare with v0.4.0</p>"},{"location":"changelog/#bug-fixes_43","title":"Bug Fixes","text":"<ul> <li>ub.traverses(): traversing a s3 directory will not return the dir itself (f0d85db by yada).</li> </ul>"},{"location":"changelog/#v040-2024-05-08_1","title":"v0.4.0 - 2024-05-08","text":"<p>Compare with v0.3.20</p>"},{"location":"changelog/#bug-fixes_44","title":"Bug Fixes","text":"<ul> <li>resizer hangs when handling large number of images to be resized (9627894 by yada).</li> </ul>"},{"location":"changelog/#v0320-2024-03-13_1","title":"v0.3.20 - 2024-03-13","text":"<p>Compare with v0.3.19</p>"},{"location":"changelog/#features_44","title":"Features","text":"<ul> <li>adding load feather support on uniloader (fee98ff by yada).</li> </ul>"},{"location":"changelog/#v0319-2024-03-09_1","title":"v0.3.19 - 2024-03-09","text":"<p>Compare with v0.3.18</p>"},{"location":"changelog/#bug-fixes_45","title":"Bug Fixes","text":"<ul> <li>incorrect behavior signature on ub.loads() on s3 uri (ca00b40 by yada).</li> </ul>"},{"location":"changelog/#v0318-2024-03-06_1","title":"v0.3.18 - 2024-03-06","text":"<p>Compare with v0.3.17</p>"},{"location":"changelog/#v0317-2024-03-06_1","title":"v0.3.17 - 2024-03-06","text":"<p>Compare with v0.3.16</p>"},{"location":"changelog/#bug-fixes_46","title":"Bug Fixes","text":"<ul> <li>unclosed image at uniresizer (b554630 by yada).</li> </ul>"},{"location":"changelog/#v0316-2024-03-05_1","title":"v0.3.16 - 2024-03-05","text":"<p>Compare with v0.3.15</p>"},{"location":"changelog/#bug-fixes_47","title":"Bug Fixes","text":"<ul> <li>adding back max_workers in uni_resizer (7c533f6 by yada).</li> </ul>"},{"location":"changelog/#v0315-2024-02-12_1","title":"v0.3.15 - 2024-02-12","text":"<p>Compare with v0.3.14</p>"},{"location":"changelog/#features_45","title":"Features","text":"<ul> <li>adding debug_print argument to unibox.traverses() (66d1c02 by yada).</li> </ul>"},{"location":"changelog/#v0314-2023-12-18_1","title":"v0.3.14 - 2023-12-18","text":"<p>Compare with v0.3.13</p>"},{"location":"changelog/#bug-fixes_48","title":"Bug Fixes","text":"<ul> <li>using unibox.loads() with s3 (2ba1a57 by yada).</li> </ul>"},{"location":"changelog/#v0313-2023-12-14_1","title":"v0.3.13 - 2023-12-14","text":"<p>Compare with v0.3.12</p>"},{"location":"changelog/#bug-fixes_49","title":"Bug Fixes","text":"<ul> <li>include ipykernel version to avoid tqdm issues (a081b70 by yada).</li> <li>traverses() with folder: incomplete s3 uri (3b4d25b by yada).</li> <li>adding unit for traverse s3 (c7d00d7 by yada).</li> </ul>"},{"location":"changelog/#v0312-2023-12-12_1","title":"v0.3.12 - 2023-12-12","text":"<p>Compare with v0.3.11</p>"},{"location":"changelog/#bug-fixes_50","title":"Bug Fixes","text":"<ul> <li>traverses(s3): allowing traverses() to return dir info (0af5fdb by yada).</li> </ul>"},{"location":"changelog/#v0311-2023-12-11_1","title":"v0.3.11 - 2023-12-11","text":"<p>Compare with v0.3.10</p>"},{"location":"changelog/#features_46","title":"Features","text":"<ul> <li>unibox.peeks(): adding list peek support &amp; proper command use (c346f99 by yada).</li> </ul>"},{"location":"changelog/#v0310-2023-12-11_1","title":"v0.3.10 - 2023-12-11","text":"<p>Compare with v0.3.9</p>"},{"location":"changelog/#features_47","title":"Features","text":"<ul> <li>adding support for s3 dir in unibox.traverses() (b90a97a by yada).</li> <li>adding unipeeker and unibox.peeks() method for previewing data (7b7a7cd by yada).</li> <li>adding traverse() in s3 client (3d786b3 by yada).</li> </ul>"},{"location":"changelog/#v039-2023-12-10_1","title":"v0.3.9 - 2023-12-10","text":"<p>Compare with v0.3.8</p>"},{"location":"changelog/#bug-fixes_51","title":"Bug Fixes","text":"<ul> <li>unibox.loads(): add ability to properly load files from url (32c5593 by yada).</li> </ul>"},{"location":"changelog/#v038-2023-11-21_1","title":"v0.3.8 - 2023-11-21","text":"<p>Compare with v0.3.6</p>"},{"location":"changelog/#features_48","title":"Features","text":"<ul> <li>support saving string as a txt file (5748e96 by yada).</li> <li>adding url support for unibox.loads() (9cc351b by yada).</li> </ul>"},{"location":"changelog/#v036-2023-11-12_1","title":"v0.3.6 - 2023-11-12","text":"<p>Compare with v0.3.5</p>"},{"location":"changelog/#features_49","title":"Features","text":"<ul> <li>support s3 uri in unibox.saves() (7e0db63 by yada).</li> <li>support s3 uri in unibox.loads() (7850ed4 by yada).</li> </ul>"},{"location":"changelog/#bug-fixes_52","title":"Bug Fixes","text":"<ul> <li>incorrect filename when using unibox.saves() on s3 (27a4121 by yada).</li> </ul>"},{"location":"changelog/#v035-2023-11-02_1","title":"v0.3.5 - 2023-11-02","text":"<p>Compare with v0.3.4</p>"},{"location":"changelog/#features_50","title":"Features","text":"<ul> <li>s3client | bump version to 0.3.5 (a0db95a by yada).</li> </ul>"},{"location":"changelog/#v034-2023-11-01_1","title":"v0.3.4 - 2023-11-01","text":"<p>Compare with v0.3.3</p>"},{"location":"changelog/#bug-fixes_53","title":"Bug Fixes","text":"<ul> <li>merges at init.py (ca658ed by yada).</li> </ul>"},{"location":"changelog/#v033-2023-11-01_1","title":"v0.3.3 - 2023-11-01","text":"<p>Compare with v0.3.2</p>"},{"location":"changelog/#v032-2023-11-01_1","title":"v0.3.2 - 2023-11-01","text":"<p>Compare with v0.3.1</p>"},{"location":"changelog/#features_51","title":"Features","text":"<ul> <li>UniMerger: unibox.merges(data1, data2) (d156e29 by yada).</li> </ul>"},{"location":"changelog/#v031-2023-09-17_1","title":"v0.3.1 - 2023-09-17","text":"<p>Compare with v0.3.0</p>"},{"location":"changelog/#v030-2023-09-03_1","title":"v0.3.0 - 2023-09-03","text":"<p>Compare with v0.2.14</p>"},{"location":"changelog/#features_52","title":"Features","text":"<ul> <li>image resizer: adding debug prints (bcb55df by yada).</li> <li>image resizer: adding ability to skip existing images (c745646 by yada).</li> <li>adding resizer-next (2ee732c by yada).</li> </ul>"},{"location":"changelog/#bug-fixes_54","title":"Bug Fixes","text":"<ul> <li>find existing images and remove them from jobs list (4ba16d9 by yada).</li> <li>adding lower() to suffix before doing checks (cc17d5c by yada).</li> </ul>"},{"location":"changelog/#v0214-2023-09-02_1","title":"v0.2.14 - 2023-09-02","text":"<p>Compare with v0.2.13</p>"},{"location":"changelog/#features_53","title":"Features","text":"<ul> <li>adding traverses() method for unibox (fe997a2 by yada).</li> </ul>"},{"location":"changelog/#v0213-2023-08-26_1","title":"v0.2.13 - 2023-08-26","text":"<p>Compare with v0.2.12</p>"},{"location":"changelog/#v0212-2023-08-23_1","title":"v0.2.12 - 2023-08-23","text":"<p>Compare with v0.2.11</p>"},{"location":"changelog/#features_54","title":"Features","text":"<ul> <li>adding wip file renameer (148280c by yada).</li> </ul>"},{"location":"changelog/#v0211-2023-08-21_1","title":"v0.2.11 - 2023-08-21","text":"<p>Compare with v0.2.10</p>"},{"location":"changelog/#bug-fixes_55","title":"Bug Fixes","text":"<ul> <li>typing alias issue in python38 (6179f3e by yada).</li> </ul>"},{"location":"changelog/#v0210-2023-08-20_1","title":"v0.2.10 - 2023-08-20","text":"<p>Compare with v0.2.9</p>"},{"location":"changelog/#features_55","title":"Features","text":"<ul> <li>reducing minimum  python dependency from 3.10 to 3.8 (bcf96e6 by yada).</li> </ul>"},{"location":"changelog/#v029-2023-08-19_1","title":"v0.2.9 - 2023-08-19","text":"<p>Compare with v0.2.8</p>"},{"location":"changelog/#v028-2023-08-18_1","title":"v0.2.8 - 2023-08-18","text":"<p>Compare with v0.2.7</p>"},{"location":"changelog/#v027-2023-08-17_1","title":"v0.2.7 - 2023-08-17","text":"<p>Compare with v0.2.6</p>"},{"location":"changelog/#v026-2023-08-15_1","title":"v0.2.6 - 2023-08-15","text":"<p>Compare with v0.2.5</p>"},{"location":"changelog/#v025-2023-08-15_1","title":"v0.2.5 - 2023-08-15","text":"<p>Compare with v0.2.3</p>"},{"location":"changelog/#features_56","title":"Features","text":"<ul> <li>updating UniTraverser for stateful calls and filepath store (0e385e9 by yada).</li> <li>adding UniTraverser class: code that traverses trhough directory (2e6ebff by yada).</li> </ul>"},{"location":"changelog/#v023-2023-08-14_1","title":"v0.2.3 - 2023-08-14","text":"<p>Compare with v0.2.2</p>"},{"location":"changelog/#features_57","title":"Features","text":"<ul> <li>remove pandas / pyarrow dep version (fe83df4 by yada).</li> </ul>"},{"location":"changelog/#v022-2023-08-13_1","title":"v0.2.2 - 2023-08-13","text":"<p>Compare with v0.1.4.3</p>"},{"location":"changelog/#features_58","title":"Features","text":"<ul> <li>adding unisaver (62d2eb2 by yada).</li> <li>adding UniSaver and unibox.saves() method; bump version number to 0.2.0 (4eec7b2 by yada).</li> </ul>"},{"location":"changelog/#v0143-2023-07-16_1","title":"v0.1.4.3 - 2023-07-16","text":"<p>Compare with v0.1.4</p>"},{"location":"changelog/#features_59","title":"Features","text":"<ul> <li>updating loads() for jsonl files (ccf54f3 by yada).</li> </ul>"},{"location":"changelog/#v014-2023-07-14_1","title":"v0.1.4 - 2023-07-14","text":"<p>Compare with v0.1.3.5</p>"},{"location":"changelog/#features_60","title":"Features","text":"<ul> <li>optimizing UniLoader class for csv &amp; parquet (383215f by yada).</li> </ul>"},{"location":"changelog/#v0135-2023-07-14_1","title":"v0.1.3.5 - 2023-07-14","text":"<p>Compare with v0.1.3.4</p>"},{"location":"changelog/#features_61","title":"Features","text":"<ul> <li>adding file mover; update image resizer (4d46099 by yada).</li> </ul>"},{"location":"changelog/#v0134-2023-07-10_1","title":"v0.1.3.4 - 2023-07-10","text":"<p>Compare with v0.1.3.3</p>"},{"location":"changelog/#features_62","title":"Features","text":"<ul> <li>using ProcessPool instead of ThreadPool; before: 80it/s -&gt; now: 105it/s (0180ad1 by yada).</li> </ul>"},{"location":"changelog/#v0133-2023-07-10_1","title":"v0.1.3.3 - 2023-07-10","text":"<p>Compare with v0.1.3.2</p>"},{"location":"changelog/#bug-fixes_56","title":"Bug Fixes","text":"<ul> <li>missing _resize (4d5fa90 by yada).</li> </ul>"},{"location":"changelog/#v0132-2023-07-10_1","title":"v0.1.3.2 - 2023-07-10","text":"<p>Compare with v0.1.3</p>"},{"location":"changelog/#features_63","title":"Features","text":"<ul> <li>updating version number (3c25239 by yada).</li> </ul>"},{"location":"changelog/#bug-fixes_57","title":"Bug Fixes","text":"<ul> <li>not resizing image when min_size &gt; actual size (85acf5d by yada).</li> </ul>"},{"location":"changelog/#v013-2023-07-10_1","title":"v0.1.3 - 2023-07-10","text":"<p>Compare with 0.1.21</p>"},{"location":"changelog/#features_64","title":"Features","text":"<ul> <li>updating cli &amp; click requirement version (cd324bd by yada).</li> <li>adding image resizer; refactor dir (8586022 by yada).</li> </ul>"},{"location":"changelog/#0121-2023-07-06_1","title":"0.1.21 - 2023-07-06","text":"<p>Compare with v0.1.2.1</p>"},{"location":"changelog/#v0121-2023-07-06_1","title":"v0.1.2.1 - 2023-07-06","text":"<p>Compare with v0.1.2</p>"},{"location":"changelog/#v012-2023-07-06_1","title":"v0.1.2 - 2023-07-06","text":"<p>Compare with v0.1</p>"},{"location":"changelog/#bug-fixes_58","title":"Bug Fixes","text":"<ul> <li>update version number (d5dc53a by yada).</li> </ul>"},{"location":"changelog/#v01-2023-07-06_1","title":"v0.1 - 2023-07-06","text":"<p>Compare with first commit</p>"},{"location":"changelog/#features_65","title":"Features","text":"<ul> <li>adding pipeline logger &amp; loader (56043e4 by yada).</li> <li>adding basic functionality (42d83cb by yada).</li> </ul>"},{"location":"code_of_conduct/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"code_of_conduct/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"code_of_conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the overall   community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or advances of   any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email address,   without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"code_of_conduct/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"code_of_conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"code_of_conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at trojblue@gmail.com. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"code_of_conduct/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"code_of_conduct/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"code_of_conduct/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"code_of_conduct/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"code_of_conduct/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"code_of_conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p>"},{"location":"contributing/#environment-setup","title":"Environment setup","text":"<p>Nothing easier!</p> <p>Fork and clone the repository, then:</p> <pre><code>cd unibox\nmake setup\n</code></pre> <p>Note</p> <p> If it fails for some reason, you'll need to install uv manually.</p> <p>You can install it with:</p> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <p>Now you can try running <code>make setup</code> again, or simply <code>uv sync</code>.</p> <p>You now have the dependencies installed.</p> <p>You can run the application with <code>make run unibox [ARGS...]</code>.</p> <p>Run <code>make help</code> to see all the available actions!</p>"},{"location":"contributing/#common-workflow","title":"Common Workflow","text":"<p>A typical workflow for publishing a new version would be:</p> <pre><code>make setup   # one-time\nmake format  # auto-format\nmake test\nmake check\nmake changelog\nmake release\n</code></pre>"},{"location":"contributing/#tasks","title":"Tasks","text":"<p>The entry-point to run commands and tasks is the <code>make</code> Python script, located in the <code>scripts</code> directory. Try running <code>make</code> to show the available commands and tasks. The commands do not need the Python dependencies to be installed, while the tasks do. The cross-platform tasks are written in Python, thanks to duty.</p> <p>If you work in VSCode, we provide an action to configure VSCode for the project.</p>"},{"location":"contributing/#development","title":"Development","text":"<p>As usual:</p> <ol> <li>create a new branch: <code>git switch -c feature-or-bugfix-name</code></li> <li>edit the code and/or the documentation</li> </ol> <p>Before committing:</p> <ol> <li>run <code>make format</code> to auto-format the code</li> <li>run <code>make check</code> to check everything (fix any warning)</li> <li>run <code>make test</code> to run the tests (fix any issue)</li> <li>if you updated the documentation or the project dependencies:<ol> <li>run <code>make docs</code></li> <li>go to http://localhost:8000 and check that everything looks good</li> </ol> </li> <li>follow our commit message convention</li> </ol> <p>If you are unsure about how to fix or ignore a warning, just let the continuous integration fail, and we will help you during review.</p> <p>Don't bother updating the changelog, we will take care of this.</p>"},{"location":"contributing/#commit-message-convention","title":"Commit message convention","text":"<p>Commit messages must follow our convention based on the Angular style or the Karma convention:</p> <pre><code>&lt;type&gt;[(scope)]: Subject\n\n[Body]\n</code></pre> <p>Subject and body must be valid Markdown. Subject must have proper casing (uppercase for first letter if it makes sense), but no dot at the end, and no punctuation in general.</p> <p>Scope and body are optional. Type can be:</p> <ul> <li><code>build</code>: About packaging, building wheels, etc.</li> <li><code>chore</code>: About packaging or repo/files management.</li> <li><code>ci</code>: About Continuous Integration.</li> <li><code>deps</code>: Dependencies update.</li> <li><code>docs</code>: About documentation.</li> <li><code>feat</code>: New feature.</li> <li><code>fix</code>: Bug fix.</li> <li><code>perf</code>: About performance.</li> <li><code>refactor</code>: Changes that are not features or bug fixes.</li> <li><code>style</code>: A change in code style/format.</li> <li><code>tests</code>: About tests.</li> </ul> <p>If you write a body, please add trailers at the end (for example issues and PR references, or co-authors), without relying on GitHub's flavored Markdown:</p> <pre><code>Body.\n\nIssue #10: https://github.com/namespace/project/issues/10\nRelated to PR namespace/other-project#15: https://github.com/namespace/other-project/pull/15\n</code></pre> <p>These \"trailers\" must appear at the end of the body, without any blank lines between them. The trailer title can contain any character except colons <code>:</code>. We expect a full URI for each trailer, not just GitHub autolinks (for example, full GitHub URLs for commits and issues, not the hash or the #issue-number).</p> <p>We do not enforce a line length on commit messages summary and body, but please avoid very long summaries, and very long lines in the body, unless they are part of code blocks that must not be wrapped.</p>"},{"location":"contributing/#pull-requests-guidelines","title":"Pull requests guidelines","text":"<p>Link to any related issue in the Pull Request message.</p> <p>During the review, we recommend using fixups:</p> <pre><code># SHA is the SHA of the commit you want to fix\ngit commit --fixup=SHA\n</code></pre> <p>Once all the changes are approved, you can squash your commits:</p> <pre><code>git rebase -i --autosquash main\n</code></pre> <p>And force-push:</p> <pre><code>git push -f\n</code></pre> <p>If this seems all too complicated, you can push or force-push each new commit, and we will squash them ourselves if needed, before merging.</p>"},{"location":"credits/","title":"Credits","text":""},{"location":"credits/#exec-1--credits","title":"Credits","text":"<p>These projects were used to build unibox. Thank you!</p> <p>Python | uv | copier-uv</p>"},{"location":"credits/#exec-1--runtime-dependencies","title":"Runtime dependencies","text":"Project Summary Version (accepted) Version (last resolved) License colorama Cross-platform colored terminal text. <code>&gt;=0.4.6, &gt;=0.4</code> <code>0.4.6</code> BSD License tomli A lil' TOML parser <code>&gt;=2.2.1, &gt;=2.0</code> <code>2.4.0</code> MIT"},{"location":"credits/#exec-1--development-dependencies","title":"Development dependencies","text":"Project Summary Version (accepted) Version (last resolved) License babel Internationalization utilities <code>&gt;=2.7.0</code> <code>2.17.0</code> BSD-3-Clause backrefs A wrapper around re and regex that adds additional back references. <code>&gt;=5.7.post1</code> <code>6.1</code> MIT black The uncompromising code formatter. <code>&gt;=24.4</code> <code>26.1.0</code> MIT certifi Python package for providing Mozilla's CA Bundle. <code>&gt;=2017.4.17</code> <code>2026.1.4</code> MPL-2.0 charset-normalizer The Real First Universal Charset Detector. Open, modern and actively maintained alternative to Chardet. <code>&gt;=2, &lt;4</code> <code>3.4.4</code> MIT click Composable command line interface toolkit <code>&gt;=8.0.0</code> <code>8.3.1</code> BSD-3-Clause colorama Cross-platform colored terminal text. <code>&gt;=0.4.6, &gt;=0.4</code> <code>0.4.6</code> BSD License csscompressor A python port of YUI CSS Compressor <code>&gt;=0.9.5</code> <code>0.9.5</code> BSD ghp-import Copy your docs directly to the gh-pages branch. <code>&gt;=1.0</code> <code>2.1.0</code> Apache Software License gitdb Git Object Database <code>&gt;=4.0.1, &lt;5</code> <code>4.0.12</code> BSD License GitPython GitPython is a Python library used to interact with Git repositories <code>&gt;=3.1.44</code> <code>3.1.46</code> BSD-3-Clause griffe Signatures for entire Python programs. Extract the structure, the frame, the skeleton of your project, to generate API documentation or find breaking changes in your API. <code>&gt;=1.13</code> <code>1.15.0</code> ISC htmlmin2 An HTML Minifier <code>&gt;=0.1.13</code> <code>0.1.13</code> BSD idna Internationalized Domain Names in Applications (IDNA) <code>&gt;=2.5, &lt;4</code> <code>3.11</code> BSD-3-Clause Jinja2 A very fast and expressive template engine. <code>&gt;=3.1</code> <code>3.1.6</code> BSD License jsmin JavaScript minifier. <code>&gt;=3.0.1</code> <code>3.0.1</code> MIT License Markdown Python implementation of John Gruber's Markdown. <code>&gt;=3.3.3</code> <code>3.10</code> BSD-3-Clause markdown-callouts Markdown extension: a classier syntax for admonitions <code>&gt;=0.4</code> <code>0.4.0</code> MIT markdown-exec Utilities to execute code blocks in Markdown files. <code>&gt;=1.8</code> <code>1.12.1</code> ISC MarkupSafe Safely add untrusted strings to HTML/XML markup. <code>&gt;=2.0.1</code> <code>3.0.3</code> BSD-3-Clause mergedeep A deep merge function for \ud83d\udc0d. <code>&gt;=1.3.4</code> <code>1.3.4</code> MIT License mkdocs Project documentation with Markdown. <code>&gt;=1.6</code> <code>1.6.1</code> BSD-2-Clause mkdocs-autorefs Automatically link across pages in MkDocs. <code>&gt;=1.4</code> <code>1.4.3</code> ISC mkdocs-coverage MkDocs plugin to integrate your coverage HTML report into your site. <code>&gt;=1.0</code> <code>2.0.0</code> ISC mkdocs-gen-files MkDocs plugin to programmatically generate documentation pages during the build <code>&gt;=0.5</code> <code>0.6.0</code> MIT mkdocs-get-deps MkDocs extension that lists all dependencies according to a mkdocs.yml file <code>&gt;=0.2.0</code> <code>0.2.0</code> MIT mkdocs-git-revision-date-localized-plugin Mkdocs plugin that enables displaying the localized date of the last git modification of a markdown file. <code>&gt;=1.2</code> <code>1.5.0</code> MIT mkdocs-literate-nav MkDocs plugin to specify the navigation in Markdown instead of YAML <code>&gt;=0.6</code> <code>0.6.2</code> MIT mkdocs-material Documentation that simply works <code>&gt;=9.5</code> <code>9.7.1</code> MIT mkdocs-material-extensions Extension pack for Python Markdown and MkDocs Material. <code>&gt;=1.3</code> <code>1.3.1</code> MIT mkdocs-minify-plugin An MkDocs plugin to minify HTML, JS or CSS files prior to being written to disk <code>&gt;=0.8</code> <code>0.8.0</code> MIT mkdocstrings Automatic documentation from sources, for MkDocs. <code>&gt;=0.25</code> <code>1.0.0</code> ISC mkdocstrings-python A Python handler for mkdocstrings. <code>&gt;=1.16.2</code> <code>2.0.1</code> ISC mypy_extensions Type system extensions for programs checked with the mypy type checker. <code>&gt;=0.4.3</code> <code>1.1.0</code> MIT packaging Core utilities for Python packages <code>&gt;=22.0</code> <code>25.0</code> Apache Software License + BSD License paginate Divides large result sets into pages for easier browsing <code>&gt;=0.5</code> <code>0.5.7</code> MIT pathspec Utility library for gitignore style pattern matching of file paths. <code>&gt;=1.0.0</code> <code>1.0.3</code> Mozilla Public License 2.0 (MPL 2.0) platformdirs A small Python package for determining appropriate platform-specific dirs, e.g. a <code>user data dir</code>. <code>&gt;=2</code> <code>4.5.1</code> MIT Pygments Pygments is a syntax highlighting package written in Python. <code>&gt;=2.16</code> <code>2.19.2</code> BSD-2-Clause pymdown-extensions Extension pack for Python Markdown. <code>&gt;=10.2</code> <code>10.20</code> MIT python-dateutil Extensions to the standard Python datetime module <code>&gt;=2.8.1</code> <code>2.9.0.post0</code> BSD License + Apache Software License pytokens A Fast, spec compliant Python 3.14+ tokenizer that runs on older Pythons. <code>&gt;=0.3.0</code> <code>0.3.0</code> MIT PyYAML YAML parser and emitter for Python <code>&gt;=5.1</code> <code>6.0.3</code> MIT pyyaml_env_tag A custom YAML tag for referencing environment variables in YAML files. <code>&gt;=0.1</code> <code>1.1</code> MIT requests Python HTTP for Humans. <code>&gt;=2.30</code> <code>2.32.5</code> Apache-2.0 six Python 2 and 3 compatibility utilities <code>&gt;=1.5</code> <code>1.17.0</code> MIT smmap A pure Python implementation of a sliding window memory map manager <code>&gt;=3.0.1, &lt;6</code> <code>5.0.2</code> BSD-3-Clause tomli A lil' TOML parser <code>&gt;=2.2.1, &gt;=2.0</code> <code>2.4.0</code> MIT typing_extensions Backported and Experimental Type Hints for Python 3.9+ <code>&gt;=4.0.1</code> <code>4.15.0</code> PSF-2.0 urllib3 HTTP library with thread-safe connection pooling, file post, and more. <code>&gt;=1.21.1, &lt;3</code> <code>2.6.3</code> MIT watchdog Filesystem events monitoring <code>&gt;=2.0</code> <code>6.0.0</code> Apache-2.0"},{"location":"design/","title":"Design","text":""},{"location":"design/#loaders","title":"Loaders:","text":"<p>save, load: takes string (instead of a Path)</p>"},{"location":"getting_started_credentials/","title":"Credentials","text":"<p>Unibox relies on standard credential chains from the underlying SDKs: - Hugging Face via <code>huggingface_hub</code>. - AWS via <code>boto3</code>.</p> <p>This page shows the most common setup paths.</p>"},{"location":"getting_started_credentials/#hugging-face","title":"Hugging Face","text":""},{"location":"getting_started_credentials/#option-1-login-via-cli-recommended","title":"Option 1: Login via CLI (recommended)","text":"<pre><code>huggingface-cli login\n</code></pre> <p>This stores a token in your HF cache. <code>huggingface_hub</code> will pick it up automatically.</p>"},{"location":"getting_started_credentials/#option-2-set-env-var","title":"Option 2: Set env var","text":"<pre><code>export HF_TOKEN=\"hf_...\"\n</code></pre> <p>Note</p> <p><code>huggingface_hub</code> also supports <code>HUGGINGFACE_HUB_TOKEN</code>. If both exist, it uses the highest-priority token it finds.</p>"},{"location":"getting_started_credentials/#verify","title":"Verify","text":"<pre><code>import unibox as ub\n\n# This should work if your token is valid\nub.loads(\"hf://huggingface/README\")\n</code></pre>"},{"location":"getting_started_credentials/#aws-s3","title":"AWS (S3)","text":""},{"location":"getting_started_credentials/#option-1-environment-variables","title":"Option 1: Environment variables","text":"<pre><code>export AWS_ACCESS_KEY_ID=\"...\"\nexport AWS_SECRET_ACCESS_KEY=\"...\"\nexport AWS_DEFAULT_REGION=\"us-east-1\"\n</code></pre>"},{"location":"getting_started_credentials/#option-2-aws-config-files","title":"Option 2: AWS config files","text":"<p>Most users already have these in: - <code>~/.aws/credentials</code> - <code>~/.aws/config</code></p> <p>Boto3 will detect them automatically.</p>"},{"location":"getting_started_credentials/#verify_1","title":"Verify","text":"<pre><code>import unibox as ub\n\n# Try listing a bucket prefix\nub.ls(\"s3://my-bucket/data\", exts=[\".csv\"])\n</code></pre> <p>Warning</p> <p>If you see permission errors, double-check the bucket policy and IAM permissions.</p>"},{"location":"getting_started_credentials/#optional-helper","title":"Optional: helper","text":"<p>If you already have credentials cached locally, you can apply them with:</p> <pre><code>from unibox.utils.credentials_manager import apply_credentials\n\napply_credentials(\"aws\", \"hf\")\n</code></pre> <p>This helper loads standard credential locations and exports environment variables for the current process.</p>"},{"location":"getting_started_credentials/#next-steps","title":"Next steps","text":"<ul> <li>Jump back to the 5\u2011minute tour. \u2192 Quickstart</li> <li>Load and save datasets and files. \u2192 Hugging Face guide</li> <li>Read and write objects from S3. \u2192 S3 guide</li> </ul>"},{"location":"getting_started_quickstart/","title":"Quickstart","text":"<p>Unibox gives you one API for local files, S3, and Hugging Face datasets. This page is a 5-minute tour.</p> <p>Tip</p> <p>You can copy-paste these snippets as-is. Replace paths, buckets, and repo IDs with your own.</p>"},{"location":"getting_started_quickstart/#install","title":"Install","text":"<pre><code>pip install unibox\n</code></pre>"},{"location":"getting_started_quickstart/#load-and-save-in-3-tabs","title":"Load and save in 3 tabs","text":"LocalS3Hugging Face <pre><code>import unibox as ub\n\n# Load a local file\ndf = ub.loads(\"data/sample.parquet\")\nprint(df.head(3))\n\n# Save to a new local file\nub.saves(df, \"data/processed.parquet\")\n</code></pre> <pre><code>import unibox as ub\n\n# Load a file from S3\ndf = ub.loads(\"s3://my-bucket/data/sample.csv\")\n\n# Save back to S3\nub.saves(df, \"s3://my-bucket/data/processed.parquet\")\n</code></pre> <p>Note</p> <p>You need AWS credentials set up. See the Credentials page.</p> <pre><code>import unibox as ub\n\n# Load a dataset\nds = ub.loads(\"hf://my-org/my-dataset\")\n\n# Save a DataFrame or Dataset to HF\nub.saves(ds, \"hf://my-org/my-new-dataset\")\n</code></pre> <p>Note</p> <p>Hugging Face dataset URIs use <code>hf://owner/repo</code>.</p>"},{"location":"getting_started_quickstart/#peek-and-list","title":"Peek and list","text":"<pre><code>import unibox as ub\n\n# Peek into a dataset or DataFrame\nub.peeks(ds)\n\n# List files in a folder or bucket prefix\nfiles = ub.ls(\"s3://my-bucket/data\", exts=[\".parquet\", \".csv\"])\nprint(files[:5])\n</code></pre>"},{"location":"getting_started_quickstart/#next-steps","title":"Next steps","text":"<ul> <li>Set up AWS and Hugging Face access. \u2192 Credentials</li> <li>See which extensions map to which loaders. \u2192 Supported formats</li> <li>Learn splits, revisions, and JSON-like saves. \u2192 Hugging Face guide</li> <li>Explore notebook helpers and image tools. \u2192 Utilities</li> <li>Try task\u2011oriented snippets. \u2192 Recipes</li> </ul>"},{"location":"guides_hugging_face/","title":"Hugging Face guide","text":"<p>Unibox supports both datasets and files hosted on the Hugging Face Hub.</p>"},{"location":"guides_hugging_face/#dataset-vs-file-uris","title":"Dataset vs file URIs","text":"<ul> <li><code>hf://owner/repo</code> -&gt; dataset</li> <li><code>hf://owner/repo/path/to/file.ext</code> -&gt; file</li> </ul> <p>Tip</p> <p>If your path includes a file extension, unibox treats it as a file. Otherwise, it treats it as a dataset.</p>"},{"location":"guides_hugging_face/#load-a-dataset","title":"Load a dataset","text":"<pre><code>import unibox as ub\n\n# Load the default split (train)\ntrain = ub.loads(\"hf://my-org/my-dataset\")\n\n# Load a specific split\nval = ub.loads(\"hf://my-org/my-dataset\", split=\"validation\")\n\n# Load as pandas DataFrame\ntrain_df = ub.loads(\"hf://my-org/my-dataset\", split=\"train\", to_pandas=True)\n</code></pre>"},{"location":"guides_hugging_face/#save-a-dataset","title":"Save a dataset","text":"<pre><code>import unibox as ub\n\n# Save a DataFrame\nub.saves(train_df, \"hf://my-org/my-dataset\")\n\n# Save a Dataset or DatasetDict\nub.saves(train, \"hf://my-org/my-dataset\")\n</code></pre>"},{"location":"guides_hugging_face/#save-options","title":"Save options","text":"<pre><code>ub.saves(train_df, \"hf://my-org/my-dataset\", split=\"train\", private=True)\n</code></pre>"},{"location":"guides_hugging_face/#save-json-like-inputs","title":"Save JSON-like inputs","text":"<p>Unibox can convert JSON-like structures into a DataFrame and upload them:</p> <pre><code>import unibox as ub\n\n# dict input (keys become rows)\nub.saves({\"a\": 1, \"b\": {\"c\": 2}}, \"hf://me/my-ds\")\n\n# list of dicts (JSONL-style)\nub.saves([{\"id\": 1}, {\"id\": 2}], \"hf://me/my-ds\")\n\n# list of scalars\nub.saves([\"foo\", \"bar\"], \"hf://me/my-ds\")\n</code></pre> <p>You can also convert explicitly:</p> <pre><code>import unibox as ub\n\ndf = ub.to_df([{\"id\": 1}, {\"id\": 2}])\nub.saves(df, \"hf://me/my-ds\")\n</code></pre> <p>Warning</p> <p>If the list mixes dicts and non-dicts, unibox will still convert but emits a warning.</p>"},{"location":"guides_hugging_face/#load-a-file-from-hf","title":"Load a file from HF","text":"<pre><code>import unibox as ub\n\n# Load a JSON file from a dataset repo\ncfg = ub.loads(\"hf://my-org/my-dataset/config.json\")\n</code></pre>"},{"location":"guides_hugging_face/#common-pitfalls","title":"Common pitfalls","text":"<ul> <li>Auth: Make sure your token is available (see Credentials).</li> <li>Repo types: <code>hf://owner/repo</code> is assumed to be a dataset.</li> <li>Splits: If the split does not exist, <code>datasets</code> will raise an error.</li> </ul>"},{"location":"guides_hugging_face/#next-steps","title":"Next steps","text":"<ul> <li>Make sure your HF token is set up. \u2192 Credentials</li> <li>See which extensions map to which loaders. \u2192 Supported formats</li> <li>Quick tasks like previews and concurrent loads. \u2192 Recipes</li> </ul>"},{"location":"guides_s3/","title":"S3 guide","text":"<p>Unibox uses boto3 under the hood. If boto3 can access your bucket, unibox will work.</p>"},{"location":"guides_s3/#load-files-from-s3","title":"Load files from S3","text":"<pre><code>import unibox as ub\n\n# CSV to DataFrame\nsales = ub.loads(\"s3://my-bucket/data/sales.csv\")\n\n# Parquet to DataFrame\nevents = ub.loads(\"s3://my-bucket/data/events.parquet\")\n\n# JSON to dict or list\ncfg = ub.loads(\"s3://my-bucket/configs/app.json\")\n</code></pre>"},{"location":"guides_s3/#save-files-to-s3","title":"Save files to S3","text":"<pre><code>import unibox as ub\n\nub.saves(sales, \"s3://my-bucket/data/sales_clean.parquet\")\nub.saves(cfg, \"s3://my-bucket/configs/app_clean.json\")\n</code></pre>"},{"location":"guides_s3/#list-objects","title":"List objects","text":"<pre><code>import unibox as ub\n\n# List only parquet files\nfiles = ub.ls(\"s3://my-bucket/data\", exts=[\".parquet\"])\nprint(files[:3])\n</code></pre>"},{"location":"guides_s3/#tips","title":"Tips","text":"<p>Tip</p> <p>Use <code>exts</code> to reduce listing noise for large prefixes.</p> <p>Warning</p> <p>If you see AccessDenied errors, check IAM permissions and bucket policy.</p>"},{"location":"guides_s3/#next-steps","title":"Next steps","text":"<ul> <li>Verify AWS access for your environment. \u2192 Credentials</li> <li>See which file types you can load/save. \u2192 Supported formats</li> <li>Task\u2011oriented snippets using S3. \u2192 Recipes</li> </ul>"},{"location":"license/","title":"License","text":"<pre><code>MIT License\n\nCopyright (c) 2025 trojblue\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n</code></pre>"},{"location":"recipes/","title":"Recipes","text":"<p>Short, task-oriented snippets for common workflows.</p>"},{"location":"recipes/#quick-peek","title":"Quick peek","text":"<pre><code>import unibox as ub\n\n# Works for DataFrames, lists, dicts, and datasets\npeek = ub.peeks(ub.loads(\"hf://my-org/my-ds\"))\nprint(peek)\n</code></pre> <p>Tip</p> <p><code>peeks(..., console_print=True)</code> prints a compact JSON preview to stdout.</p>"},{"location":"recipes/#concurrent-loads","title":"Concurrent loads","text":"<pre><code>import unibox as ub\n\nuris = [\n    \"s3://my-bucket/data/a.parquet\",\n    \"s3://my-bucket/data/b.parquet\",\n    \"s3://my-bucket/data/c.parquet\",\n]\n\nitems = ub.concurrent_loads(uris, num_workers=8)\nprint(len(items))\n</code></pre>"},{"location":"recipes/#list-and-filter-by-extension","title":"List and filter by extension","text":"<pre><code>import unibox as ub\n\nimages = ub.ls(\"s3://my-bucket/images\", exts=[\".jpg\", \".png\"])\nprint(images[:5])\n</code></pre>"},{"location":"recipes/#save-json-like-data-to-hf","title":"Save JSON-like data to HF","text":"<pre><code>import unibox as ub\n\n# List of dicts\nub.saves([{\"id\": 1}, {\"id\": 2}], \"hf://me/quick-ds\")\n\n# List of strings\nub.saves([\"alpha\", \"beta\"], \"hf://me/strings-ds\")\n</code></pre>"},{"location":"recipes/#next-steps","title":"Next steps","text":"<ul> <li>Notebook helpers and image tools. \u2192 Utilities</li> <li>Full dataset save/load options. \u2192 Hugging Face guide</li> <li>Load and save files on S3. \u2192 S3 guide</li> </ul>"},{"location":"supported_formats/","title":"Supported formats","text":"<p>Unibox routes files by extension and URI scheme. This page summarizes the current mapping.</p>"},{"location":"supported_formats/#file-types-by-extension","title":"File types by extension","text":"Type Extensions Loader Notes Tabular <code>.csv</code> CSVLoader Loads to DataFrame Tabular <code>.parquet</code> ParquetLoader Loads to DataFrame JSON <code>.json</code> JSONLoader Loads to dict/list JSONL <code>.jsonl</code> JSONLLoader Loads to list of dicts Text <code>.txt</code>, <code>.md</code>, <code>.markdown</code> TxtLoader Loads to string Images common image types ImageLoader Returns PIL images or arrays Config <code>.yaml</code>, <code>.yml</code> YAMLLoader Loads to dict Config <code>.toml</code> TOMLLoader Loads to dict <p>Note</p> <p>Image extensions are defined in <code>src/unibox/utils/constants.py</code>.</p>"},{"location":"supported_formats/#hugging-face-uris","title":"Hugging Face URIs","text":"<ul> <li><code>hf://owner/repo</code> (no file extension) is treated as a dataset.</li> <li><code>hf://owner/repo/path/file.ext</code> is treated as a file and uses the extension mapping above.</li> </ul>"},{"location":"supported_formats/#json-like-saves-to-hugging-face","title":"JSON-like saves to Hugging Face","text":"<p>When saving to a dataset URI, <code>ub.saves</code> also accepts JSON-like inputs: - dict - list of dicts (JSONL-style) - list of scalars</p> <p>These are converted into a DataFrame and then uploaded as a dataset.</p>"},{"location":"supported_formats/#next-steps","title":"Next steps","text":"<ul> <li>Learn dataset vs file semantics and save options. \u2192 Hugging Face guide</li> <li>Load and save common formats from S3. \u2192 S3 guide</li> <li>Task\u2011oriented snippets using these formats. \u2192 Recipes</li> </ul>"},{"location":"utilities/","title":"Utilities &amp; helpers","text":"<p>Unibox includes small, high\u2011leverage helpers that make exploration and labeling faster. This page highlights the most useful ones.</p>"},{"location":"utilities/#notebook-galleries","title":"Notebook galleries","text":""},{"location":"utilities/#image-gallery","title":"Image gallery","text":"<pre><code>import unibox as ub\n\nub.gallery(\n    [\"/path/to/img1.jpg\", \"/path/to/img2.jpg\"],\n    labels=[\"cat\", \"dog\"],\n    row_height=\"200px\",\n)\n</code></pre>"},{"location":"utilities/#label-gallery","title":"Label gallery","text":"<pre><code>import unibox as ub\n\nub.label_gallery(\n    [\"/path/to/img1.jpg\", \"/path/to/img2.jpg\"],\n    labels=[\"good\", \"bad\"],\n    row_height=\"150px\",\n)\n</code></pre> <p>Note</p> <p>These functions require IPython/Jupyter. If IPython is not available, they print a warning instead of rendering.</p>"},{"location":"utilities/#image-utilities","title":"Image utilities","text":"<p>Helpers for assembling and annotating images live in <code>unibox.utils.image_utils</code>:</p> <pre><code>from unibox.utils.image_utils import (\n    add_annotation,\n    add_annotations,\n    concatenate_images_horizontally,\n)\n</code></pre>"},{"location":"utilities/#concatenate-images","title":"Concatenate images","text":"<pre><code>from PIL import Image\n\nimages = [Image.open(p) for p in [\"a.jpg\", \"b.jpg\", \"c.jpg\"]]\ncombo = concatenate_images_horizontally(images, max_height=512)\ncombo.save(\"combo.jpg\")\n</code></pre>"},{"location":"utilities/#add-annotations","title":"Add annotations","text":"<pre><code>from PIL import Image\nfrom unibox.utils.image_utils import add_annotation\n\nimg = Image.open(\"sample.jpg\")\nannotated = add_annotation(img, \"good\", position=\"top\", alignment=\"center\", size=\"larger\")\nannotated.save(\"annotated.jpg\")\n</code></pre>"},{"location":"utilities/#dataframe-utilities","title":"DataFrame utilities","text":"<p>Useful helpers from <code>unibox.utils.df_utils</code>:</p> <pre><code>from unibox.utils.df_utils import (\n    column_memory_usage,\n    convert_object_to_category,\n    downcast_numerical_columns,\n)\n</code></pre>"},{"location":"utilities/#json-like-to-dataframe","title":"JSON-like to DataFrame","text":"<p>If you have dicts or lists and want a DataFrame:</p> <pre><code>import unibox as ub\n\ndf = ub.to_df({\"a\": 1, \"b\": {\"c\": 2}})\n</code></pre>"},{"location":"utilities/#memory-usage-per-column","title":"Memory usage per column","text":"<pre><code>import pandas as pd\n\nmem = column_memory_usage(df)\nprint(mem.head())\n</code></pre>"},{"location":"utilities/#reduce-memory-footprint","title":"Reduce memory footprint","text":"<pre><code># Downcast numeric columns to smaller dtypes\nsmall_df = downcast_numerical_columns(df)\n\n# Convert low-cardinality text columns to categoricals\nsmall_df = convert_object_to_category(small_df, [\"country\", \"segment\"])\n</code></pre>"},{"location":"utilities/#quick-llm-calls","title":"Quick LLM calls","text":"<p>Lightweight wrappers live in <code>unibox.utils.llm_api</code>:</p> <pre><code>from unibox.utils.llm_api import generate_openai, generate_gemini\n\ntext = generate_openai(\"Summarize this text\", api_key=\"...\")\ntext = generate_gemini(\"Summarize this text\", api_key=\"...\")\n</code></pre> <p>Warning</p> <p>These are thin wrappers; handle retries, logging, and rate limits in your own code if needed.</p>"},{"location":"utilities/#s3-presigned-links","title":"S3 presigned links","text":"<p>If you need short\u2011lived URLs for S3 objects:</p> <pre><code>import unibox as ub\n\nurl = ub.presigns(\"s3://my-bucket/data/file.parquet\", expiration=3600)\n</code></pre>"},{"location":"utilities/#next-steps","title":"Next steps","text":"<ul> <li>Task\u2011oriented snippets using these helpers. \u2192 Recipes</li> <li>Back to the 5\u2011minute tour. \u2192 Quickstart</li> <li>See which file types you can load/save. \u2192 Supported formats</li> </ul>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li> unibox<ul> <li> backends<ul> <li> backend_router</li> <li> base_backend</li> <li> hf_hybrid_backend</li> <li> http_backend</li> <li> local_backend</li> <li> s3_backend</li> </ul> </li> <li> cli</li> <li> debug</li> <li> loaders<ul> <li> base_loader</li> <li> csv_loader</li> <li> hf_dataset_loader</li> <li> image_loder</li> <li> json_loader</li> <li> jsonl_loader</li> <li> loader_router</li> <li> parquet_loader</li> <li> toml_loader</li> <li> txt_loader</li> <li> yaml_loader</li> </ul> </li> <li> nb_helpers<ul> <li> ipython_utils</li> <li> uni_peeker</li> </ul> </li> <li> unibox</li> <li> utils<ul> <li> constants</li> <li> credentials_manager</li> <li> df_utils</li> <li> globals</li> <li> image_utils</li> <li> io_utils</li> <li> llm_api</li> <li> logger</li> <li> s3_client</li> <li> utils</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/unibox/","title":"unibox","text":""},{"location":"reference/unibox/#unibox","title":"unibox","text":"<p>unibox package.</p> <p>unibox provides unified interface for common file operations</p> <p>Modules:</p> <ul> <li> <code>backends</code>           \u2013            </li> <li> <code>cli</code>           \u2013            <p>Module that contains the command line application.</p> </li> <li> <code>debug</code>           \u2013            <p>Debugging utilities.</p> </li> <li> <code>loaders</code>           \u2013            </li> <li> <code>nb_helpers</code>           \u2013            </li> <li> <code>unibox</code>           \u2013            </li> <li> <code>utils</code>           \u2013            </li> </ul> <p>Classes:</p> <ul> <li> <code>UniLogger</code>           \u2013            <p>A logger that:</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>concurrent_loads</code>             \u2013              <p>Load multiple files concurrently.</p> </li> <li> <code>loads</code>             \u2013              <p>Load data from a file or dataset.</p> </li> <li> <code>ls</code>             \u2013              <p>List files in a directory or dataset.</p> </li> <li> <code>peeks</code>             \u2013              <p>Peek at the first n items of data in a structured way.</p> </li> <li> <code>presigns</code>             \u2013              <p>Generate a presigned URL for an S3 URI.</p> </li> <li> <code>saves</code>             \u2013              <p>Save data to a file or dataset.</p> </li> <li> <code>to_df</code>             \u2013              <p>Convert JSON-like input to a pandas DataFrame.</p> </li> </ul>"},{"location":"reference/unibox/#unibox.UniLogger","title":"UniLogger","text":"<pre><code>UniLogger(\n    output_dir: str = \"logs\",\n    file_suffix: str = \"log\",\n    verbose: bool = False,\n    logger_name: str = None,\n    write_log: bool = True,\n    shorten_levels: int = 2,\n    use_color: bool = True,\n)\n</code></pre> <p>A logger that: 1) Uses colorlog for console color. 2) Writes an optional log file without color. 3) Shows the caller's class and method. 4) Conditionally includes file paths for specific log levels. 5) Detects if console supports color and allows disabling colors.</p> <p>Methods:</p> <ul> <li> <code>log</code>             \u2013              <p>Log with custom formatting based on log level.</p> </li> </ul> Source code in <code>src/unibox/utils/logger.py</code> <pre><code>def __init__(\n    self,\n    output_dir: str = \"logs\",\n    file_suffix: str = \"log\",\n    verbose: bool = False,\n    logger_name: str = None,\n    write_log: bool = True,\n    shorten_levels: int = 2,  # how many path parts to show for debug logs\n    use_color: bool = True,  # manually enable/disable color\n):\n    self.verbose = verbose\n    self.write_log = write_log\n    self.shorten_levels = shorten_levels\n\n    # Determine if console supports color\n    self.supports_color = self._detect_color_support() if use_color else False\n\n    self.logger = logging.getLogger(logger_name if logger_name else self.__class__.__name__)\n    self.logger.setLevel(logging.DEBUG if verbose else logging.INFO)\n\n    # Prepare handlers\n    self.handlers = []\n\n    # Optional file handler\n    if self.write_log:\n        # Use resolved log directory if default output_dir is used\n        if output_dir == \"logs\":\n            log_dir = _resolve_log_dir()\n        else:\n            log_dir = output_dir\n\n        os.makedirs(log_dir, exist_ok=True)\n        log_file = os.path.join(log_dir, f\"{file_suffix}_{datetime.now().strftime('%Y%m%d')}.log\")\n        fh = logging.FileHandler(log_file, mode=\"a\", encoding=\"utf-8\")\n        self.handlers.append(fh)\n\n    # Console handler\n    ch = logging.StreamHandler(sys.stdout)\n    self.handlers.append(ch)\n\n    # Add handlers to logger\n    if not self.logger.hasHandlers():\n        for h in self.handlers:\n            self.logger.addHandler(h)\n\n    self._setup_formatters()\n</code></pre>"},{"location":"reference/unibox/#unibox.UniLogger.log","title":"log","text":"<pre><code>log(level_name: str, message: str)\n</code></pre> <p>Log with custom formatting based on log level.</p> Source code in <code>src/unibox/utils/logger.py</code> <pre><code>def log(self, level_name: str, message: str):\n    \"\"\"Log with custom formatting based on log level.\"\"\"\n    level = getattr(logging, level_name.upper(), logging.INFO)\n\n    # Skip frames to find real caller\n    caller_frame = inspect.currentframe().f_back.f_back\n    method_name = caller_frame.f_code.co_name\n    class_name = None\n\n    if \"self\" in caller_frame.f_locals:\n        class_name = caller_frame.f_locals[\"self\"].__class__.__name__\n\n    if class_name:\n        full_func_name = f\"{class_name}.{method_name}\"\n    else:\n        full_func_name = method_name\n\n    full_path = os.path.abspath(caller_frame.f_code.co_filename)\n    short_path = self._shorten_path(full_path, self.shorten_levels)\n    lineno = caller_frame.f_lineno\n\n    # Conditionally include the path for debug, warning, error, critical levels\n    if level in [logging.DEBUG, logging.WARNING, logging.ERROR, logging.CRITICAL]:\n        extra_path = f\" {short_path}:{lineno}\"\n    else:\n        extra_path = \"\"\n\n    # Provide custom fields in `extra`\n    extra = {\n        \"my_func\": full_func_name,\n        \"my_lineno\": lineno,\n        \"extra_path\": extra_path,\n    }\n\n    # Log using extra fields\n    self.logger.log(level, message, extra=extra)\n</code></pre>"},{"location":"reference/unibox/#unibox.concurrent_loads","title":"concurrent_loads","text":"<pre><code>concurrent_loads(\n    uris_list: List[str],\n    num_workers: int = 8,\n    debug_print: bool = True,\n) -&gt; List[Any]\n</code></pre> <p>Load multiple files concurrently.</p> Source code in <code>src/unibox/unibox.py</code> <pre><code>def concurrent_loads(uris_list: List[str], num_workers: int = 8, debug_print: bool = True) -&gt; List[Any]:\n    \"\"\"Load multiple files concurrently.\"\"\"\n    results = [None] * len(uris_list)\n    with ProcessPoolExecutor(max_workers=num_workers) as executor:\n        partial_load = partial(loads, debug_print=False)\n        future_to_idx = {executor.submit(partial_load, u): i for i, u in enumerate(uris_list)}\n\n        if debug_print:\n            futures_iter = tqdm(as_completed(future_to_idx), total=len(uris_list), desc=\"Loading concurrent\")\n        else:\n            futures_iter = as_completed(future_to_idx)\n\n        for future in futures_iter:\n            idx = future_to_idx[future]\n            try:\n                results[idx] = future.result()\n            except Exception as e:\n                logger.error(f\"Exception reading {uris_list[idx]}: {e}\")\n\n    missing = sum(r is None for r in results)\n    if missing &gt; 0:\n        logger.warning(f\"{missing} loads returned None.\")\n    return results\n</code></pre>"},{"location":"reference/unibox/#unibox.loads","title":"loads","text":"<pre><code>loads(\n    uri: Union[str, Path],\n    file: bool = False,\n    debug_print: bool = True,\n    **kwargs\n) -&gt; Any\n</code></pre> <p>Load data from a file or dataset.</p> <p>For HuggingFace URIs: - hf://owner/repo =&gt; loads as dataset - hf://owner/repo/path/to/file.ext =&gt; loads as file</p> <p>Parameters:</p> <ul> <li> <code>uri</code>               (<code>Union[str, Path]</code>)           \u2013            <p>Path or URI to load from</p> </li> <li> <code>file</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, return the local file path instead of parsing</p> </li> <li> <code>debug_print</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to print debug info</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional arguments passed to loader For HF datasets: split, streaming, revision, etc. For files: loader-specific arguments</p> </li> </ul> Source code in <code>src/unibox/unibox.py</code> <pre><code>def loads(uri: Union[str, Path], file: bool = False, debug_print: bool = True, **kwargs) -&gt; Any:\n    \"\"\"Load data from a file or dataset.\n\n    For HuggingFace URIs:\n    - hf://owner/repo =&gt; loads as dataset\n    - hf://owner/repo/path/to/file.ext =&gt; loads as file\n\n    Args:\n        uri: Path or URI to load from\n        file: If True, return the local file path instead of parsing\n        debug_print: Whether to print debug info\n        **kwargs: Additional arguments passed to loader\n            For HF datasets: split, streaming, revision, etc.\n            For files: loader-specific arguments\n    \"\"\"\n    if debug_print:\n        logger.info(f\"Loading from {uri}\")\n\n    # If file=True, just get the local path\n    if file:\n        backend = get_backend_for_uri(str(uri))\n        if backend is None:\n            raise ValueError(f\"No backend found for URI: {uri}\")\n        local_path = backend.download(str(uri), **kwargs)\n        resolved_path = Path(local_path).resolve(strict=False)\n        if not (resolved_path.exists() or os.path.lexists(str(resolved_path))):\n            raise FileNotFoundError(f\"File not found: {resolved_path}\")\n        return resolved_path\n\n    # Use the loader router to handle both dataset and file loading\n    return load_data(uri, loader_config=kwargs)\n</code></pre>"},{"location":"reference/unibox/#unibox.ls","title":"ls","text":"<pre><code>ls(\n    uri: Union[str, Path],\n    exts: Optional[List[str]] = None,\n    relative_unix: bool = False,\n    debug_print: bool = True,\n    **kwargs\n) -&gt; list[str]\n</code></pre> <p>List files in a directory or dataset.</p> Source code in <code>src/unibox/unibox.py</code> <pre><code>def ls(\n    uri: Union[str, Path],\n    exts: Optional[List[str]] = None,\n    relative_unix: bool = False,\n    debug_print: bool = True,\n    **kwargs,\n) -&gt; list[str]:\n    \"\"\"List files in a directory or dataset.\"\"\"\n    if debug_print:\n        logger.info(f\"Listing contents of {uri}\")\n\n    # Get the appropriate backend\n    backend = get_backend_for_uri(str(uri))\n    if backend is None:\n        raise ValueError(f\"No backend found for URI: {uri}\")\n\n    # List the files\n    return backend.ls(str(uri), exts=exts, relative_unix=relative_unix, **kwargs)\n</code></pre>"},{"location":"reference/unibox/#unibox.peeks","title":"peeks","text":"<pre><code>peeks(\n    data: Any, n: int = 3, console_print: bool = False\n) -&gt; Dict[str, Any]\n</code></pre> <p>Peek at the first n items of data in a structured way.</p> Source code in <code>src/unibox/unibox.py</code> <pre><code>def peeks(data: Any, n: int = 3, console_print: bool = False) -&gt; Dict[str, Any]:\n    \"\"\"Peek at the first n items of data in a structured way.\"\"\"\n    result: Dict[str, Any] = {}\n\n    if isinstance(data, pd.DataFrame):\n        result[\"type\"] = \"DataFrame\"\n        result[\"shape\"] = data.shape\n        result[\"columns\"] = list(data.columns)\n        result[\"head\"] = data.head(n).to_dict(\"records\")\n    elif isinstance(data, (list, tuple)):\n        result[\"type\"] = type(data).__name__\n        result[\"length\"] = len(data)\n        result[\"head\"] = data[:n]\n    elif isinstance(data, dict):\n        result[\"type\"] = \"dict\"\n        result[\"length\"] = len(data)\n        result[\"keys\"] = list(data.keys())[:n]\n        result[\"head\"] = dict(list(data.items())[:n])\n    else:\n        result[\"type\"] = type(data).__name__\n        result[\"value\"] = str(data)\n\n    if console_print:\n        import json\n\n        print(json.dumps(result, indent=2, default=str))\n\n    return result\n</code></pre>"},{"location":"reference/unibox/#unibox.presigns","title":"presigns","text":"<pre><code>presigns(\n    s3_uri: str | list[str], expiration: int = 604800\n) -&gt; str\n</code></pre> <p>Generate a presigned URL for an S3 URI.</p> Source code in <code>src/unibox/unibox.py</code> <pre><code>def presigns(s3_uri: str | list[str], expiration: int = 604800) -&gt; str:\n    \"\"\"Generate a presigned URL for an S3 URI.\"\"\"\n    global s3_client\n    if s3_client is None:\n        s3_client = S3Client()\n\n    if isinstance(s3_uri, list):\n        return [s3_client.generate_presigned_uri(uri, expiration=expiration) for uri in s3_uri]\n\n    return s3_client.generate_presigned_uri(s3_uri, expiration=expiration)\n</code></pre>"},{"location":"reference/unibox/#unibox.saves","title":"saves","text":"<pre><code>saves(\n    data: Any,\n    uri: Union[str, Path],\n    debug_print: bool = True,\n    create_dir: bool = True,\n    **kwargs\n) -&gt; None\n</code></pre> <p>Save data to a file or dataset.</p> <p>For HuggingFace URIs: - hf://owner/repo =&gt; saves as dataset - hf://owner/repo/path/to/file.ext =&gt; saves as file</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>Any</code>)           \u2013            <p>Data to save</p> </li> <li> <code>uri</code>               (<code>Union[str, Path]</code>)           \u2013            <p>Path or URI to save to</p> </li> <li> <code>debug_print</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to print debug info</p> </li> <li> <code>create_dir</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to create parent directories for local file paths</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional arguments passed to loader For HF datasets: split, private, etc. For files: loader-specific arguments</p> </li> </ul> Source code in <code>src/unibox/unibox.py</code> <pre><code>def saves(\n    data: Any,\n    uri: Union[str, Path],\n    debug_print: bool = True,\n    create_dir: bool = True,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Save data to a file or dataset.\n\n    For HuggingFace URIs:\n    - hf://owner/repo =&gt; saves as dataset\n    - hf://owner/repo/path/to/file.ext =&gt; saves as file\n\n    Args:\n        data: Data to save\n        uri: Path or URI to save to\n        debug_print: Whether to print debug info\n        create_dir: Whether to create parent directories for local file paths\n        **kwargs: Additional arguments passed to loader\n            For HF datasets: split, private, etc.\n            For files: loader-specific arguments\n    \"\"\"\n    if debug_print:\n        logger.info(f\"Saving to {uri}\")\n\n    if create_dir and \"://\" not in str(uri):\n        Path(uri).expanduser().parent.mkdir(parents=True, exist_ok=True)\n\n    # Get the appropriate loader\n    loader = get_loader_for_path(uri)\n    if loader is None:\n        raise ValueError(f\"No loader found for path: {uri}\")\n\n    # Save using the loader (it will handle both dataset and file cases)\n    loader.save(uri, data, loader_config=kwargs)\n</code></pre>"},{"location":"reference/unibox/#unibox.to_df","title":"to_df","text":"<pre><code>to_df(\n    data: Any,\n    dict_key_column: str = \"DICT_KEY\",\n    value_column: str = \"VALUE\",\n    flatten_sep: str = \"__\",\n    max_depth: int = 2,\n) -&gt; DataFrame\n</code></pre> <p>Convert JSON-like input to a pandas DataFrame.</p> <p>Supports dict, list of dicts, list of scalars, or a DataFrame.</p> Source code in <code>src/unibox/unibox.py</code> <pre><code>def to_df(\n    data: Any,\n    dict_key_column: str = \"DICT_KEY\",\n    value_column: str = \"VALUE\",\n    flatten_sep: str = \"__\",\n    max_depth: int = 2,\n) -&gt; pd.DataFrame:\n    \"\"\"Convert JSON-like input to a pandas DataFrame.\n\n    Supports dict, list of dicts, list of scalars, or a DataFrame.\n    \"\"\"\n    if isinstance(data, pd.DataFrame):\n        return data\n    if isinstance(data, (dict, list, tuple)):\n        return coerce_json_like_to_df(\n            data,\n            dict_key_column=dict_key_column,\n            value_column=value_column,\n            flatten_sep=flatten_sep,\n            max_depth=max_depth,\n        )\n    raise ValueError(\"to_df expects a dict, list/tuple, or DataFrame\")\n</code></pre>"},{"location":"reference/unibox/cli/","title":"unibox.cli","text":""},{"location":"reference/unibox/cli/#unibox.cli","title":"cli","text":"<p>Module that contains the command line application.</p> <p>Functions:</p> <ul> <li> <code>apply_credentials</code>             \u2013              <p>Apply credentials for AWS and Hugging Face.</p> </li> <li> <code>get_parser</code>             \u2013              <p>Return the CLI argument parser.</p> </li> <li> <code>main</code>             \u2013              <p>Run the main program.</p> </li> </ul>"},{"location":"reference/unibox/cli/#unibox.cli.apply_credentials","title":"apply_credentials","text":"<pre><code>apply_credentials(*services: str) -&gt; None\n</code></pre> <p>Apply credentials for AWS and Hugging Face.</p> <p>This function is called when the program starts. It hides the credentials for AWS and Hugging Face in a hidden directory.</p> Source code in <code>src/unibox/cli.py</code> <pre><code>def apply_credentials(*services: str) -&gt; None:\n    \"\"\"Apply credentials for AWS and Hugging Face.\n\n    This function is called when the program starts.\n    It hides the credentials for AWS and Hugging Face in a hidden directory.\n    \"\"\"\n    if not services:\n        services = [\"aws\", \"huggingface\"]\n    try:\n        print(f\"Applying credentials: {services}\")\n        _apply_credentials(*services)\n    except Exception as e:\n        print(f\"Error applying credentials: {e}\")\n        sys.exit(1)\n</code></pre>"},{"location":"reference/unibox/cli/#unibox.cli.get_parser","title":"get_parser","text":"<pre><code>get_parser() -&gt; ArgumentParser\n</code></pre> <p>Return the CLI argument parser.</p> Source code in <code>src/unibox/cli.py</code> <pre><code>def get_parser() -&gt; argparse.ArgumentParser:\n    \"\"\"Return the CLI argument parser.\"\"\"\n    parser = argparse.ArgumentParser(prog=\"unibox\")\n    parser.add_argument(\"-V\", \"--version\", action=\"version\", version=f\"%(prog)s {debug.get_version()}\")\n    parser.add_argument(\"--debug-info\", action=_DebugInfo, help=\"Print debug information.\")\n\n    subparsers = parser.add_subparsers(dest=\"command\", required=False)\n\n    # Add `apply-cred` command\n    subparsers.add_parser(\"apply-cred\", help=\"Apply credentials for AWS and Hugging Face. Hides them if not hidden\")\n    subparsers.add_parser(\"ac\", help=\"Alias for apply-cred\")\n\n    return parser\n</code></pre>"},{"location":"reference/unibox/cli/#unibox.cli.main","title":"main","text":"<pre><code>main(args: list[str] | None = None) -&gt; int\n</code></pre> <p>Run the main program.</p> <p>This function is executed when you type <code>unibox</code> or <code>python -m unibox</code>.</p> <p>Parameters:</p> <ul> <li> <code>args</code>               (<code>list[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Arguments passed from the command line.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>An exit code.</p> </li> </ul> Source code in <code>src/unibox/cli.py</code> <pre><code>def main(args: list[str] | None = None) -&gt; int:\n    \"\"\"Run the main program.\n\n    This function is executed when you type `unibox` or `python -m unibox`.\n\n    Parameters:\n        args: Arguments passed from the command line.\n\n    Returns:\n        An exit code.\n    \"\"\"\n    parser = get_parser()\n    opts = parser.parse_args(args=args)\n\n    if opts.command in (\"apply-cred\", \"ac\"):\n        apply_credentials()\n        return 0\n    if opts.command is None:\n        # No command provided - show help and return 0 (expected by test)\n        parser.print_help()\n        return 0\n\n    # Unknown command\n    parser.print_help()\n    return 1\n</code></pre>"},{"location":"reference/unibox/debug/","title":"unibox.debug","text":""},{"location":"reference/unibox/debug/#unibox.debug","title":"debug","text":"<p>Debugging utilities.</p> <p>Classes:</p> <ul> <li> <code>Environment</code>           \u2013            <p>Dataclass to store environment information.</p> </li> <li> <code>Package</code>           \u2013            <p>Dataclass describing a Python package.</p> </li> <li> <code>Variable</code>           \u2013            <p>Dataclass describing an environment variable.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>get_debug_info</code>             \u2013              <p>Get debug/environment information.</p> </li> <li> <code>get_version</code>             \u2013              <p>Get version of the given distribution.</p> </li> <li> <code>print_debug_info</code>             \u2013              <p>Print debug/environment information.</p> </li> </ul>"},{"location":"reference/unibox/debug/#unibox.debug.Environment","title":"Environment  <code>dataclass</code>","text":"<pre><code>Environment(\n    interpreter_name: str,\n    interpreter_version: str,\n    interpreter_path: str,\n    platform: str,\n    packages: list[Package],\n    variables: list[Variable],\n)\n</code></pre> <p>Dataclass to store environment information.</p> <p>Attributes:</p> <ul> <li> <code>interpreter_name</code>               (<code>str</code>)           \u2013            <p>Python interpreter name.</p> </li> <li> <code>interpreter_path</code>               (<code>str</code>)           \u2013            <p>Path to Python executable.</p> </li> <li> <code>interpreter_version</code>               (<code>str</code>)           \u2013            <p>Python interpreter version.</p> </li> <li> <code>packages</code>               (<code>list[Package]</code>)           \u2013            <p>Installed packages.</p> </li> <li> <code>platform</code>               (<code>str</code>)           \u2013            <p>Operating System.</p> </li> <li> <code>variables</code>               (<code>list[Variable]</code>)           \u2013            <p>Environment variables.</p> </li> </ul>"},{"location":"reference/unibox/debug/#unibox.debug.Environment.interpreter_name","title":"interpreter_name  <code>instance-attribute</code>","text":"<pre><code>interpreter_name: str\n</code></pre> <p>Python interpreter name.</p>"},{"location":"reference/unibox/debug/#unibox.debug.Environment.interpreter_path","title":"interpreter_path  <code>instance-attribute</code>","text":"<pre><code>interpreter_path: str\n</code></pre> <p>Path to Python executable.</p>"},{"location":"reference/unibox/debug/#unibox.debug.Environment.interpreter_version","title":"interpreter_version  <code>instance-attribute</code>","text":"<pre><code>interpreter_version: str\n</code></pre> <p>Python interpreter version.</p>"},{"location":"reference/unibox/debug/#unibox.debug.Environment.packages","title":"packages  <code>instance-attribute</code>","text":"<pre><code>packages: list[Package]\n</code></pre> <p>Installed packages.</p>"},{"location":"reference/unibox/debug/#unibox.debug.Environment.platform","title":"platform  <code>instance-attribute</code>","text":"<pre><code>platform: str\n</code></pre> <p>Operating System.</p>"},{"location":"reference/unibox/debug/#unibox.debug.Environment.variables","title":"variables  <code>instance-attribute</code>","text":"<pre><code>variables: list[Variable]\n</code></pre> <p>Environment variables.</p>"},{"location":"reference/unibox/debug/#unibox.debug.Package","title":"Package  <code>dataclass</code>","text":"<pre><code>Package(name: str, version: str)\n</code></pre> <p>Dataclass describing a Python package.</p> <p>Attributes:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Package name.</p> </li> <li> <code>version</code>               (<code>str</code>)           \u2013            <p>Package version.</p> </li> </ul>"},{"location":"reference/unibox/debug/#unibox.debug.Package.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>Package name.</p>"},{"location":"reference/unibox/debug/#unibox.debug.Package.version","title":"version  <code>instance-attribute</code>","text":"<pre><code>version: str\n</code></pre> <p>Package version.</p>"},{"location":"reference/unibox/debug/#unibox.debug.Variable","title":"Variable  <code>dataclass</code>","text":"<pre><code>Variable(name: str, value: str)\n</code></pre> <p>Dataclass describing an environment variable.</p> <p>Attributes:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Variable name.</p> </li> <li> <code>value</code>               (<code>str</code>)           \u2013            <p>Variable value.</p> </li> </ul>"},{"location":"reference/unibox/debug/#unibox.debug.Variable.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>Variable name.</p>"},{"location":"reference/unibox/debug/#unibox.debug.Variable.value","title":"value  <code>instance-attribute</code>","text":"<pre><code>value: str\n</code></pre> <p>Variable value.</p>"},{"location":"reference/unibox/debug/#unibox.debug.get_debug_info","title":"get_debug_info","text":"<pre><code>get_debug_info() -&gt; Environment\n</code></pre> <p>Get debug/environment information.</p> <p>Returns:</p> <ul> <li> <code>Environment</code>           \u2013            <p>Environment information.</p> </li> </ul> Source code in <code>src/unibox/debug.py</code> <pre><code>def get_debug_info() -&gt; Environment:\n    \"\"\"Get debug/environment information.\n\n    Returns:\n        Environment information.\n    \"\"\"\n    py_name, py_version = _interpreter_name_version()\n    packages = [\"unibox\"]\n    variables = [\"PYTHONPATH\", *[var for var in os.environ if var.startswith(\"UNIBOX\")]]\n    return Environment(\n        interpreter_name=py_name,\n        interpreter_version=py_version,\n        interpreter_path=sys.executable,\n        platform=platform.platform(),\n        variables=[Variable(var, val) for var in variables if (val := os.getenv(var))],\n        packages=[Package(pkg, get_version(pkg)) for pkg in packages],\n    )\n</code></pre>"},{"location":"reference/unibox/debug/#unibox.debug.get_version","title":"get_version","text":"<pre><code>get_version(dist: str = 'unibox') -&gt; str\n</code></pre> <p>Get version of the given distribution.</p> <p>Parameters:</p> <ul> <li> <code>dist</code>               (<code>str</code>, default:                   <code>'unibox'</code> )           \u2013            <p>A distribution name.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>A version number.</p> </li> </ul> Source code in <code>src/unibox/debug.py</code> <pre><code>def get_version(dist: str = \"unibox\") -&gt; str:\n    \"\"\"Get version of the given distribution.\n\n    Parameters:\n        dist: A distribution name.\n\n    Returns:\n        A version number.\n    \"\"\"\n    try:\n        return metadata.version(dist)\n    except metadata.PackageNotFoundError:\n        return \"0.0.0\"\n</code></pre>"},{"location":"reference/unibox/debug/#unibox.debug.print_debug_info","title":"print_debug_info","text":"<pre><code>print_debug_info() -&gt; None\n</code></pre> <p>Print debug/environment information.</p> Source code in <code>src/unibox/debug.py</code> <pre><code>def print_debug_info() -&gt; None:\n    \"\"\"Print debug/environment information.\"\"\"\n    info = get_debug_info()\n    print(f\"- __System__: {info.platform}\")\n    print(f\"- __Python__: {info.interpreter_name} {info.interpreter_version} ({info.interpreter_path})\")\n    print(\"- __Environment variables__:\")\n    for var in info.variables:\n        print(f\"  - `{var.name}`: `{var.value}`\")\n    print(\"- __Installed packages__:\")\n    for pkg in info.packages:\n        print(f\"  - `{pkg.name}` v{pkg.version}\")\n</code></pre>"},{"location":"reference/unibox/unibox/","title":"unibox.unibox","text":""},{"location":"reference/unibox/unibox/#unibox.unibox","title":"unibox","text":"<p>Functions:</p> <ul> <li> <code>concurrent_loads</code>             \u2013              <p>Load multiple files concurrently.</p> </li> <li> <code>load_file</code>             \u2013              <p>Load a file's contents as a string.</p> </li> <li> <code>loads</code>             \u2013              <p>Load data from a file or dataset.</p> </li> <li> <code>ls</code>             \u2013              <p>List files in a directory or dataset.</p> </li> <li> <code>peeks</code>             \u2013              <p>Peek at the first n items of data in a structured way.</p> </li> <li> <code>presigns</code>             \u2013              <p>Generate a presigned URL for an S3 URI.</p> </li> <li> <code>saves</code>             \u2013              <p>Save data to a file or dataset.</p> </li> <li> <code>to_df</code>             \u2013              <p>Convert JSON-like input to a pandas DataFrame.</p> </li> </ul>"},{"location":"reference/unibox/unibox/#unibox.unibox.concurrent_loads","title":"concurrent_loads","text":"<pre><code>concurrent_loads(\n    uris_list: List[str],\n    num_workers: int = 8,\n    debug_print: bool = True,\n) -&gt; List[Any]\n</code></pre> <p>Load multiple files concurrently.</p> Source code in <code>src/unibox/unibox.py</code> <pre><code>def concurrent_loads(uris_list: List[str], num_workers: int = 8, debug_print: bool = True) -&gt; List[Any]:\n    \"\"\"Load multiple files concurrently.\"\"\"\n    results = [None] * len(uris_list)\n    with ProcessPoolExecutor(max_workers=num_workers) as executor:\n        partial_load = partial(loads, debug_print=False)\n        future_to_idx = {executor.submit(partial_load, u): i for i, u in enumerate(uris_list)}\n\n        if debug_print:\n            futures_iter = tqdm(as_completed(future_to_idx), total=len(uris_list), desc=\"Loading concurrent\")\n        else:\n            futures_iter = as_completed(future_to_idx)\n\n        for future in futures_iter:\n            idx = future_to_idx[future]\n            try:\n                results[idx] = future.result()\n            except Exception as e:\n                logger.error(f\"Exception reading {uris_list[idx]}: {e}\")\n\n    missing = sum(r is None for r in results)\n    if missing &gt; 0:\n        logger.warning(f\"{missing} loads returned None.\")\n    return results\n</code></pre>"},{"location":"reference/unibox/unibox/#unibox.unibox.load_file","title":"load_file","text":"<pre><code>load_file(\n    uri: Union[str, Path],\n    debug_print: bool = True,\n    **kwargs\n) -&gt; str\n</code></pre> <p>Load a file's contents as a string.</p> Source code in <code>src/unibox/unibox.py</code> <pre><code>def load_file(uri: Union[str, Path], debug_print: bool = True, **kwargs) -&gt; str:\n    \"\"\"Load a file's contents as a string.\"\"\"\n    if debug_print:\n        logger.info(f\"Loading file from {uri}\")\n\n    # Get the appropriate backend\n    backend = get_backend_for_uri(str(uri))\n    if backend is None:\n        raise ValueError(f\"No backend found for URI: {uri}\")\n\n    # Download the file if needed\n    local_path = backend.download(str(uri), **kwargs)\n    resolved_path = Path(local_path).resolve(strict=False)\n    if not (resolved_path.exists() or os.path.lexists(str(resolved_path))):\n        raise FileNotFoundError(f\"File not found: {resolved_path}\")\n\n    # Load the file contents\n    with open(local_path, encoding=\"utf-8\") as f:\n        return f.read()\n</code></pre>"},{"location":"reference/unibox/unibox/#unibox.unibox.loads","title":"loads","text":"<pre><code>loads(\n    uri: Union[str, Path],\n    file: bool = False,\n    debug_print: bool = True,\n    **kwargs\n) -&gt; Any\n</code></pre> <p>Load data from a file or dataset.</p> <p>For HuggingFace URIs: - hf://owner/repo =&gt; loads as dataset - hf://owner/repo/path/to/file.ext =&gt; loads as file</p> <p>Parameters:</p> <ul> <li> <code>uri</code>               (<code>Union[str, Path]</code>)           \u2013            <p>Path or URI to load from</p> </li> <li> <code>file</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, return the local file path instead of parsing</p> </li> <li> <code>debug_print</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to print debug info</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional arguments passed to loader For HF datasets: split, streaming, revision, etc. For files: loader-specific arguments</p> </li> </ul> Source code in <code>src/unibox/unibox.py</code> <pre><code>def loads(uri: Union[str, Path], file: bool = False, debug_print: bool = True, **kwargs) -&gt; Any:\n    \"\"\"Load data from a file or dataset.\n\n    For HuggingFace URIs:\n    - hf://owner/repo =&gt; loads as dataset\n    - hf://owner/repo/path/to/file.ext =&gt; loads as file\n\n    Args:\n        uri: Path or URI to load from\n        file: If True, return the local file path instead of parsing\n        debug_print: Whether to print debug info\n        **kwargs: Additional arguments passed to loader\n            For HF datasets: split, streaming, revision, etc.\n            For files: loader-specific arguments\n    \"\"\"\n    if debug_print:\n        logger.info(f\"Loading from {uri}\")\n\n    # If file=True, just get the local path\n    if file:\n        backend = get_backend_for_uri(str(uri))\n        if backend is None:\n            raise ValueError(f\"No backend found for URI: {uri}\")\n        local_path = backend.download(str(uri), **kwargs)\n        resolved_path = Path(local_path).resolve(strict=False)\n        if not (resolved_path.exists() or os.path.lexists(str(resolved_path))):\n            raise FileNotFoundError(f\"File not found: {resolved_path}\")\n        return resolved_path\n\n    # Use the loader router to handle both dataset and file loading\n    return load_data(uri, loader_config=kwargs)\n</code></pre>"},{"location":"reference/unibox/unibox/#unibox.unibox.ls","title":"ls","text":"<pre><code>ls(\n    uri: Union[str, Path],\n    exts: Optional[List[str]] = None,\n    relative_unix: bool = False,\n    debug_print: bool = True,\n    **kwargs\n) -&gt; list[str]\n</code></pre> <p>List files in a directory or dataset.</p> Source code in <code>src/unibox/unibox.py</code> <pre><code>def ls(\n    uri: Union[str, Path],\n    exts: Optional[List[str]] = None,\n    relative_unix: bool = False,\n    debug_print: bool = True,\n    **kwargs,\n) -&gt; list[str]:\n    \"\"\"List files in a directory or dataset.\"\"\"\n    if debug_print:\n        logger.info(f\"Listing contents of {uri}\")\n\n    # Get the appropriate backend\n    backend = get_backend_for_uri(str(uri))\n    if backend is None:\n        raise ValueError(f\"No backend found for URI: {uri}\")\n\n    # List the files\n    return backend.ls(str(uri), exts=exts, relative_unix=relative_unix, **kwargs)\n</code></pre>"},{"location":"reference/unibox/unibox/#unibox.unibox.peeks","title":"peeks","text":"<pre><code>peeks(\n    data: Any, n: int = 3, console_print: bool = False\n) -&gt; Dict[str, Any]\n</code></pre> <p>Peek at the first n items of data in a structured way.</p> Source code in <code>src/unibox/unibox.py</code> <pre><code>def peeks(data: Any, n: int = 3, console_print: bool = False) -&gt; Dict[str, Any]:\n    \"\"\"Peek at the first n items of data in a structured way.\"\"\"\n    result: Dict[str, Any] = {}\n\n    if isinstance(data, pd.DataFrame):\n        result[\"type\"] = \"DataFrame\"\n        result[\"shape\"] = data.shape\n        result[\"columns\"] = list(data.columns)\n        result[\"head\"] = data.head(n).to_dict(\"records\")\n    elif isinstance(data, (list, tuple)):\n        result[\"type\"] = type(data).__name__\n        result[\"length\"] = len(data)\n        result[\"head\"] = data[:n]\n    elif isinstance(data, dict):\n        result[\"type\"] = \"dict\"\n        result[\"length\"] = len(data)\n        result[\"keys\"] = list(data.keys())[:n]\n        result[\"head\"] = dict(list(data.items())[:n])\n    else:\n        result[\"type\"] = type(data).__name__\n        result[\"value\"] = str(data)\n\n    if console_print:\n        import json\n\n        print(json.dumps(result, indent=2, default=str))\n\n    return result\n</code></pre>"},{"location":"reference/unibox/unibox/#unibox.unibox.presigns","title":"presigns","text":"<pre><code>presigns(\n    s3_uri: str | list[str], expiration: int = 604800\n) -&gt; str\n</code></pre> <p>Generate a presigned URL for an S3 URI.</p> Source code in <code>src/unibox/unibox.py</code> <pre><code>def presigns(s3_uri: str | list[str], expiration: int = 604800) -&gt; str:\n    \"\"\"Generate a presigned URL for an S3 URI.\"\"\"\n    global s3_client\n    if s3_client is None:\n        s3_client = S3Client()\n\n    if isinstance(s3_uri, list):\n        return [s3_client.generate_presigned_uri(uri, expiration=expiration) for uri in s3_uri]\n\n    return s3_client.generate_presigned_uri(s3_uri, expiration=expiration)\n</code></pre>"},{"location":"reference/unibox/unibox/#unibox.unibox.saves","title":"saves","text":"<pre><code>saves(\n    data: Any,\n    uri: Union[str, Path],\n    debug_print: bool = True,\n    create_dir: bool = True,\n    **kwargs\n) -&gt; None\n</code></pre> <p>Save data to a file or dataset.</p> <p>For HuggingFace URIs: - hf://owner/repo =&gt; saves as dataset - hf://owner/repo/path/to/file.ext =&gt; saves as file</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>Any</code>)           \u2013            <p>Data to save</p> </li> <li> <code>uri</code>               (<code>Union[str, Path]</code>)           \u2013            <p>Path or URI to save to</p> </li> <li> <code>debug_print</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to print debug info</p> </li> <li> <code>create_dir</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to create parent directories for local file paths</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional arguments passed to loader For HF datasets: split, private, etc. For files: loader-specific arguments</p> </li> </ul> Source code in <code>src/unibox/unibox.py</code> <pre><code>def saves(\n    data: Any,\n    uri: Union[str, Path],\n    debug_print: bool = True,\n    create_dir: bool = True,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Save data to a file or dataset.\n\n    For HuggingFace URIs:\n    - hf://owner/repo =&gt; saves as dataset\n    - hf://owner/repo/path/to/file.ext =&gt; saves as file\n\n    Args:\n        data: Data to save\n        uri: Path or URI to save to\n        debug_print: Whether to print debug info\n        create_dir: Whether to create parent directories for local file paths\n        **kwargs: Additional arguments passed to loader\n            For HF datasets: split, private, etc.\n            For files: loader-specific arguments\n    \"\"\"\n    if debug_print:\n        logger.info(f\"Saving to {uri}\")\n\n    if create_dir and \"://\" not in str(uri):\n        Path(uri).expanduser().parent.mkdir(parents=True, exist_ok=True)\n\n    # Get the appropriate loader\n    loader = get_loader_for_path(uri)\n    if loader is None:\n        raise ValueError(f\"No loader found for path: {uri}\")\n\n    # Save using the loader (it will handle both dataset and file cases)\n    loader.save(uri, data, loader_config=kwargs)\n</code></pre>"},{"location":"reference/unibox/unibox/#unibox.unibox.to_df","title":"to_df","text":"<pre><code>to_df(\n    data: Any,\n    dict_key_column: str = \"DICT_KEY\",\n    value_column: str = \"VALUE\",\n    flatten_sep: str = \"__\",\n    max_depth: int = 2,\n) -&gt; DataFrame\n</code></pre> <p>Convert JSON-like input to a pandas DataFrame.</p> <p>Supports dict, list of dicts, list of scalars, or a DataFrame.</p> Source code in <code>src/unibox/unibox.py</code> <pre><code>def to_df(\n    data: Any,\n    dict_key_column: str = \"DICT_KEY\",\n    value_column: str = \"VALUE\",\n    flatten_sep: str = \"__\",\n    max_depth: int = 2,\n) -&gt; pd.DataFrame:\n    \"\"\"Convert JSON-like input to a pandas DataFrame.\n\n    Supports dict, list of dicts, list of scalars, or a DataFrame.\n    \"\"\"\n    if isinstance(data, pd.DataFrame):\n        return data\n    if isinstance(data, (dict, list, tuple)):\n        return coerce_json_like_to_df(\n            data,\n            dict_key_column=dict_key_column,\n            value_column=value_column,\n            flatten_sep=flatten_sep,\n            max_depth=max_depth,\n        )\n    raise ValueError(\"to_df expects a dict, list/tuple, or DataFrame\")\n</code></pre>"},{"location":"reference/unibox/backends/","title":"unibox.backends","text":""},{"location":"reference/unibox/backends/#unibox.backends","title":"backends","text":"<p>Modules:</p> <ul> <li> <code>backend_router</code>           \u2013            </li> <li> <code>base_backend</code>           \u2013            </li> <li> <code>hf_hybrid_backend</code>           \u2013            </li> <li> <code>http_backend</code>           \u2013            </li> <li> <code>local_backend</code>           \u2013            </li> <li> <code>s3_backend</code>           \u2013            </li> </ul>"},{"location":"reference/unibox/backends/backend_router/","title":"unibox.backends.backend_router","text":""},{"location":"reference/unibox/backends/backend_router/#unibox.backends.backend_router","title":"backend_router","text":"<p>Functions:</p> <ul> <li> <code>get_backend_for_uri</code>             \u2013              <p>Get the appropriate backend for the given URI.</p> </li> </ul>"},{"location":"reference/unibox/backends/backend_router/#unibox.backends.backend_router.get_backend_for_uri","title":"get_backend_for_uri","text":"<pre><code>get_backend_for_uri(uri: str) -&gt; BaseBackend\n</code></pre> <p>Get the appropriate backend for the given URI.</p> <p>Parameters:</p> <ul> <li> <code>uri</code>               (<code>str</code>)           \u2013            <p>The URI to check. Can be a local path, S3 URI, Hugging Face URI, or URL.</p> </li> </ul> Source code in <code>src/unibox/backends/backend_router.py</code> <pre><code>def get_backend_for_uri(uri: str) -&gt; BaseBackend:\n    \"\"\"Get the appropriate backend for the given URI.\n\n    Args:\n        uri (str): The URI to check. Can be a local path, S3 URI, Hugging Face URI, or URL.\n    \"\"\"\n    if is_s3_uri(uri):\n        return S3Backend()\n\n    if is_hf_uri(uri):\n        return HuggingfaceHybridBackend()\n\n    if is_url(uri):\n        # Use HTTPBackend for HTTP/HTTPS URLs\n        return HTTPBackend()\n\n    return LocalBackend()\n</code></pre>"},{"location":"reference/unibox/backends/base_backend/","title":"unibox.backends.base_backend","text":""},{"location":"reference/unibox/backends/base_backend/#unibox.backends.base_backend","title":"base_backend","text":"<p>Classes:</p> <ul> <li> <code>BaseBackend</code>           \u2013            <p>Interface for storage backends (local, S3, etc.).</p> </li> </ul>"},{"location":"reference/unibox/backends/base_backend/#unibox.backends.base_backend.BaseBackend","title":"BaseBackend","text":"<p>Interface for storage backends (local, S3, etc.).</p> <p>Methods:</p> <ul> <li> <code>download</code>             \u2013              <p>Download the resource identified by <code>uri</code> to a local temp path.</p> </li> <li> <code>ls</code>             \u2013              <p>List files under <code>uri</code> with optional extension filtering.</p> </li> <li> <code>upload</code>             \u2013              <p>Upload local_path to the specified <code>uri</code>.</p> </li> </ul>"},{"location":"reference/unibox/backends/base_backend/#unibox.backends.base_backend.BaseBackend.download","title":"download","text":"<pre><code>download(\n    uri: str, target_dir: Optional[str] = None\n) -&gt; Path\n</code></pre> <p>Download the resource identified by <code>uri</code> to a local temp path.</p> <p>Parameters:</p> <ul> <li> <code>uri</code>               (<code>str</code>)           \u2013            <p>URI of the resource to download</p> </li> <li> <code>target_dir</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional directory to download to. If None, uses a temp directory.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Path</code> (              <code>Path</code> )          \u2013            <p>Local path to the downloaded resource</p> </li> </ul> Source code in <code>src/unibox/backends/base_backend.py</code> <pre><code>def download(self, uri: str, target_dir: Optional[str] = None) -&gt; Path:\n    \"\"\"Download the resource identified by `uri` to a local temp path.\n\n    Args:\n        uri: URI of the resource to download\n        target_dir: Optional directory to download to. If None, uses a temp directory.\n\n    Returns:\n        Path: Local path to the downloaded resource\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/unibox/backends/base_backend/#unibox.backends.base_backend.BaseBackend.ls","title":"ls","text":"<pre><code>ls(\n    uri: str,\n    exts: Optional[List[str]] = None,\n    relative_unix: bool = False,\n    debug_print: bool = True,\n    **kwargs\n) -&gt; List[str]\n</code></pre> <p>List files under <code>uri</code> with optional extension filtering.</p> <p>Parameters:</p> <ul> <li> <code>uri</code>               (<code>str</code>)           \u2013            <p>A string representing a directory path or location.</p> </li> <li> <code>exts</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>A list of file extensions to include (['.txt', '.csv']).</p> </li> <li> <code>relative_unix</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Return relative paths with forward slashes if True.</p> </li> <li> <code>debug_print</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Show progress bar.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional arguments for compatibility (ignored by default).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List[str]: A list of file paths.</p> </li> </ul> Source code in <code>src/unibox/backends/base_backend.py</code> <pre><code>def ls(\n    self,\n    uri: str,\n    exts: Optional[List[str]] = None,\n    relative_unix: bool = False,\n    debug_print: bool = True,\n    **kwargs,\n) -&gt; List[str]:\n    \"\"\"List files under `uri` with optional extension filtering.\n\n    Args:\n        uri: A string representing a directory path or location.\n        exts: A list of file extensions to include (['.txt', '.csv']).\n        relative_unix: Return relative paths with forward slashes if True.\n        debug_print: Show progress bar.\n        **kwargs: Additional arguments for compatibility (ignored by default).\n\n    Returns:\n        List[str]: A list of file paths.\n    \"\"\"\n    include_extensions = kwargs.pop(\"include_extensions\", None)\n    if include_extensions is not None:\n        warnings.warn(\n            \"`include_extensions` is deprecated; use `exts` instead.\",\n            category=DeprecationWarning,\n            stacklevel=2,\n        )\n        exts = include_extensions\n\n    # By default, raise NotImplementedError.\n    # LocalBackend or other backends can override with real logic.\n    raise NotImplementedError(\"ls() is not implemented in BaseBackend.\")\n</code></pre>"},{"location":"reference/unibox/backends/base_backend/#unibox.backends.base_backend.BaseBackend.upload","title":"upload","text":"<pre><code>upload(local_path: Path, uri: str) -&gt; None\n</code></pre> <p>Upload local_path to the specified <code>uri</code>.</p> Source code in <code>src/unibox/backends/base_backend.py</code> <pre><code>def upload(self, local_path: Path, uri: str) -&gt; None:\n    \"\"\"Upload local_path to the specified `uri`.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/unibox/backends/hf_hybrid_backend/","title":"unibox.backends.hf_hybrid_backend","text":""},{"location":"reference/unibox/backends/hf_hybrid_backend/#unibox.backends.hf_hybrid_backend","title":"hf_hybrid_backend","text":"<p>Classes:</p> <ul> <li> <code>HuggingfaceHybridBackend</code>           \u2013            <p>A backend that uses low-level HfApi to handle single-file or folder usage in HF repos.</p> </li> </ul>"},{"location":"reference/unibox/backends/hf_hybrid_backend/#unibox.backends.hf_hybrid_backend.HuggingfaceHybridBackend","title":"HuggingfaceHybridBackend","text":"<pre><code>HuggingfaceHybridBackend()\n</code></pre> <p>               Bases: <code>BaseBackend</code></p> <p>A backend that uses low-level HfApi to handle single-file or folder usage in HF repos.</p> It can <ul> <li>download a single file (download)</li> <li>upload a single file (upload)</li> <li>list files in a repo (ls)</li> </ul> <p>Methods:</p> <ul> <li> <code>cp_to_hf</code>             \u2013              <p>Copy (upload) a local file to a Hugging Face repository.</p> </li> <li> <code>cp_to_local</code>             \u2013              <p>Copy (download) a file from a Hugging Face repository to a local path.</p> </li> <li> <code>download</code>             \u2013              <p>Download a single file from a HF repo to <code>target_dir</code>.</p> </li> <li> <code>load_file</code>             \u2013              <p>Download a single file from the HF repo and return the local path.</p> </li> <li> <code>ls</code>             \u2013              <p>List all files in the HF repo. If path_in_repo is a subfolder prefix, we can filter.</p> </li> <li> <code>update_readme</code>             \u2013              <p>Overwrite ONLY the 'hfbackend_statistics' region in the existing README.md</p> </li> <li> <code>upload</code>             \u2013              <p>Upload a single local file to HF at the given subpath in repo.</p> </li> </ul> Source code in <code>src/unibox/backends/hf_hybrid_backend.py</code> <pre><code>def __init__(self):\n    self.api = HfApi()\n</code></pre>"},{"location":"reference/unibox/backends/hf_hybrid_backend/#unibox.backends.hf_hybrid_backend.HuggingfaceHybridBackend.cp_to_hf","title":"cp_to_hf","text":"<pre><code>cp_to_hf(\n    local_file_path: str, hf_uri: str, private: bool = True\n)\n</code></pre> <p>Copy (upload) a local file to a Hugging Face repository. (Provided for reference; 'upload()' is the simpler approach.)</p> Source code in <code>src/unibox/backends/hf_hybrid_backend.py</code> <pre><code>def cp_to_hf(self, local_file_path: str, hf_uri: str, private: bool = True):\n    \"\"\"Copy (upload) a local file to a Hugging Face repository.\n    (Provided for reference; 'upload()' is the simpler approach.)\n    \"\"\"\n    parts = parse_hf_uri(hf_uri)\n    repo_id = parts.repo_id\n    path_in_repo = parts.path_in_repo\n    repo_type = parts.repo_type\n    if hf_uri.endswith(\"/\") or not os.path.basename(path_in_repo):\n        path_in_repo = path_in_repo.rstrip(\"/\") + \"/\" + os.path.basename(local_file_path)\n    self.api.create_repo(repo_id=repo_id, private=private, exist_ok=True, repo_type=repo_type)\n    self.api.upload_file(\n        path_or_fileobj=local_file_path,\n        path_in_repo=path_in_repo,\n        repo_id=repo_id,\n        repo_type=repo_type,\n    )\n    print(f\"cp {local_file_path} hf://{repo_id}/{path_in_repo}\")\n</code></pre>"},{"location":"reference/unibox/backends/hf_hybrid_backend/#unibox.backends.hf_hybrid_backend.HuggingfaceHybridBackend.cp_to_local","title":"cp_to_local","text":"<pre><code>cp_to_local(\n    hf_uri: str,\n    local_file_path: str,\n    revision: str = \"main\",\n)\n</code></pre> <p>Copy (download) a file from a Hugging Face repository to a local path. (Provided for reference; 'download()' is the simpler approach.)</p> Source code in <code>src/unibox/backends/hf_hybrid_backend.py</code> <pre><code>def cp_to_local(self, hf_uri: str, local_file_path: str, revision: str = \"main\"):\n    \"\"\"Copy (download) a file from a Hugging Face repository to a local path.\n    (Provided for reference; 'download()' is the simpler approach.)\n    \"\"\"\n    parts = parse_hf_uri(hf_uri)\n    repo_id = parts.repo_id\n    path_in_repo = parts.path_in_repo\n    repo_type = parts.repo_type\n    if os.path.isdir(local_file_path):\n        local_file_path = os.path.join(local_file_path, os.path.basename(path_in_repo))\n    effective_revision = revision if revision != \"main\" or parts.revision is None else parts.revision\n    cached_file_path = hf_hub_download(\n        repo_id=repo_id,\n        filename=path_in_repo,\n        revision=effective_revision,\n        repo_type=repo_type,\n    )\n    shutil.copy(cached_file_path, local_file_path)\n    print(f\"cp {hf_uri} {local_file_path}\")\n</code></pre>"},{"location":"reference/unibox/backends/hf_hybrid_backend/#unibox.backends.hf_hybrid_backend.HuggingfaceHybridBackend.download","title":"download","text":"<pre><code>download(\n    uri: str, target_dir: str | None = None\n) -&gt; Path | str\n</code></pre> <p>Download a single file from a HF repo to <code>target_dir</code>. If the path_in_repo is actually a folder or there's no final file, we raise NotImplemented.</p> Source code in <code>src/unibox/backends/hf_hybrid_backend.py</code> <pre><code>def download(self, uri: str, target_dir: str | None = None) -&gt; Path | str:\n    \"\"\"Download a single file from a HF repo to `target_dir`.\n    If the path_in_repo is actually a folder or there's no final file, we raise NotImplemented.\n    \"\"\"\n    if not uri.startswith(HF_PREFIX):\n        raise ValueError(f\"Invalid HF URI: {uri}\")\n    parts = parse_hf_uri(uri)\n    repo_id = parts.repo_id\n    path_in_repo = parts.path_in_repo\n    repo_type = parts.repo_type\n    revision = parts.revision or \"main\"\n    if not path_in_repo:\n        # do nothing, let loader handle it (load dataset as hf://.../)\n        logger.info(f\"{uri}: is a Huggingface dataset; skipping download.\")\n        return uri\n\n    if not target_dir:\n        target_dir = tempfile.gettempdir()\n    os.makedirs(target_dir, exist_ok=True)\n\n    # Download\n    if repo_type == \"model\":\n        try:\n            local_path = hf_hub_download(repo_id=repo_id, filename=path_in_repo, revision=revision)\n        except RepositoryNotFoundError:\n            logger.info(f\"{uri}: is not a model repo; trying to download as dataset...\")\n            try:\n                local_path = hf_hub_download(\n                    repo_id=repo_id,\n                    filename=path_in_repo,\n                    revision=revision,\n                    repo_type=\"dataset\",\n                )\n            except RepositoryNotFoundError:\n                raise ValueError(f\"File not found in HF repo: {uri}\")\n    else:\n        local_path = hf_hub_download(\n            repo_id=repo_id,\n            filename=path_in_repo,\n            revision=revision,\n            repo_type=repo_type,\n        )\n\n    # Copy from HF cache to target_dir if needed\n    filename_only = os.path.basename(path_in_repo)\n    final_path = Path(target_dir) / filename_only\n    shutil.copy(local_path, final_path)\n    return final_path\n</code></pre>"},{"location":"reference/unibox/backends/hf_hybrid_backend/#unibox.backends.hf_hybrid_backend.HuggingfaceHybridBackend.load_file","title":"load_file","text":"<pre><code>load_file(hf_uri: str, revision: str = 'main') -&gt; str\n</code></pre> <p>Download a single file from the HF repo and return the local path.</p> Source code in <code>src/unibox/backends/hf_hybrid_backend.py</code> <pre><code>def load_file(self, hf_uri: str, revision: str = \"main\") -&gt; str:\n    \"\"\"Download a single file from the HF repo and return the local path.\"\"\"\n    parts = parse_hf_uri(hf_uri)\n    repo_id = parts.repo_id\n    path_in_repo = parts.path_in_repo\n    repo_type = parts.repo_type\n    effective_revision = revision if revision != \"main\" or parts.revision is None else parts.revision\n    local_path = hf_hub_download(\n        repo_id=repo_id,\n        filename=path_in_repo,\n        revision=effective_revision,\n        repo_type=repo_type,\n    )\n    print(f\"load_file {hf_uri} -&gt; {local_path}\")\n    return local_path\n</code></pre>"},{"location":"reference/unibox/backends/hf_hybrid_backend/#unibox.backends.hf_hybrid_backend.HuggingfaceHybridBackend.ls","title":"ls","text":"<pre><code>ls(\n    uri: str,\n    exts: Optional[List[str]] = None,\n    relative_unix: bool = False,\n    debug_print: bool = True,\n    **kwargs\n) -&gt; List[str]\n</code></pre> <p>List all files in the HF repo. If path_in_repo is a subfolder prefix, we can filter. For extension filtering or subpath filtering, you'd manually do it. Here we do a simple approach.</p> Source code in <code>src/unibox/backends/hf_hybrid_backend.py</code> <pre><code>def ls(\n    self,\n    uri: str,\n    exts: Optional[List[str]] = None,\n    relative_unix: bool = False,\n    debug_print: bool = True,\n    **kwargs,\n) -&gt; List[str]:\n    \"\"\"List all files in the HF repo. If path_in_repo is a subfolder prefix, we can filter.\n    For extension filtering or subpath filtering, you'd manually do it. Here we do a simple approach.\n    \"\"\"\n    parts = parse_hf_uri(uri)\n    repo_id = parts.repo_id\n    path_in_repo = parts.path_in_repo\n    repo_type = parts.repo_type\n    try:\n        if repo_type == \"model\":\n            files = self.api.list_repo_files(repo_id=repo_id)\n        else:\n            files = self.api.list_repo_files(repo_id=repo_id, repo_type=repo_type)\n    except RepositoryNotFoundError:\n        logger.info(f\"{uri}: is not a model repo; trying to list as dataset...\")\n        files = self.api.list_repo_files(repo_id=repo_id, repo_type=\"dataset\")\n\n    # If path_in_repo is not empty, we can filter by that prefix\n    if path_in_repo:\n        path_in_repo = path_in_repo.rstrip(\"/\")\n        files = [f for f in files if f.startswith(path_in_repo)]\n\n    # If exts is given, filter\n    if exts:\n        exts = [e.lower() for e in exts]\n        files = [f for f in files if any(f.lower().endswith(x) for x in exts)]\n\n    # Possibly convert to relative or restore full \"hf://...\"\n    results = []\n    for f in files:\n        if relative_unix:\n            # show subpath relative\n            sub = f[len(path_in_repo) :].lstrip(\"/\") if path_in_repo else f\n            sub = sub.replace(\"\\\\\", \"/\")\n            results.append(sub)\n        else:\n            # return a full HF URI\n            results.append(f\"hf://{repo_id}/{f}\")\n    return results\n</code></pre>"},{"location":"reference/unibox/backends/hf_hybrid_backend/#unibox.backends.hf_hybrid_backend.HuggingfaceHybridBackend.update_readme","title":"update_readme","text":"<pre><code>update_readme(\n    repo_id: str,\n    new_stats_content: str,\n    commit_message: str = \"Update README.md\",\n    repo_type: str = \"dataset\",\n)\n</code></pre> <p>Overwrite ONLY the 'hfbackend_statistics' region in the existing README.md         (delimited by  and ).</p> <pre><code>    If the region does not exist, it is appended to the bottom of the non-YAML area.\n    The YAML block at the top of the file (---\n</code></pre>"},{"location":"reference/unibox/backends/hf_hybrid_backend/#unibox.backends.hf_hybrid_backend.HuggingfaceHybridBackend.update_readme--_1","title":"...","text":"<p>) is never overwritten.</p> <pre><code>    Args:\n        repo_id (str): The ID of the repo to update (e.g. \"org/repo\").\n        new_stats_content (str): The text that should appear in the 'statistics' block.\n        commit_message (str): The commit message to use for the update.\n        repo_type (str): The type of repo (\"dataset\", \"model\", or \"space\").\n</code></pre> Source code in <code>src/unibox/backends/hf_hybrid_backend.py</code> <pre><code>def update_readme(\n    self,\n    repo_id: str,\n    new_stats_content: str,\n    commit_message: str = \"Update README.md\",\n    repo_type: str = \"dataset\",\n):\n    \"\"\"Overwrite ONLY the 'hfbackend_statistics' region in the existing README.md\n    (delimited by &lt;!-- BEGIN ... --&gt; and &lt;!-- END ... --&gt;).\n\n    If the region does not exist, it is appended to the bottom of the non-YAML area.\n    The YAML block at the top of the file (---\\n...\\n---\\n) is never overwritten.\n\n    Args:\n        repo_id (str): The ID of the repo to update (e.g. \"org/repo\").\n        new_stats_content (str): The text that should appear in the 'statistics' block.\n        commit_message (str): The commit message to use for the update.\n        repo_type (str): The type of repo (\"dataset\", \"model\", or \"space\").\n    \"\"\"\n    logger.info(f\"Updating README.md in {repo_id} (repo_type={repo_type})\")\n\n    # ------------------------------------------------\n    # 1) Try to download the existing README.md content\n    # ------------------------------------------------\n    try:\n        existing_readme_path = hf_hub_download(\n            repo_id=repo_id,\n            filename=\"README.md\",\n            repo_type=repo_type,\n        )\n        with open(existing_readme_path, encoding=\"utf-8\") as f:\n            existing_text = f.read()\n    except Exception:\n        # If README.md does not exist or fails to download, treat as empty\n        existing_text = \"\"\n\n    # ------------------------------------------------\n    # 2) Separate out any YAML front-matter (--- ... ---)\n    #    so we never overwrite that automatically generated block.\n    # ------------------------------------------------\n    # This regex looks for the very first '---' then the next '---', capturing everything in between\n    # including newlines. We allow for zero or more lines in the middle.\n    yaml_pattern = re.compile(r\"^---\\s*\\n.*?\\n---\\s*\\n\", flags=re.DOTALL | re.MULTILINE)\n\n    yaml_match = yaml_pattern.search(existing_text)\n    if yaml_match:\n        yaml_block = yaml_match.group(0)\n        rest_of_file = existing_text[yaml_match.end() :]\n    else:\n        yaml_block = \"\"\n        rest_of_file = existing_text\n\n    # ------------------------------------------------\n    # 3) Build the new block we want to insert/replace\n    # ------------------------------------------------\n    # For clarity, we create a single string that includes the markers and the user stats content\n    statistics_block = f\"{START_MARKER}\\n{new_stats_content}\\n{END_MARKER}\"\n\n    # ------------------------------------------------\n    # 4) Search in 'rest_of_file' for an existing block with those markers\n    # ------------------------------------------------\n    block_pattern = re.compile(\n        re.escape(START_MARKER) + r\".*?\" + re.escape(END_MARKER),\n        flags=re.DOTALL,\n    )\n    if block_pattern.search(rest_of_file):\n        # Replace whatever is between the existing markers with the new text\n        new_rest_of_file = block_pattern.sub(statistics_block, rest_of_file)\n    else:\n        # Markers don't exist -&gt; append at the end of the rest_of_file\n        # (with a little spacing to ensure we start on a new line).\n        if not rest_of_file.endswith(\"\\n\"):\n            rest_of_file += \"\\n\"\n        new_rest_of_file = rest_of_file + \"\\n\" + statistics_block + \"\\n\"\n\n    # ------------------------------------------------\n    # 5) Reassemble the final text\n    # ------------------------------------------------\n    updated_readme = yaml_block + new_rest_of_file\n\n    # ------------------------------------------------\n    # 6) Write new content to a temp file, then upload to HF\n    # ------------------------------------------------\n    with tempfile.NamedTemporaryFile(mode=\"w\", delete=False, encoding=\"utf-8\") as tmpf:\n        tmpf.write(updated_readme)\n        temp_path = tmpf.name\n\n    # Make sure the repo exists\n    self.api.create_repo(repo_id=repo_id, private=True, exist_ok=True, repo_type=repo_type)\n\n    # Now upload the new README\n    try:\n        self.api.upload_file(\n            path_or_fileobj=temp_path,\n            path_in_repo=\"README.md\",\n            repo_id=repo_id,\n            repo_type=repo_type,\n            commit_message=commit_message,\n        )\n    finally:\n        # Cleanup local file\n        if os.path.exists(temp_path):\n            os.remove(temp_path)\n</code></pre>"},{"location":"reference/unibox/backends/hf_hybrid_backend/#unibox.backends.hf_hybrid_backend.HuggingfaceHybridBackend.upload","title":"upload","text":"<pre><code>upload(local_path: Path, uri: str) -&gt; None\n</code></pre> <p>Upload a single local file to HF at the given subpath in repo. If the subpath is empty =&gt; we treat that as 'folder'? Or raise error?</p> Source code in <code>src/unibox/backends/hf_hybrid_backend.py</code> <pre><code>def upload(self, local_path: Path, uri: str) -&gt; None:\n    \"\"\"Upload a single local file to HF at the given subpath in repo.\n    If the subpath is empty =&gt; we treat that as 'folder'? Or raise error?\n    \"\"\"\n    parts = parse_hf_uri(uri)\n    repo_id = parts.repo_id\n    path_in_repo = parts.path_in_repo\n    repo_type = parts.repo_type\n    if not path_in_repo or path_in_repo.endswith(\"/\"):\n        # user wants a directory push\n        path_in_repo = path_in_repo.rstrip(\"/\") + \"/\" + local_path.name\n\n    # Ensure repo exists\n    self.api.create_repo(repo_id=repo_id, private=True, exist_ok=True, repo_type=repo_type)\n    # Upload\n    self.api.upload_file(\n        path_or_fileobj=str(local_path),\n        path_in_repo=path_in_repo,\n        repo_id=repo_id,\n        repo_type=repo_type,\n    )\n</code></pre>"},{"location":"reference/unibox/backends/http_backend/","title":"unibox.backends.http_backend","text":""},{"location":"reference/unibox/backends/http_backend/#unibox.backends.http_backend","title":"http_backend","text":"<p>Classes:</p> <ul> <li> <code>HTTPBackend</code>           \u2013            <p>Backend for downloading files from HTTP/HTTPS URLs.</p> </li> </ul>"},{"location":"reference/unibox/backends/http_backend/#unibox.backends.http_backend.HTTPBackend","title":"HTTPBackend","text":"<pre><code>HTTPBackend()\n</code></pre> <p>               Bases: <code>BaseBackend</code></p> <p>Backend for downloading files from HTTP/HTTPS URLs.</p> <p>Methods:</p> <ul> <li> <code>download</code>             \u2013              <p>Download a file from HTTP/HTTPS URL to local storage.</p> </li> <li> <code>ls</code>             \u2013              <p>HTTP backend does not support directory listing.</p> </li> <li> <code>upload</code>             \u2013              <p>HTTP backend does not support upload operations.</p> </li> </ul> Source code in <code>src/unibox/backends/http_backend.py</code> <pre><code>def __init__(self):\n    self.logger = UniLogger()\n</code></pre>"},{"location":"reference/unibox/backends/http_backend/#unibox.backends.http_backend.HTTPBackend.download","title":"download","text":"<pre><code>download(\n    uri: str, target_dir: Optional[str] = None\n) -&gt; Path\n</code></pre> <p>Download a file from HTTP/HTTPS URL to local storage.</p> <p>Parameters:</p> <ul> <li> <code>uri</code>               (<code>str</code>)           \u2013            <p>HTTP/HTTPS URL of the file to download</p> </li> <li> <code>target_dir</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional directory to download to. If None, uses global temp directory.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Path</code> (              <code>Path</code> )          \u2013            <p>Local path to the downloaded file</p> </li> </ul> Source code in <code>src/unibox/backends/http_backend.py</code> <pre><code>def download(self, uri: str, target_dir: Optional[str] = None) -&gt; Path:\n    \"\"\"Download a file from HTTP/HTTPS URL to local storage.\n\n    Args:\n        uri: HTTP/HTTPS URL of the file to download\n        target_dir: Optional directory to download to. If None, uses global temp directory.\n\n    Returns:\n        Path: Local path to the downloaded file\n    \"\"\"\n    uri = self._validate_http_uri(uri)\n\n    from unibox.utils.globals import GLOBAL_TMP_DIR\n\n    _target_dir = target_dir or GLOBAL_TMP_DIR\n    abs_target_dir = Path(_target_dir).resolve()\n\n    # Security check: ensure target directory is not blacklisted\n    for blocked in self.BLACKLISTED_PATHS:\n        blocked_path = Path(blocked).resolve()\n        if abs_target_dir == blocked_path or str(abs_target_dir).startswith(str(blocked_path) + os.sep):\n            raise PermissionError(f\"Download blocked: {abs_target_dir} is a restricted path.\")\n\n    # Create target directory if it doesn't exist\n    os.makedirs(_target_dir, exist_ok=True)\n\n    # Generate local filename\n    filename = self._get_filename_from_uri(uri)\n    local_path = Path(_target_dir) / filename\n\n    # Check if file already exists (simple caching)\n    if local_path.exists():\n        self.logger.debug(f\"File already exists locally: {local_path}\")\n        return local_path\n\n    try:\n        self.logger.debug(f\"Downloading {uri} to {local_path}\")\n\n        # Download the file\n        urlretrieve(uri, str(local_path))\n\n        if not local_path.exists():\n            raise FileNotFoundError(f\"Download failed: {local_path} was not created\")\n\n        self.logger.info(f\"Successfully downloaded {uri}\")\n        return local_path\n\n    except Exception as e:\n        # Clean up partial download if it exists\n        if local_path.exists():\n            local_path.unlink()\n        raise RuntimeError(f\"Failed to download {uri}: {e}\")\n</code></pre>"},{"location":"reference/unibox/backends/http_backend/#unibox.backends.http_backend.HTTPBackend.ls","title":"ls","text":"<pre><code>ls(\n    uri: str,\n    exts: Optional[List[str]] = None,\n    relative_unix: bool = False,\n    debug_print: bool = True,\n    **kwargs\n) -&gt; List[str]\n</code></pre> <p>HTTP backend does not support directory listing.</p> <p>HTTP URLs point to individual files, not directories that can be listed.</p> Source code in <code>src/unibox/backends/http_backend.py</code> <pre><code>def ls(\n    self,\n    uri: str,\n    exts: Optional[List[str]] = None,\n    relative_unix: bool = False,\n    debug_print: bool = True,\n    **kwargs,\n) -&gt; List[str]:\n    \"\"\"HTTP backend does not support directory listing.\n\n    HTTP URLs point to individual files, not directories that can be listed.\n    \"\"\"\n    # Handle backward compatibility for include_extensions\n    include_extensions = kwargs.pop(\"include_extensions\", None)\n    if include_extensions is not None:\n        warnings.warn(\n            \"`include_extensions` is deprecated; use `exts` instead.\",\n            category=DeprecationWarning,\n            stacklevel=2,\n        )\n        exts = include_extensions\n\n    raise NotImplementedError(\n        \"HTTP backend does not support directory listing. HTTP URLs point to individual files, not directories.\",\n    )\n</code></pre>"},{"location":"reference/unibox/backends/http_backend/#unibox.backends.http_backend.HTTPBackend.upload","title":"upload","text":"<pre><code>upload(local_path: Path, uri: str) -&gt; None\n</code></pre> <p>HTTP backend does not support upload operations.</p> Source code in <code>src/unibox/backends/http_backend.py</code> <pre><code>def upload(self, local_path: Path, uri: str) -&gt; None:\n    \"\"\"HTTP backend does not support upload operations.\"\"\"\n    raise NotImplementedError(\"HTTP backend does not support upload operations. HTTP URLs are read-only.\")\n</code></pre>"},{"location":"reference/unibox/backends/local_backend/","title":"unibox.backends.local_backend","text":""},{"location":"reference/unibox/backends/local_backend/#unibox.backends.local_backend","title":"local_backend","text":"<p>Classes:</p> <ul> <li> <code>LocalBackend</code>           \u2013            </li> </ul>"},{"location":"reference/unibox/backends/local_backend/#unibox.backends.local_backend.LocalBackend","title":"LocalBackend","text":"<p>               Bases: <code>BaseBackend</code></p> <p>Methods:</p> <ul> <li> <code>ls</code>             \u2013              <p>Lists files in the local directory with optional extension filtering.</p> </li> <li> <code>upload</code>             \u2013              <p>Safely uploads a file while preventing path traversal and restricted locations.</p> </li> </ul>"},{"location":"reference/unibox/backends/local_backend/#unibox.backends.local_backend.LocalBackend.ls","title":"ls","text":"<pre><code>ls(\n    uri: str,\n    exts: Optional[List[str]] = None,\n    relative_unix: bool = False,\n    debug_print: bool = True,\n    **kwargs\n) -&gt; List[str]\n</code></pre> <p>Lists files in the local directory with optional extension filtering.</p> <p>Parameters:</p> <ul> <li> <code>uri</code>               (<code>str</code>)           \u2013            <p>Directory URI to list.</p> </li> <li> <code>exts</code>               (<code>List[str]</code>, default:                   <code>None</code> )           \u2013            <p>List of extensions to include. Defaults to None.</p> </li> <li> <code>relative_unix</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return relative Unix-style paths. Defaults to False.</p> </li> <li> <code>debug_print</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to display a progress bar. Defaults to True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List[str]: List of file paths.</p> </li> </ul> Source code in <code>src/unibox/backends/local_backend.py</code> <pre><code>def ls(\n    self,\n    uri: str,\n    exts: Optional[List[str]] = None,\n    relative_unix: bool = False,\n    debug_print: bool = True,\n    **kwargs,\n) -&gt; List[str]:\n    \"\"\"Lists files in the local directory with optional extension filtering.\n\n    Args:\n        uri (str): Directory URI to list.\n        exts (List[str], optional): List of extensions to include. Defaults to None.\n        relative_unix (bool, optional): Whether to return relative Unix-style paths. Defaults to False.\n        debug_print (bool, optional): Whether to display a progress bar. Defaults to True.\n\n    Returns:\n        List[str]: List of file paths.\n    \"\"\"\n    # Handle backward compatibility for `include_extensions`\n    include_extensions = kwargs.pop(\"include_extensions\", None)\n    if include_extensions is not None:\n        warnings.warn(\n            \"`include_extensions` is deprecated; use `exts` instead.\",\n            category=DeprecationWarning,\n            stacklevel=2,\n        )\n        exts = include_extensions\n\n    return self._traverse_local_dir(\n        root_dir=uri,\n        exts=exts,\n        relative_unix=relative_unix,\n        debug_print=debug_print,\n    )\n</code></pre>"},{"location":"reference/unibox/backends/local_backend/#unibox.backends.local_backend.LocalBackend.upload","title":"upload","text":"<pre><code>upload(local_path: Path, uri: str) -&gt; None\n</code></pre> <p>Safely uploads a file while preventing path traversal and restricted locations.</p> Source code in <code>src/unibox/backends/local_backend.py</code> <pre><code>def upload(self, local_path: Path, uri: str) -&gt; None:\n    \"\"\"Safely uploads a file while preventing path traversal and restricted locations.\"\"\"\n    dest = Path(uri).resolve()\n\n    # Prevent uploads to blacklisted paths or within system directories\n    for blocked in self.BLACKLISTED_PATHS:\n        if dest == Path(blocked) or dest.is_relative_to(Path(blocked)):\n            raise PermissionError(f\"Upload blocked: {dest} is a restricted path.\")\n\n    if dest != local_path:\n        # Ensure destination directory exists\n        dest.parent.mkdir(parents=True, exist_ok=True)\n        shutil.copy2(str(local_path), str(dest))\n</code></pre>"},{"location":"reference/unibox/backends/s3_backend/","title":"unibox.backends.s3_backend","text":""},{"location":"reference/unibox/backends/s3_backend/#unibox.backends.s3_backend","title":"s3_backend","text":"<p>Classes:</p> <ul> <li> <code>S3Backend</code>           \u2013            </li> </ul>"},{"location":"reference/unibox/backends/s3_backend/#unibox.backends.s3_backend.S3Backend","title":"S3Backend","text":"<pre><code>S3Backend()\n</code></pre> <p>               Bases: <code>BaseBackend</code></p> <p>Methods:</p> <ul> <li> <code>download</code>             \u2013              <p>Download the file from S3. If target_dir is given, place it there,</p> </li> <li> <code>ls</code>             \u2013              <p>List files in the S3 \"directory\" with optional extension filtering.</p> </li> <li> <code>upload</code>             \u2013              <p>Upload the local file to S3.</p> </li> </ul> Source code in <code>src/unibox/backends/s3_backend.py</code> <pre><code>def __init__(self):\n    self._client = S3Client()\n</code></pre>"},{"location":"reference/unibox/backends/s3_backend/#unibox.backends.s3_backend.S3Backend.download","title":"download","text":"<pre><code>download(uri: str, target_dir: str | None = None) -&gt; Path\n</code></pre> <p>Download the file from S3. If target_dir is given, place it there, else use your global or stable temp directory.</p> Source code in <code>src/unibox/backends/s3_backend.py</code> <pre><code>def download(self, uri: str, target_dir: str | None = None) -&gt; Path:\n    \"\"\"Download the file from S3. If target_dir is given, place it there,\n    else use your global or stable temp directory.\n    \"\"\"\n    uri = self._validate_s3_uri(uri)\n    from unibox.utils.globals import GLOBAL_TMP_DIR\n\n    _target_dir = target_dir or GLOBAL_TMP_DIR\n\n    abs_target_dir = Path(_target_dir).resolve()\n\n    # Fix: Ensure blacklisted directories and their subdirectories are blocked\n    for blocked in BLACKLISTED_PATHS:\n        blocked_path = Path(blocked).resolve()\n        if abs_target_dir == blocked_path or str(abs_target_dir).startswith(str(blocked_path) + os.sep):\n            raise PermissionError(f\"Download blocked: {abs_target_dir} is a restricted path.\")\n\n    os.makedirs(_target_dir, exist_ok=True)\n    local_path = self._client.download(uri, _target_dir)\n\n    return Path(local_path)\n</code></pre>"},{"location":"reference/unibox/backends/s3_backend/#unibox.backends.s3_backend.S3Backend.ls","title":"ls","text":"<pre><code>ls(\n    uri: str,\n    exts: Optional[List[str]] = None,\n    relative_unix: bool = False,\n    debug_print: bool = True,\n    **kwargs\n) -&gt; List[str]\n</code></pre> <p>List files in the S3 \"directory\" with optional extension filtering.</p> <p>Parameters:</p> <ul> <li> <code>uri</code>               (<code>str</code>)           \u2013            <p>S3 directory URI to list.</p> </li> <li> <code>exts</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>List of file extensions to include. Defaults to None.</p> </li> <li> <code>relative_unix</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return relative Unix-style paths. Defaults to False.</p> </li> <li> <code>debug_print</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to display a progress bar. Defaults to True.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional arguments for backward compatibility.       Supports 'include_extensions' (deprecated) and 'exclude_extensions'.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List[str]: List of file keys or full S3 URIs.</p> </li> </ul> Source code in <code>src/unibox/backends/s3_backend.py</code> <pre><code>def ls(\n    self,\n    uri: str,\n    exts: Optional[List[str]] = None,\n    relative_unix: bool = False,\n    debug_print: bool = True,\n    **kwargs,\n) -&gt; List[str]:\n    \"\"\"List files in the S3 \"directory\" with optional extension filtering.\n\n    Args:\n        uri (str): S3 directory URI to list.\n        exts (Optional[List[str]]): List of file extensions to include. Defaults to None.\n        relative_unix (bool): Whether to return relative Unix-style paths. Defaults to False.\n        debug_print (bool): Whether to display a progress bar. Defaults to True.\n        **kwargs: Additional arguments for backward compatibility.\n                  Supports 'include_extensions' (deprecated) and 'exclude_extensions'.\n\n    Returns:\n        List[str]: List of file keys or full S3 URIs.\n    \"\"\"\n    # Handle backward compatibility for include_extensions.\n    include_extensions = kwargs.pop(\"include_extensions\", None)\n    if include_extensions is not None:\n        warnings.warn(\n            \"`include_extensions` is deprecated; use `exts` instead.\",\n            category=DeprecationWarning,\n            stacklevel=2,\n        )\n        exts = include_extensions\n\n    # Also allow passing 'exclude_extensions' if needed.\n    exclude_extensions = kwargs.pop(\"exclude_extensions\", None)\n\n    uri = self._validate_s3_uri(uri)\n    return self._client.traverse(\n        s3_uri=uri,\n        include_extensions=exts,\n        exclude_extensions=exclude_extensions,\n        relative_unix=relative_unix,\n        debug_print=debug_print,\n    )\n</code></pre>"},{"location":"reference/unibox/backends/s3_backend/#unibox.backends.s3_backend.S3Backend.upload","title":"upload","text":"<pre><code>upload(local_path: Path, uri: str) -&gt; None\n</code></pre> <p>Upload the local file to S3.</p> Source code in <code>src/unibox/backends/s3_backend.py</code> <pre><code>def upload(self, local_path: Path, uri: str) -&gt; None:\n    \"\"\"Upload the local file to S3.\"\"\"\n    uri = self._validate_s3_uri(uri)\n    self._client.upload(str(local_path), s3_uri=uri)\n</code></pre>"},{"location":"reference/unibox/loaders/","title":"unibox.loaders","text":""},{"location":"reference/unibox/loaders/#unibox.loaders","title":"loaders","text":"<p>Modules:</p> <ul> <li> <code>base_loader</code>           \u2013            <p>basic loader class</p> </li> <li> <code>csv_loader</code>           \u2013            </li> <li> <code>hf_dataset_loader</code>           \u2013            </li> <li> <code>image_loder</code>           \u2013            </li> <li> <code>json_loader</code>           \u2013            </li> <li> <code>jsonl_loader</code>           \u2013            </li> <li> <code>loader_router</code>           \u2013            </li> <li> <code>parquet_loader</code>           \u2013            </li> <li> <code>toml_loader</code>           \u2013            </li> <li> <code>txt_loader</code>           \u2013            </li> <li> <code>yaml_loader</code>           \u2013            </li> </ul>"},{"location":"reference/unibox/loaders/base_loader/","title":"unibox.loaders.base_loader","text":""},{"location":"reference/unibox/loaders/base_loader/#unibox.loaders.base_loader","title":"base_loader","text":"<p>basic loader class</p> <p>Classes:</p> <ul> <li> <code>BaseLoader</code>           \u2013            </li> </ul>"},{"location":"reference/unibox/loaders/base_loader/#unibox.loaders.base_loader.BaseLoader","title":"BaseLoader","text":"<p>Methods:</p> <ul> <li> <code>load</code>             \u2013              <p>Load data from the given path with optional loader-specific configuration.</p> </li> <li> <code>save</code>             \u2013              <p>Save data to the given path with optional loader-specific configuration.</p> </li> </ul>"},{"location":"reference/unibox/loaders/base_loader/#unibox.loaders.base_loader.BaseLoader.load","title":"load","text":"<pre><code>load(\n    local_path: Union[str, Path],\n    loader_config: Optional[Dict] = None,\n) -&gt; Any\n</code></pre> <p>Load data from the given path with optional loader-specific configuration.</p> <p>Parameters:</p> <ul> <li> <code>local_path</code>               (<code>Union[str, Path]</code>)           \u2013            <p>Path or URI to load from</p> </li> <li> <code>loader_config</code>               (<code>Optional[Dict]</code>, default:                   <code>None</code> )           \u2013            <p>Loader-specific configuration options</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code> (              <code>Any</code> )          \u2013            <p>The loaded data</p> </li> </ul> Source code in <code>src/unibox/loaders/base_loader.py</code> <pre><code>def load(self, local_path: Union[str, Path], loader_config: Optional[Dict] = None) -&gt; Any:\n    \"\"\"Load data from the given path with optional loader-specific configuration.\n\n    Args:\n        local_path (Union[str, Path]): Path or URI to load from\n        loader_config (Optional[Dict]): Loader-specific configuration options\n\n    Returns:\n        Any: The loaded data\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/unibox/loaders/base_loader/#unibox.loaders.base_loader.BaseLoader.save","title":"save","text":"<pre><code>save(\n    local_path: Union[str, Path],\n    data: Any,\n    loader_config: Optional[Dict] = None,\n) -&gt; None\n</code></pre> <p>Save data to the given path with optional loader-specific configuration.</p> <p>Parameters:</p> <ul> <li> <code>local_path</code>               (<code>Union[str, Path]</code>)           \u2013            <p>Path or URI where to save the data</p> </li> <li> <code>data</code>               (<code>Any</code>)           \u2013            <p>Data to save</p> </li> <li> <code>loader_config</code>               (<code>Optional[Dict]</code>, default:                   <code>None</code> )           \u2013            <p>Loader-specific configuration options</p> </li> </ul> Source code in <code>src/unibox/loaders/base_loader.py</code> <pre><code>def save(self, local_path: Union[str, Path], data: Any, loader_config: Optional[Dict] = None) -&gt; None:\n    \"\"\"Save data to the given path with optional loader-specific configuration.\n\n    Args:\n        local_path (Union[str, Path]): Path or URI where to save the data\n        data (Any): Data to save\n        loader_config (Optional[Dict]): Loader-specific configuration options\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/unibox/loaders/csv_loader/","title":"unibox.loaders.csv_loader","text":""},{"location":"reference/unibox/loaders/csv_loader/#unibox.loaders.csv_loader","title":"csv_loader","text":"<p>Classes:</p> <ul> <li> <code>CSVLoader</code>           \u2013            <p>Load and save CSV files using pandas.</p> </li> </ul>"},{"location":"reference/unibox/loaders/csv_loader/#unibox.loaders.csv_loader.CSVLoader","title":"CSVLoader","text":"<p>               Bases: <code>BaseLoader</code></p> <p>Load and save CSV files using pandas.</p> <p>Methods:</p> <ul> <li> <code>load</code>             \u2013              <p>Load a CSV file with optional configuration.</p> </li> <li> <code>save</code>             \u2013              <p>Save a dataframe to CSV with optional configuration.</p> </li> </ul>"},{"location":"reference/unibox/loaders/csv_loader/#unibox.loaders.csv_loader.CSVLoader.load","title":"load","text":"<pre><code>load(\n    file_path: Path,\n    loader_config: Optional[Dict[str, Any]] = None,\n) -&gt; DataFrame\n</code></pre> <p>Load a CSV file with optional configuration.</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>Path</code>)           \u2013            <p>Path to the CSV file</p> </li> <li> <code>loader_config</code>               (<code>Optional[Dict]</code>, default:                   <code>None</code> )           \u2013            <p>Configuration options for pd.read_csv</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: The loaded dataframe</p> </li> </ul> Source code in <code>src/unibox/loaders/csv_loader.py</code> <pre><code>def load(self, file_path: Path, loader_config: Optional[Dict[str, Any]] = None) -&gt; pd.DataFrame:\n    \"\"\"Load a CSV file with optional configuration.\n\n    Args:\n        file_path (Path): Path to the CSV file\n        loader_config (Optional[Dict]): Configuration options for pd.read_csv\n\n    Returns:\n        pd.DataFrame: The loaded dataframe\n    \"\"\"\n    config = loader_config or {}\n    used_keys: Set[str] = set()\n\n    # Extract supported arguments from config\n    kwargs = {}\n    for key in self.SUPPORTED_LOAD_CONFIG:\n        if key in config:\n            kwargs[key] = config[key]\n            used_keys.add(key)\n\n    # Warn about unused config options\n    self._warn_unused_config(config, used_keys, \"CSVLoader\")\n\n    return pd.read_csv(file_path, **kwargs)\n</code></pre>"},{"location":"reference/unibox/loaders/csv_loader/#unibox.loaders.csv_loader.CSVLoader.save","title":"save","text":"<pre><code>save(\n    file_path: Path,\n    data: DataFrame,\n    loader_config: Optional[Dict[str, Any]] = None,\n) -&gt; None\n</code></pre> <p>Save a dataframe to CSV with optional configuration.</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>Path</code>)           \u2013            <p>Where to save the CSV file</p> </li> <li> <code>data</code>               (<code>DataFrame</code>)           \u2013            <p>DataFrame to save</p> </li> <li> <code>loader_config</code>               (<code>Optional[Dict]</code>, default:                   <code>None</code> )           \u2013            <p>Configuration options for to_csv</p> </li> </ul> Source code in <code>src/unibox/loaders/csv_loader.py</code> <pre><code>def save(self, file_path: Path, data: pd.DataFrame, loader_config: Optional[Dict[str, Any]] = None) -&gt; None:\n    \"\"\"Save a dataframe to CSV with optional configuration.\n\n    Args:\n        file_path (Path): Where to save the CSV file\n        data (pd.DataFrame): DataFrame to save\n        loader_config (Optional[Dict]): Configuration options for to_csv\n    \"\"\"\n    config = loader_config or {}\n    used_keys: Set[str] = set()\n\n    # Start with default kwargs\n    kwargs: Dict[str, Any] = {\"index\": False}\n\n    # Extract supported arguments from config\n    for key in self.SUPPORTED_SAVE_CONFIG:\n        if key in config:\n            kwargs[key] = config[key]\n            used_keys.add(key)\n\n    # Warn about unused config options\n    self._warn_unused_config(config, used_keys, \"CSVLoader\")\n\n    data.to_csv(file_path, **kwargs)\n</code></pre>"},{"location":"reference/unibox/loaders/hf_dataset_loader/","title":"unibox.loaders.hf_dataset_loader","text":""},{"location":"reference/unibox/loaders/hf_dataset_loader/#unibox.loaders.hf_dataset_loader","title":"hf_dataset_loader","text":"<p>Classes:</p> <ul> <li> <code>HFDatasetLoader</code>           \u2013            <p>Loader for HuggingFace datasets using the datasets library.</p> </li> </ul>"},{"location":"reference/unibox/loaders/hf_dataset_loader/#unibox.loaders.hf_dataset_loader.HFDatasetLoader","title":"HFDatasetLoader","text":"<pre><code>HFDatasetLoader()\n</code></pre> <p>               Bases: <code>BaseLoader</code></p> <p>Loader for HuggingFace datasets using the datasets library. This loader handles only the data format conversion between DataFrame/Dataset objects and local dataset files. Remote operations are handled by the HF backends.</p> <p>Methods:</p> <ul> <li> <code>load</code>             \u2013              <p>Load a dataset from a local path or cache.</p> </li> <li> <code>save</code>             \u2013              <p>Save data as a HuggingFace dataset to a local path.</p> </li> </ul> Source code in <code>src/unibox/loaders/hf_dataset_loader.py</code> <pre><code>def __init__(self):\n    super().__init__()\n    self.hf_api_backend = HuggingfaceHybridBackend()\n    self._max_rows_for_df_summary = 200_000\n</code></pre>"},{"location":"reference/unibox/loaders/hf_dataset_loader/#unibox.loaders.hf_dataset_loader.HFDatasetLoader.load","title":"load","text":"<pre><code>load(\n    local_path: str, loader_config: Optional[Dict] = None\n) -&gt; Any\n</code></pre> <p>Load a dataset from a local path or cache.</p> <p>Parameters:</p> <ul> <li> <code>local_path</code>               (<code>Union[str, Path]</code>)           \u2013            <p>Local path to dataset files</p> </li> <li> <code>loader_config</code>               (<code>Optional[Dict]</code>, default:                   <code>None</code> )           \u2013            <p>Configuration options for dataset loading split (str): Which split to load (default: None, loads all splits) revision (str): Git revision to use to_pandas (bool): Whether to convert to pandas DataFrame name (str): Dataset name/configuration cache_dir (str): Where to cache the dataset streaming (bool): Whether to stream the dataset num_proc (int): Number of processes for loading</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>           \u2013            <p>Union[Dataset, Dict[str, Dataset], pd.DataFrame]: The loaded dataset If to_pandas=True, returns DataFrame If split is specified, returns Dataset Otherwise returns Dict[split_name, Dataset]</p> </li> </ul> Source code in <code>src/unibox/loaders/hf_dataset_loader.py</code> <pre><code>def load(self, local_path: str, loader_config: Optional[Dict] = None) -&gt; Any:\n    \"\"\"Load a dataset from a local path or cache.\n\n    Args:\n        local_path (Union[str, Path]): Local path to dataset files\n        loader_config (Optional[Dict]): Configuration options for dataset loading\n            split (str): Which split to load (default: None, loads all splits)\n            revision (str): Git revision to use\n            to_pandas (bool): Whether to convert to pandas DataFrame\n            name (str): Dataset name/configuration\n            cache_dir (str): Where to cache the dataset\n            streaming (bool): Whether to stream the dataset\n            num_proc (int): Number of processes for loading\n\n    Returns:\n        Union[Dataset, Dict[str, Dataset], pd.DataFrame]: The loaded dataset\n            If to_pandas=True, returns DataFrame\n            If split is specified, returns Dataset\n            Otherwise returns Dict[split_name, Dataset]\n    \"\"\"\n    if not loader_config:\n        loader_config = {}\n\n    to_pandas = loader_config.get(\"to_pandas\", False)\n    parts = parse_hf_uri(local_path)\n    repo_id = parts.repo_id\n    split = loader_config.get(\"split\", \"train\")\n    revision = loader_config.get(\"revision\") or parts.revision or \"main\"\n    # num_proc = loader_config.get(\"num_proc\", 8)\n    num_proc = 8\n\n    if to_pandas:\n        return load_dataset(repo_id, split=split, revision=revision, num_proc=num_proc).to_pandas()\n    return load_dataset(repo_id, split=split, revision=revision, num_proc=num_proc)\n</code></pre>"},{"location":"reference/unibox/loaders/hf_dataset_loader/#unibox.loaders.hf_dataset_loader.HFDatasetLoader.save","title":"save","text":"<pre><code>save(\n    hf_uri: str,\n    data: Any,\n    loader_config: Optional[Dict] = None,\n) -&gt; None\n</code></pre> <p>Save data as a HuggingFace dataset to a local path.</p> <p>Parameters:</p> <ul> <li> <code>local_path</code>               (<code>Union[str, Path]</code>)           \u2013            <p>Local path to save dataset</p> </li> <li> <code>data</code>               (<code>Union[Dataset, DataFrame]</code>)           \u2013            <p>Dataset to save</p> </li> <li> <code>loader_config</code>               (<code>Optional[Dict]</code>, default:                   <code>None</code> )           \u2013            <p>Configuration options</p> </li> </ul> Source code in <code>src/unibox/loaders/hf_dataset_loader.py</code> <pre><code>def save(self, hf_uri: str, data: Any, loader_config: Optional[Dict] = None) -&gt; None:\n    \"\"\"Save data as a HuggingFace dataset to a local path.\n\n    Args:\n        local_path (Union[str, Path]): Local path to save dataset\n        data (Union[Dataset, pd.DataFrame]): Dataset to save\n        loader_config (Optional[Dict]): Configuration options\n    \"\"\"\n    # Parse configs\n    parts = parse_hf_uri(hf_uri)\n    repo_id = parts.repo_id\n    if not loader_config:\n        loader_config = {}\n\n    dataset_split = loader_config.get(\"split\", \"train\")\n    is_private = loader_config.get(\"private\", True)\n    dict_key_column = loader_config.get(\"dict_key_column\")\n    value_column = loader_config.get(\"value_column\")\n    flatten_sep = loader_config.get(\"flatten_sep\")\n    max_depth = loader_config.get(\"max_depth\")\n\n    readme_text = None  # if not a dataframe, we don't generate a readme update\n\n    # Convert JSON-like input to DataFrame if needed\n    if isinstance(data, (dict, list, tuple)):\n        try:\n            data = coerce_json_like_to_df(\n                data,\n                dict_key_column=dict_key_column,\n                value_column=value_column,\n                flatten_sep=flatten_sep,\n                max_depth=max_depth,\n            )\n        except Exception as e:\n            raise ValueError(\n                f\"Cannot convert JSON-like input to DataFrame for HF dataset save: {e}\",\n            ) from e\n\n    # Convert DataFrame to Dataset if needed and prepare README\n    if isinstance(data, pd.DataFrame):\n        if \"__index_level_0__\" in data.columns:\n            data.drop(columns=[\"__index_level_0__\"], inplace=True)\n        try:\n            readme_text = generate_dataset_summary(data, repo_id)\n        except Exception as e:\n            logger.warning(f\"Failed to generate dataset summary for {hf_uri}: {e}\")\n            readme_text = None\n        data = Dataset.from_pandas(data)\n    elif isinstance(data, Dataset):\n        try:\n            readme_text = self._generate_hf_readme_for_datasets(repo_id, data)\n        except Exception as e:\n            logger.warning(f\"Failed to generate dataset card for {hf_uri}: {e}\")\n            readme_text = None\n    elif isinstance(data, DatasetDict):\n        try:\n            readme_text = self._generate_hf_readme_for_datasets(repo_id, data)\n        except Exception as e:\n            logger.warning(f\"Failed to generate dataset card for {hf_uri}: {e}\")\n            readme_text = None\n    else:\n        raise ValueError(\"Data must be a pandas DataFrame, datasets.Dataset, or datasets.DatasetDict\")\n\n    # Save to Hugging Face Hub\n    try:\n        if isinstance(data, DatasetDict):\n            for split_name, split_ds in data.items():\n                split_to_use = split_name\n                try:\n                    split_ds.push_to_hub(\n                        repo_id=repo_id,\n                        private=is_private,\n                        split=split_to_use,\n                    )\n                    logger.debug(f\"Saved split '{split_to_use}' to {hf_uri}\")\n                except Exception as e:\n                    logger.error(f\"Failed to save split '{split_to_use}' to {hf_uri}: {e}\")\n                    raise\n        else:\n            data.push_to_hub(\n                repo_id=repo_id,\n                private=is_private,\n                split=dataset_split,\n            )\n            logger.debug(f\"Successfully saved dataset to {hf_uri}\")\n    except Exception as e:\n        logger.error(f\"Failed to save dataset to {hf_uri}: {e}\")\n        raise\n\n    # Update the README if needed\n    if readme_text:\n        try:\n            self.hf_api_backend.update_readme(repo_id, readme_text, repo_type=\"dataset\")\n            logger.info(f\"Successfully updated README for dataset {hf_uri}\")\n        except Exception as e:\n            logger.warning(f\"Failed to update README for dataset {hf_uri}: {e}\")\n</code></pre>"},{"location":"reference/unibox/loaders/image_loder/","title":"unibox.loaders.image_loder","text":""},{"location":"reference/unibox/loaders/image_loder/#unibox.loaders.image_loder","title":"image_loder","text":"<p>Classes:</p> <ul> <li> <code>ImageLoader</code>           \u2013            <p>Load and save images using PIL.</p> </li> </ul>"},{"location":"reference/unibox/loaders/image_loder/#unibox.loaders.image_loder.ImageLoader","title":"ImageLoader","text":"<p>               Bases: <code>BaseLoader</code></p> <p>Load and save images using PIL.</p> <p>Methods:</p> <ul> <li> <code>load</code>             \u2013              <p>Load an image with optional configuration.</p> </li> <li> <code>save</code>             \u2013              <p>Save an image with optional configuration.</p> </li> </ul>"},{"location":"reference/unibox/loaders/image_loder/#unibox.loaders.image_loder.ImageLoader.load","title":"load","text":"<pre><code>load(\n    file_path: Path,\n    loader_config: Optional[Dict[str, Any]] = None,\n) -&gt; Union[Image, ndarray]\n</code></pre> <p>Load an image with optional configuration.</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>Path</code>)           \u2013            <p>Path to the image file</p> </li> <li> <code>loader_config</code>               (<code>Optional[Dict]</code>, default:                   <code>None</code> )           \u2013            <p>Configuration options for image loading</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[Image, ndarray]</code>           \u2013            <p>Union[Image.Image, np.ndarray]: The loaded image</p> </li> </ul> Source code in <code>src/unibox/loaders/image_loder.py</code> <pre><code>def load(self, file_path: Path, loader_config: Optional[Dict[str, Any]] = None) -&gt; Union[Image.Image, \"np.ndarray\"]:\n    \"\"\"Load an image with optional configuration.\n\n    Args:\n        file_path (Path): Path to the image file\n        loader_config (Optional[Dict]): Configuration options for image loading\n\n    Returns:\n        Union[Image.Image, np.ndarray]: The loaded image\n    \"\"\"\n    config = loader_config or {}\n    used_keys: Set[str] = set()\n\n    # Load the image\n    img = Image.open(file_path)\n\n    # Handle mode conversion\n    if \"mode\" in config:\n        img = img.convert(config[\"mode\"])\n        used_keys.add(\"mode\")\n\n    # Handle resizing\n    if \"size\" in config:\n        img = img.resize(config[\"size\"])\n        used_keys.add(\"size\")\n\n    # Handle numpy conversion\n    if config.get(\"as_array\", False):\n        import numpy as np\n\n        img = np.array(img)\n        used_keys.add(\"as_array\")\n\n    # Warn about unused config options\n    self._warn_unused_config(config, used_keys, \"ImageLoader\")\n\n    return img\n</code></pre>"},{"location":"reference/unibox/loaders/image_loder/#unibox.loaders.image_loder.ImageLoader.save","title":"save","text":"<pre><code>save(\n    file_path: Path,\n    data: Image,\n    loader_config: Optional[Dict[str, Any]] = None,\n) -&gt; None\n</code></pre> <p>Save an image with optional configuration.</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>Path</code>)           \u2013            <p>Where to save the image</p> </li> <li> <code>data</code>               (<code>Image</code>)           \u2013            <p>Image to save</p> </li> <li> <code>loader_config</code>               (<code>Optional[Dict]</code>, default:                   <code>None</code> )           \u2013            <p>Configuration options for image saving</p> </li> </ul> Source code in <code>src/unibox/loaders/image_loder.py</code> <pre><code>def save(self, file_path: Path, data: Image.Image, loader_config: Optional[Dict[str, Any]] = None) -&gt; None:\n    \"\"\"Save an image with optional configuration.\n\n    Args:\n        file_path (Path): Where to save the image\n        data (Image.Image): Image to save\n        loader_config (Optional[Dict]): Configuration options for image saving\n    \"\"\"\n    config = loader_config or {}\n    used_keys: Set[str] = set()\n\n    # Ensure the image is loaded in memory without relying on private attrs like `.fp`\n    data.load()\n\n    # Extract supported arguments from config\n    kwargs = {}\n    for key in self.SUPPORTED_SAVE_CONFIG:\n        if key in config:\n            kwargs[key] = config[key]\n            used_keys.add(key)\n\n    # Warn about unused config options\n    self._warn_unused_config(config, used_keys, \"ImageLoader\")\n\n    data.save(file_path, **kwargs)\n</code></pre>"},{"location":"reference/unibox/loaders/json_loader/","title":"unibox.loaders.json_loader","text":""},{"location":"reference/unibox/loaders/json_loader/#unibox.loaders.json_loader","title":"json_loader","text":"<p>Classes:</p> <ul> <li> <code>JSONLoader</code>           \u2013            <p>Load and save JSON files using orjson.</p> </li> </ul>"},{"location":"reference/unibox/loaders/json_loader/#unibox.loaders.json_loader.JSONLoader","title":"JSONLoader","text":"<p>               Bases: <code>BaseLoader</code></p> <p>Load and save JSON files using orjson.</p> <p>Methods:</p> <ul> <li> <code>load</code>             \u2013              <p>Load a JSON file with optional configuration.</p> </li> <li> <code>save</code>             \u2013              <p>Save data to a JSON file with optional configuration.</p> </li> </ul>"},{"location":"reference/unibox/loaders/json_loader/#unibox.loaders.json_loader.JSONLoader.load","title":"load","text":"<pre><code>load(\n    file_path: Path,\n    loader_config: Optional[Dict[str, Any]] = None,\n) -&gt; Any\n</code></pre> <p>Load a JSON file with optional configuration.</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>Path</code>)           \u2013            <p>Path to the JSON file</p> </li> <li> <code>loader_config</code>               (<code>Optional[Dict]</code>, default:                   <code>None</code> )           \u2013            <p>Configuration options for JSON loading</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code> (              <code>Any</code> )          \u2013            <p>The loaded JSON data</p> </li> </ul> Source code in <code>src/unibox/loaders/json_loader.py</code> <pre><code>def load(self, file_path: Path, loader_config: Optional[Dict[str, Any]] = None) -&gt; Any:\n    \"\"\"Load a JSON file with optional configuration.\n\n    Args:\n        file_path (Path): Path to the JSON file\n        loader_config (Optional[Dict]): Configuration options for JSON loading\n\n    Returns:\n        Any: The loaded JSON data\n    \"\"\"\n    config = loader_config or {}\n    used_keys: Set[str] = set()\n\n    # Handle encoding if specified\n    if \"encoding\" in config:\n        used_keys.add(\"encoding\")\n        with open(file_path, encoding=config[\"encoding\"]) as f:\n            file_content = f.read().encode()\n    else:\n        with open(file_path, \"rb\") as f:\n            file_content = f.read()\n\n    if not file_content:\n        return None\n\n    # Handle default function if specified\n    kwargs = {}\n    if \"default\" in config:\n        kwargs[\"default\"] = config[\"default\"]\n        used_keys.add(\"default\")\n\n    # Warn about unused config options\n    self._warn_unused_config(config, used_keys, \"JSONLoader\")\n\n    return orjson.loads(file_content, **kwargs)\n</code></pre>"},{"location":"reference/unibox/loaders/json_loader/#unibox.loaders.json_loader.JSONLoader.save","title":"save","text":"<pre><code>save(\n    file_path: Path,\n    data: Any,\n    loader_config: Optional[Dict[str, Any]] = None,\n) -&gt; None\n</code></pre> <p>Save data to a JSON file with optional configuration.</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>Path</code>)           \u2013            <p>Where to save the JSON file</p> </li> <li> <code>data</code>               (<code>Any</code>)           \u2013            <p>Data to save</p> </li> <li> <code>loader_config</code>               (<code>Optional[Dict]</code>, default:                   <code>None</code> )           \u2013            <p>Configuration options for JSON saving</p> </li> </ul> Source code in <code>src/unibox/loaders/json_loader.py</code> <pre><code>def save(self, file_path: Path, data: Any, loader_config: Optional[Dict[str, Any]] = None) -&gt; None:\n    \"\"\"Save data to a JSON file with optional configuration.\n\n    Args:\n        file_path (Path): Where to save the JSON file\n        data (Any): Data to save\n        loader_config (Optional[Dict]): Configuration options for JSON saving\n    \"\"\"\n    config = loader_config or {}\n    used_keys: Set[str] = set()\n\n    # Extract supported arguments from config\n    kwargs = {}\n    for key in self.SUPPORTED_SAVE_CONFIG:\n        if key in config:\n            kwargs[key] = config[key]\n            used_keys.add(key)\n\n    # Warn about unused config options\n    self._warn_unused_config(config, used_keys, \"JSONLoader\")\n\n    with open(file_path, \"wb\") as f:\n        f.write(orjson.dumps(data, **kwargs))\n</code></pre>"},{"location":"reference/unibox/loaders/jsonl_loader/","title":"unibox.loaders.jsonl_loader","text":""},{"location":"reference/unibox/loaders/jsonl_loader/#unibox.loaders.jsonl_loader","title":"jsonl_loader","text":"<p>Classes:</p> <ul> <li> <code>JSONLLoader</code>           \u2013            <p>Load and save JSONL files using orjson.</p> </li> </ul>"},{"location":"reference/unibox/loaders/jsonl_loader/#unibox.loaders.jsonl_loader.JSONLLoader","title":"JSONLLoader","text":"<p>               Bases: <code>BaseLoader</code></p> <p>Load and save JSONL files using orjson.</p> <p>Methods:</p> <ul> <li> <code>load</code>             \u2013              <p>Load a JSONL file with optional configuration.</p> </li> <li> <code>save</code>             \u2013              <p>Save a list of objects to a JSONL file with optional configuration.</p> </li> </ul>"},{"location":"reference/unibox/loaders/jsonl_loader/#unibox.loaders.jsonl_loader.JSONLLoader.load","title":"load","text":"<pre><code>load(\n    file_path: Path,\n    loader_config: Optional[Dict[str, Any]] = None,\n) -&gt; List[Any]\n</code></pre> <p>Load a JSONL file with optional configuration.</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>Path</code>)           \u2013            <p>Path to the JSONL file</p> </li> <li> <code>loader_config</code>               (<code>Optional[Dict]</code>, default:                   <code>None</code> )           \u2013            <p>Configuration options for JSONL loading</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[Any]</code>           \u2013            <p>List[Any]: List of parsed JSON objects</p> </li> </ul> Source code in <code>src/unibox/loaders/jsonl_loader.py</code> <pre><code>def load(self, file_path: Path, loader_config: Optional[Dict[str, Any]] = None) -&gt; List[Any]:\n    \"\"\"Load a JSONL file with optional configuration.\n\n    Args:\n        file_path (Path): Path to the JSONL file\n        loader_config (Optional[Dict]): Configuration options for JSONL loading\n\n    Returns:\n        List[Any]: List of parsed JSON objects\n    \"\"\"\n    config = loader_config or {}\n    used_keys: Set[str] = set()\n\n    # Handle config options\n    encoding = config.get(\"encoding\", \"utf-8\")\n    if \"encoding\" in config:\n        used_keys.add(\"encoding\")\n\n    skip_errors = config.get(\"skip_errors\", True)\n    if \"skip_errors\" in config:\n        used_keys.add(\"skip_errors\")\n\n    replace_nan = config.get(\"replace_nan\", True)\n    if \"replace_nan\" in config:\n        used_keys.add(\"replace_nan\")\n\n    # Handle default function if specified\n    kwargs = {}\n    if \"default\" in config:\n        kwargs[\"default\"] = config[\"default\"]\n        used_keys.add(\"default\")\n\n    # Warn about unused config options\n    self._warn_unused_config(config, used_keys, \"JSONLLoader\")\n\n    data = []\n    with open(file_path, \"rb\") as f:\n        for line in f:\n            line_str = line.decode(encoding, errors=\"replace\")\n            if replace_nan and \"NaN\" in line_str:\n                line_str = re.sub(r\"\\bNaN\\b\", \"null\", line_str)\n            try:\n                data.append(orjson.loads(line_str, **kwargs))\n            except orjson.JSONDecodeError:\n                if not skip_errors:\n                    raise\n    return data\n</code></pre>"},{"location":"reference/unibox/loaders/jsonl_loader/#unibox.loaders.jsonl_loader.JSONLLoader.save","title":"save","text":"<pre><code>save(\n    file_path: Path,\n    data: List[Any],\n    loader_config: Optional[Dict[str, Any]] = None,\n) -&gt; None\n</code></pre> <p>Save a list of objects to a JSONL file with optional configuration.</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>Path</code>)           \u2013            <p>Where to save the JSONL file</p> </li> <li> <code>data</code>               (<code>List[Any]</code>)           \u2013            <p>Objects to save</p> </li> <li> <code>loader_config</code>               (<code>Optional[Dict]</code>, default:                   <code>None</code> )           \u2013            <p>Configuration options for JSONL saving</p> </li> </ul> Source code in <code>src/unibox/loaders/jsonl_loader.py</code> <pre><code>def save(self, file_path: Path, data: List[Any], loader_config: Optional[Dict[str, Any]] = None) -&gt; None:\n    \"\"\"Save a list of objects to a JSONL file with optional configuration.\n\n    Args:\n        file_path (Path): Where to save the JSONL file\n        data (List[Any]): Objects to save\n        loader_config (Optional[Dict]): Configuration options for JSONL saving\n    \"\"\"\n    config = loader_config or {}\n    used_keys: Set[str] = set()\n\n    # Extract supported arguments from config\n    kwargs = {}\n    for key in self.SUPPORTED_SAVE_CONFIG - {\"encoding\"}:\n        if key in config:\n            kwargs[key] = config[key]\n            used_keys.add(key)\n\n    # Handle encoding\n    encoding = config.get(\"encoding\", \"utf-8\")\n    if \"encoding\" in config:\n        used_keys.add(\"encoding\")\n\n    # Warn about unused config options\n    self._warn_unused_config(config, used_keys, \"JSONLLoader\")\n\n    with open(file_path, \"wb\") as f:\n        for item in data:\n            line = orjson.dumps(item, **kwargs)\n            if encoding != \"utf-8\":\n                line = line.decode(\"utf-8\").encode(encoding)\n            f.write(line + b\"\\n\")\n</code></pre>"},{"location":"reference/unibox/loaders/loader_router/","title":"unibox.loaders.loader_router","text":""},{"location":"reference/unibox/loaders/loader_router/#unibox.loaders.loader_router","title":"loader_router","text":"<p>Functions:</p> <ul> <li> <code>get_loader_for_path</code>             \u2013              <p>Get appropriate loader for a given path.</p> </li> <li> <code>get_loader_for_suffix</code>             \u2013              <p>Deprecated</p> <p>Use get_loader_for_path instead.</p> </li> <li> <code>is_hf_dataset_dir</code>             \u2013              <p>Check if the given path is a Hugging Face dataset directory.</p> </li> <li> <code>load_data</code>             \u2013              <p>High-level function to load data using the appropriate loader.</p> </li> </ul>"},{"location":"reference/unibox/loaders/loader_router/#unibox.loaders.loader_router.get_loader_for_path","title":"get_loader_for_path","text":"<pre><code>get_loader_for_path(\n    path: Union[str, Path],\n) -&gt; Optional[BaseLoader]\n</code></pre> <p>Get appropriate loader for a given path.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Union[str, Path]</code>)           \u2013            <p>File path or URI to load</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[BaseLoader]</code>           \u2013            <p>Optional[BaseLoader]: Appropriate loader instance or None if no loader found</p> </li> </ul> Source code in <code>src/unibox/loaders/loader_router.py</code> <pre><code>def get_loader_for_path(path: Union[str, Path]) -&gt; Optional[BaseLoader]:\n    \"\"\"Get appropriate loader for a given path.\n\n    Args:\n        path (Union[str, Path]): File path or URI to load\n\n    Returns:\n        Optional[BaseLoader]: Appropriate loader instance or None if no loader found\n    \"\"\"\n    path_str = str(path).lower()\n\n    # Handle HuggingFace URIs\n    if path_str.startswith(\"hf://\"):\n        parts = parse_hf_uri(path_str)\n        subpath = parts.path_in_repo\n        # If no subpath or no extension in subpath, treat as dataset\n        if not subpath or \".\" not in subpath:\n            return HFDatasetLoader()\n        # If there's a subpath with extension, treat as file and use appropriate loader\n        path_str = subpath\n\n    # For other URIs (s3://, etc.), extract the filename part\n    elif \"://\" in path_str:\n        # Get the last part of the path (filename)\n        path_str = path_str.split(\"/\")[-1]\n        if not path_str:\n            return None\n\n    if is_hf_dataset_dir(path_str):\n        return HFDatasetLoader()\n\n    # Handle file extensions\n    suffix = Path(path_str).suffix.lower()\n    if suffix == \".csv\":\n        return CSVLoader()\n    if suffix in IMG_FILES:\n        return ImageLoader()\n    if suffix == \".json\":\n        return JSONLoader()\n    if suffix == \".jsonl\":\n        return JSONLLoader()\n    if suffix == \".parquet\":\n        return ParquetLoader()\n    if suffix in [\".txt\", \".md\", \".markdown\"]:\n        return TxtLoader()\n    if suffix == \".toml\":\n        return TOMLLoader()\n    if suffix == \".yaml\" or suffix == \".yml\":\n        return YAMLLoader()\n\n    return None\n</code></pre>"},{"location":"reference/unibox/loaders/loader_router/#unibox.loaders.loader_router.get_loader_for_suffix","title":"get_loader_for_suffix","text":"<pre><code>get_loader_for_suffix(suffix: str) -&gt; Optional[BaseLoader]\n</code></pre> <p>Deprecated</p> <p>Use get_loader_for_path instead.</p> <p>This function is kept for backward compatibility and will be removed in a future version.</p> Source code in <code>src/unibox/loaders/loader_router.py</code> <pre><code>def get_loader_for_suffix(suffix: str) -&gt; Optional[BaseLoader]:\n    \"\"\"DEPRECATED: Use get_loader_for_path instead.\n\n    This function is kept for backward compatibility and will be removed in a future version.\n    \"\"\"\n    warnings.warn(\n        \"get_loader_for_suffix is deprecated and will be removed in a future version. Use get_loader_for_path instead.\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    # Create a fake path with the given suffix to reuse the logic\n    return get_loader_for_path(f\"file{suffix}\")\n</code></pre>"},{"location":"reference/unibox/loaders/loader_router/#unibox.loaders.loader_router.is_hf_dataset_dir","title":"is_hf_dataset_dir","text":"<pre><code>is_hf_dataset_dir(path_str: str) -&gt; bool\n</code></pre> <p>Check if the given path is a Hugging Face dataset directory.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Path</code>)           \u2013            <p>Path to check</p> </li> </ul> Source code in <code>src/unibox/loaders/loader_router.py</code> <pre><code>def is_hf_dataset_dir(path_str: str) -&gt; bool:\n    \"\"\"Check if the given path is a Hugging Face dataset directory.\n\n    Args:\n        path (Path): Path to check\n    \"\"\"\n    path = Path(path_str)\n    if not path.is_dir():\n        return False\n\n    hf_signature_files = [\"dataset_dict.json\", \"dataset_info.json\"]\n    return any((path / f).is_file() for f in hf_signature_files)\n</code></pre>"},{"location":"reference/unibox/loaders/loader_router/#unibox.loaders.loader_router.load_data","title":"load_data","text":"<pre><code>load_data(\n    path: Union[str, Path],\n    loader_config: Optional[dict] = None,\n) -&gt; Any\n</code></pre> <p>High-level function to load data using the appropriate loader.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Union[str, Path]</code>)           \u2013            <p>Path to the file or dataset to load</p> </li> <li> <code>loader_config</code>               (<code>Optional[dict]</code>, default:                   <code>None</code> )           \u2013            <p>Configuration options for the loader</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code> (              <code>Any</code> )          \u2013            <p>The loaded data</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no appropriate loader is found</p> </li> </ul> Source code in <code>src/unibox/loaders/loader_router.py</code> <pre><code>def load_data(path: Union[str, Path], loader_config: Optional[dict] = None) -&gt; Any:\n    \"\"\"High-level function to load data using the appropriate loader.\n\n    Args:\n        path (Union[str, Path]): Path to the file or dataset to load\n        loader_config (Optional[dict]): Configuration options for the loader\n\n    Returns:\n        Any: The loaded data\n\n    Raises:\n        ValueError: If no appropriate loader is found\n    \"\"\"\n    path_str = str(path)\n    # 1. Which backend?\n    backend = get_backend_for_uri(path_str)\n    if not backend:\n        raise ValueError(f\"No backend found for: {path_str}\")\n\n    # 2. Download\n    local_path = backend.download(path_str)\n\n    # if it's huggingface, let loader load it instead of downloading at backend\n    if not str(local_path).startswith(\"hf://\"):\n        # Resolve symlinks (non-strict) so linked files work; accept symlinks via lexists\n        resolved_path = Path(local_path).resolve(strict=False)\n        if not (resolved_path.exists() or os.path.lexists(str(resolved_path))):\n            raise FileNotFoundError(f\"Downloaded file/folder not found: {resolved_path}\")\n        local_path = resolved_path\n\n    # Otherwise, extension-based logic\n    loader = get_loader_for_path(local_path)\n\n    if not loader:\n        raise ValueError(f\"No loader found for path: {local_path}\")\n\n    # 4. Let the loader parse\n    return loader.load(local_path, loader_config=loader_config or {})\n</code></pre>"},{"location":"reference/unibox/loaders/parquet_loader/","title":"unibox.loaders.parquet_loader","text":""},{"location":"reference/unibox/loaders/parquet_loader/#unibox.loaders.parquet_loader","title":"parquet_loader","text":"<p>Classes:</p> <ul> <li> <code>ParquetLoader</code>           \u2013            <p>Load and save Parquet files using pandas.</p> </li> </ul>"},{"location":"reference/unibox/loaders/parquet_loader/#unibox.loaders.parquet_loader.ParquetLoader","title":"ParquetLoader","text":"<p>               Bases: <code>BaseLoader</code></p> <p>Load and save Parquet files using pandas.</p> <p>Methods:</p> <ul> <li> <code>load</code>             \u2013              <p>Load a parquet file with optional configuration.</p> </li> <li> <code>save</code>             \u2013              <p>Save a dataframe to parquet with optional configuration.</p> </li> </ul>"},{"location":"reference/unibox/loaders/parquet_loader/#unibox.loaders.parquet_loader.ParquetLoader.load","title":"load","text":"<pre><code>load(\n    file_path: Path, loader_config: Optional[Dict] = None\n) -&gt; DataFrame\n</code></pre> <p>Load a parquet file with optional configuration.</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>Path</code>)           \u2013            <p>Path to the parquet file</p> </li> <li> <code>loader_config</code>               (<code>Optional[Dict]</code>, default:                   <code>None</code> )           \u2013            <p>Configuration options for pd.read_parquet</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: The loaded dataframe</p> </li> </ul> Source code in <code>src/unibox/loaders/parquet_loader.py</code> <pre><code>def load(self, file_path: Path, loader_config: Optional[Dict] = None) -&gt; pd.DataFrame:\n    \"\"\"Load a parquet file with optional configuration.\n\n    Args:\n        file_path (Path): Path to the parquet file\n        loader_config (Optional[Dict]): Configuration options for pd.read_parquet\n\n    Returns:\n        pd.DataFrame: The loaded dataframe\n    \"\"\"\n    config = loader_config or {}\n    used_keys: Set[str] = set()\n\n    # Extract supported arguments from config\n    kwargs = {}\n    for key in self.SUPPORTED_LOAD_CONFIG:\n        if key in config:\n            kwargs[key] = config[key]\n            used_keys.add(key)\n\n    # Warn about unused config options\n    self._warn_unused_config(config, used_keys, \"ParquetLoader\")\n\n    return pd.read_parquet(file_path, **kwargs)\n</code></pre>"},{"location":"reference/unibox/loaders/parquet_loader/#unibox.loaders.parquet_loader.ParquetLoader.save","title":"save","text":"<pre><code>save(\n    file_path: Path,\n    data: DataFrame,\n    loader_config: Optional[Dict] = None,\n) -&gt; None\n</code></pre> <p>Save a dataframe to parquet with optional configuration.</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>Path</code>)           \u2013            <p>Where to save the parquet file</p> </li> <li> <code>data</code>               (<code>DataFrame</code>)           \u2013            <p>DataFrame to save</p> </li> <li> <code>loader_config</code>               (<code>Optional[Dict]</code>, default:                   <code>None</code> )           \u2013            <p>Configuration options for to_parquet</p> </li> </ul> Source code in <code>src/unibox/loaders/parquet_loader.py</code> <pre><code>def save(self, file_path: Path, data: pd.DataFrame, loader_config: Optional[Dict] = None) -&gt; None:\n    \"\"\"Save a dataframe to parquet with optional configuration.\n\n    Args:\n        file_path (Path): Where to save the parquet file\n        data (pd.DataFrame): DataFrame to save\n        loader_config (Optional[Dict]): Configuration options for to_parquet\n    \"\"\"\n    config = loader_config or {}\n    used_keys: Set[str] = set()\n\n    # Extract supported arguments from config\n    kwargs = {}\n    for key in self.SUPPORTED_SAVE_CONFIG:\n        if key in config:\n            kwargs[key] = config[key]\n            used_keys.add(key)\n\n    # Warn about unused config options\n    self._warn_unused_config(config, used_keys, \"ParquetLoader\")\n\n    data.to_parquet(file_path, **kwargs)\n</code></pre>"},{"location":"reference/unibox/loaders/toml_loader/","title":"unibox.loaders.toml_loader","text":""},{"location":"reference/unibox/loaders/toml_loader/#unibox.loaders.toml_loader","title":"toml_loader","text":"<p>Classes:</p> <ul> <li> <code>TOMLLoader</code>           \u2013            <p>Load and save TOML files using tomli/tomli_w.</p> </li> </ul>"},{"location":"reference/unibox/loaders/toml_loader/#unibox.loaders.toml_loader.TOMLLoader","title":"TOMLLoader","text":"<p>               Bases: <code>BaseLoader</code></p> <p>Load and save TOML files using tomli/tomli_w.</p> <p>Methods:</p> <ul> <li> <code>load</code>             \u2013              <p>Load a TOML file with optional configuration.</p> </li> <li> <code>save</code>             \u2013              <p>Save data to a TOML file with optional configuration.</p> </li> </ul>"},{"location":"reference/unibox/loaders/toml_loader/#unibox.loaders.toml_loader.TOMLLoader.load","title":"load","text":"<pre><code>load(\n    file_path: Path,\n    loader_config: Optional[Dict[str, Any]] = None,\n) -&gt; Any\n</code></pre> <p>Load a TOML file with optional configuration.</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>Path</code>)           \u2013            <p>Path to the TOML file</p> </li> <li> <code>loader_config</code>               (<code>Optional[Dict]</code>, default:                   <code>None</code> )           \u2013            <p>Configuration options for TOML loading</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code> (              <code>Any</code> )          \u2013            <p>The loaded TOML data</p> </li> </ul> Source code in <code>src/unibox/loaders/toml_loader.py</code> <pre><code>def load(self, file_path: Path, loader_config: Optional[Dict[str, Any]] = None) -&gt; Any:\n    \"\"\"Load a TOML file with optional configuration.\n\n    Args:\n        file_path (Path): Path to the TOML file\n        loader_config (Optional[Dict]): Configuration options for TOML loading\n\n    Returns:\n        Any: The loaded TOML data\n    \"\"\"\n    config = loader_config or {}\n    used_keys: Set[str] = set()\n\n    # Extract supported arguments from config\n    kwargs = {}\n    for key in self.SUPPORTED_LOAD_CONFIG:\n        if key in config:\n            kwargs[key] = config[key]\n            used_keys.add(key)\n\n    # Warn about unused config options\n    self._warn_unused_config(config, used_keys, \"TOMLLoader\")\n\n    with open(file_path, \"rb\") as f:\n        return tomli.load(f, **kwargs)\n</code></pre>"},{"location":"reference/unibox/loaders/toml_loader/#unibox.loaders.toml_loader.TOMLLoader.save","title":"save","text":"<pre><code>save(\n    file_path: Path,\n    data: Any,\n    loader_config: Optional[Dict[str, Any]] = None,\n) -&gt; None\n</code></pre> <p>Save data to a TOML file with optional configuration.</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>Path</code>)           \u2013            <p>Where to save the TOML file</p> </li> <li> <code>data</code>               (<code>Any</code>)           \u2013            <p>Data to save</p> </li> <li> <code>loader_config</code>               (<code>Optional[Dict]</code>, default:                   <code>None</code> )           \u2013            <p>Configuration options for TOML saving</p> </li> </ul> Source code in <code>src/unibox/loaders/toml_loader.py</code> <pre><code>def save(self, file_path: Path, data: Any, loader_config: Optional[Dict[str, Any]] = None) -&gt; None:\n    \"\"\"Save data to a TOML file with optional configuration.\n\n    Args:\n        file_path (Path): Where to save the TOML file\n        data (Any): Data to save\n        loader_config (Optional[Dict]): Configuration options for TOML saving\n    \"\"\"\n    config = loader_config or {}\n    used_keys: Set[str] = set()\n\n    # Extract supported arguments from config\n    kwargs = {}\n    for key in self.SUPPORTED_SAVE_CONFIG:\n        if key in config:\n            kwargs[key] = config[key]\n            used_keys.add(key)\n\n    # Warn about unused config options\n    self._warn_unused_config(config, used_keys, \"TOMLLoader\")\n\n    with open(file_path, \"wb\") as f:\n        tomli_w.dump(data, f, **kwargs)\n</code></pre>"},{"location":"reference/unibox/loaders/txt_loader/","title":"unibox.loaders.txt_loader","text":""},{"location":"reference/unibox/loaders/txt_loader/#unibox.loaders.txt_loader","title":"txt_loader","text":"<p>Classes:</p> <ul> <li> <code>TxtLoader</code>           \u2013            <p>Load and save text files.</p> </li> </ul>"},{"location":"reference/unibox/loaders/txt_loader/#unibox.loaders.txt_loader.TxtLoader","title":"TxtLoader","text":"<p>               Bases: <code>BaseLoader</code></p> <p>Load and save text files.</p> <p>Methods:</p> <ul> <li> <code>load</code>             \u2013              <p>Load a text file and return a list of lines.</p> </li> <li> <code>save</code>             \u2013              <p>Save a list of strings to a text file.</p> </li> </ul>"},{"location":"reference/unibox/loaders/txt_loader/#unibox.loaders.txt_loader.TxtLoader.load","title":"load","text":"<pre><code>load(\n    file_path: Path,\n    loader_config: Optional[Dict[str, Any]] = None,\n) -&gt; List[str]\n</code></pre> <p>Load a text file and return a list of lines.</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>Path</code>)           \u2013            <p>Path to the text file</p> </li> <li> <code>loader_config</code>               (<code>Optional[Dict]</code>, default:                   <code>None</code> )           \u2013            <p>Configuration options for text loading</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List[str]: A list of lines from the file</p> </li> </ul> Source code in <code>src/unibox/loaders/txt_loader.py</code> <pre><code>def load(self, file_path: Path, loader_config: Optional[Dict[str, Any]] = None) -&gt; List[str]:\n    \"\"\"Load a text file and return a list of lines.\n\n    Args:\n        file_path (Path): Path to the text file\n        loader_config (Optional[Dict]): Configuration options for text loading\n\n    Returns:\n        List[str]: A list of lines from the file\n    \"\"\"\n    config = loader_config or {}\n    used_keys: Set[str] = set()\n\n    # Handle encoding\n    encoding = config.get(\"encoding\", \"utf-8\")\n    if \"encoding\" in config:\n        used_keys.add(\"encoding\")\n\n    # Handle line processing options\n    strip = config.get(\"strip\", True)\n    if \"strip\" in config:\n        used_keys.add(\"strip\")\n\n    skip_empty = config.get(\"skip_empty\", False)\n    if \"skip_empty\" in config:\n        used_keys.add(\"skip_empty\")\n\n    # Warn about unused config options\n    self._warn_unused_config(config, used_keys, \"TxtLoader\")\n\n    with open(file_path, encoding=encoding) as f:\n        lines = [line.strip() if strip else line.rstrip(\"\\n\") for line in f]\n        if skip_empty:\n            lines = [line for line in lines if line]\n        return lines\n</code></pre>"},{"location":"reference/unibox/loaders/txt_loader/#unibox.loaders.txt_loader.TxtLoader.save","title":"save","text":"<pre><code>save(\n    file_path: Path,\n    data: List[str],\n    loader_config: Optional[Dict[str, Any]] = None,\n) -&gt; None\n</code></pre> <p>Save a list of strings to a text file.</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>Path</code>)           \u2013            <p>Where to save the text file</p> </li> <li> <code>data</code>               (<code>List[str]</code>)           \u2013            <p>Lines to write to the file</p> </li> <li> <code>loader_config</code>               (<code>Optional[Dict]</code>, default:                   <code>None</code> )           \u2013            <p>Configuration options for text saving</p> </li> </ul> Source code in <code>src/unibox/loaders/txt_loader.py</code> <pre><code>def save(self, file_path: Path, data: List[str], loader_config: Optional[Dict[str, Any]] = None) -&gt; None:\n    \"\"\"Save a list of strings to a text file.\n\n    Args:\n        file_path (Path): Where to save the text file\n        data (List[str]): Lines to write to the file\n        loader_config (Optional[Dict]): Configuration options for text saving\n    \"\"\"\n    config = loader_config or {}\n    used_keys: Set[str] = set()\n\n    # Handle encoding\n    encoding = config.get(\"encoding\", \"utf-8\")\n    if \"encoding\" in config:\n        used_keys.add(\"encoding\")\n\n    # Handle write mode\n    mode = \"a\" if config.get(\"append\", False) else \"w\"\n    if \"append\" in config:\n        used_keys.add(\"append\")\n\n    # Handle line ending\n    newline = config.get(\"newline\", \"\\n\")\n    if \"newline\" in config:\n        used_keys.add(\"newline\")\n\n    # Warn about unused config options\n    self._warn_unused_config(config, used_keys, \"TxtLoader\")\n\n    with open(file_path, mode, encoding=encoding, newline=newline) as f:\n        f.writelines(f\"{line}{newline}\" for line in data)\n</code></pre>"},{"location":"reference/unibox/loaders/yaml_loader/","title":"unibox.loaders.yaml_loader","text":""},{"location":"reference/unibox/loaders/yaml_loader/#unibox.loaders.yaml_loader","title":"yaml_loader","text":"<p>Classes:</p> <ul> <li> <code>YAMLLoader</code>           \u2013            <p>Load and save YAML files.</p> </li> </ul>"},{"location":"reference/unibox/loaders/yaml_loader/#unibox.loaders.yaml_loader.YAMLLoader","title":"YAMLLoader","text":"<p>               Bases: <code>BaseLoader</code></p> <p>Load and save YAML files.</p> <p>Methods:</p> <ul> <li> <code>load</code>             \u2013              <p>Load a YAML file with optional configuration.</p> </li> <li> <code>save</code>             \u2013              <p>Save data to a YAML file with optional configuration.</p> </li> </ul>"},{"location":"reference/unibox/loaders/yaml_loader/#unibox.loaders.yaml_loader.YAMLLoader.load","title":"load","text":"<pre><code>load(\n    file_path: Path,\n    loader_config: Optional[Dict[str, Any]] = None,\n) -&gt; Any\n</code></pre> <p>Load a YAML file with optional configuration.</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>Path</code>)           \u2013            <p>Path to the YAML file</p> </li> <li> <code>loader_config</code>               (<code>Optional[Dict]</code>, default:                   <code>None</code> )           \u2013            <p>Configuration options for YAML loading</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code> (              <code>Any</code> )          \u2013            <p>The loaded YAML data</p> </li> </ul> Source code in <code>src/unibox/loaders/yaml_loader.py</code> <pre><code>def load(self, file_path: Path, loader_config: Optional[Dict[str, Any]] = None) -&gt; Any:\n    \"\"\"Load a YAML file with optional configuration.\n\n    Args:\n        file_path (Path): Path to the YAML file\n        loader_config (Optional[Dict]): Configuration options for YAML loading\n\n    Returns:\n        Any: The loaded YAML data\n    \"\"\"\n    config = loader_config or {}\n    used_keys: Set[str] = set()\n\n    # Extract supported arguments from config\n    kwargs = {}\n\n    # Handle encoding\n    encoding = config.get(\"encoding\", \"utf-8\")\n    if \"encoding\" in config:\n        used_keys.add(\"encoding\")\n\n    # Handle other load options\n    for key in self.SUPPORTED_LOAD_CONFIG - {\"encoding\"}:\n        if key in config:\n            kwargs[key] = config[key]\n            used_keys.add(key)\n\n    # Warn about unused config options\n    self._warn_unused_config(config, used_keys, \"YAMLLoader\")\n\n    with open(file_path, encoding=encoding) as f:\n        return yaml.safe_load(f, **kwargs)\n</code></pre>"},{"location":"reference/unibox/loaders/yaml_loader/#unibox.loaders.yaml_loader.YAMLLoader.save","title":"save","text":"<pre><code>save(\n    file_path: Path,\n    data: Any,\n    loader_config: Optional[Dict[str, Any]] = None,\n) -&gt; None\n</code></pre> <p>Save data to a YAML file with optional configuration.</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>Path</code>)           \u2013            <p>Where to save the YAML file</p> </li> <li> <code>data</code>               (<code>Any</code>)           \u2013            <p>Data to save</p> </li> <li> <code>loader_config</code>               (<code>Optional[Dict]</code>, default:                   <code>None</code> )           \u2013            <p>Configuration options for YAML saving</p> </li> </ul> Source code in <code>src/unibox/loaders/yaml_loader.py</code> <pre><code>def save(self, file_path: Path, data: Any, loader_config: Optional[Dict[str, Any]] = None) -&gt; None:\n    \"\"\"Save data to a YAML file with optional configuration.\n\n    Args:\n        file_path (Path): Where to save the YAML file\n        data (Any): Data to save\n        loader_config (Optional[Dict]): Configuration options for YAML saving\n    \"\"\"\n    config = loader_config or {}\n    used_keys: Set[str] = set()\n\n    # Extract supported arguments from config\n    kwargs = {}\n\n    # Handle encoding\n    encoding = config.get(\"encoding\", \"utf-8\")\n    if \"encoding\" in config:\n        used_keys.add(\"encoding\")\n\n    # Handle other save options\n    for key in self.SUPPORTED_SAVE_CONFIG - {\"encoding\"}:\n        if key in config:\n            kwargs[key] = config[key]\n            used_keys.add(key)\n\n    # Warn about unused config options\n    self._warn_unused_config(config, used_keys, \"YAMLLoader\")\n\n    with open(file_path, \"w\", encoding=encoding) as f:\n        yaml.safe_dump(data, f, **kwargs)\n</code></pre>"},{"location":"reference/unibox/nb_helpers/","title":"unibox.nb_helpers","text":""},{"location":"reference/unibox/nb_helpers/#unibox.nb_helpers","title":"nb_helpers","text":"<p>Modules:</p> <ul> <li> <code>ipython_utils</code>           \u2013            </li> <li> <code>uni_peeker</code>           \u2013            </li> </ul>"},{"location":"reference/unibox/nb_helpers/ipython_utils/","title":"unibox.nb_helpers.ipython_utils","text":""},{"location":"reference/unibox/nb_helpers/ipython_utils/#unibox.nb_helpers.ipython_utils","title":"ipython_utils","text":""},{"location":"reference/unibox/nb_helpers/uni_peeker/","title":"unibox.nb_helpers.uni_peeker","text":""},{"location":"reference/unibox/nb_helpers/uni_peeker/#unibox.nb_helpers.uni_peeker","title":"uni_peeker","text":"<p>Classes:</p> <ul> <li> <code>CompactJSONEncoder</code>           \u2013            <p>Custom JSON Encoder for specific formatting of dictionaries and lists.</p> </li> <li> <code>UniPeeker</code>           \u2013            <p>Utility class for peeking into data with efficient methods.</p> </li> </ul>"},{"location":"reference/unibox/nb_helpers/uni_peeker/#unibox.nb_helpers.uni_peeker.CompactJSONEncoder","title":"CompactJSONEncoder","text":"<p>               Bases: <code>JSONEncoder</code></p> <p>Custom JSON Encoder for specific formatting of dictionaries and lists.</p> <p>Methods:</p> <ul> <li> <code>iterencode</code>             \u2013              <p>Overridden method for custom JSON formatting.</p> </li> </ul>"},{"location":"reference/unibox/nb_helpers/uni_peeker/#unibox.nb_helpers.uni_peeker.CompactJSONEncoder.iterencode","title":"iterencode","text":"<pre><code>iterencode(o, _one_shot=False)\n</code></pre> <p>Overridden method for custom JSON formatting.</p> Source code in <code>src/unibox/nb_helpers/uni_peeker.py</code> <pre><code>def iterencode(self, o, _one_shot=False):\n    \"\"\"Overridden method for custom JSON formatting.\"\"\"\n    if isinstance(o, (list, dict)):\n        items = []\n        if isinstance(o, dict):\n            for key, value in o.items():\n                formatted_value = json.dumps(value, separators=(\",\", \": \")).replace('\"', \"'\")\n                items.append(f\"'{key}': {formatted_value}\")\n        else:  # For lists\n            items = [json.dumps(item, separators=(\",\", \": \")).replace('\"', \"'\") for item in o]\n        yield from self._encode_line(items, isinstance(o, dict))\n    else:\n        yield from super().iterencode(o, _one_shot)\n</code></pre>"},{"location":"reference/unibox/nb_helpers/uni_peeker/#unibox.nb_helpers.uni_peeker.UniPeeker","title":"UniPeeker","text":"<pre><code>UniPeeker(n: int = 3, console_print: bool = False)\n</code></pre> <p>Utility class for peeking into data with efficient methods.</p> <p>Methods:</p> <ul> <li> <code>peeks</code>             \u2013              <p>Peek into the data and return metadata and a preview of the data, with efficient handling for large data.</p> </li> </ul> Source code in <code>src/unibox/nb_helpers/uni_peeker.py</code> <pre><code>def __init__(self, n: int = 3, console_print: bool = False):\n    self.n = n\n    self.console_print = console_print\n</code></pre>"},{"location":"reference/unibox/nb_helpers/uni_peeker/#unibox.nb_helpers.uni_peeker.UniPeeker.peeks","title":"peeks","text":"<pre><code>peeks(\n    data: Any, n: int = None, console_print: bool = None\n) -&gt; dict | None\n</code></pre> <p>Peek into the data and return metadata and a preview of the data, with efficient handling for large data.</p> Source code in <code>src/unibox/nb_helpers/uni_peeker.py</code> <pre><code>def peeks(self, data: Any, n: int = None, console_print: bool = None) -&gt; dict | None:\n    \"\"\"Peek into the data and return metadata and a preview of the data, with efficient handling for large data.\"\"\"\n    peek_n = n if n else self.n\n    _print = console_print if console_print is not None else self.console_print\n\n    data_type = type(data).__name__\n    meta_dict = {}\n    preview = None\n\n    if data_type == \"DataFrame\":  # special handling for dataframes\n        try:\n            from .ipython_utils import peek_df\n\n            return peek_df(data, n=3)\n\n        except ModuleNotFoundError:  # If IPython is not available\n            meta_dict, preview = self._peek_dataframe(data, peek_n)\n            return {\"metadata\": meta_dict, \"preview\": preview}\n\n    elif data_type == \"list\":\n        meta_dict, preview = self._peek_list(data, peek_n)\n    if data_type == \"dict\":\n        meta_dict, preview = self._peek_dict(data, peek_n)\n    elif data_type == \"set\" or data_type == \"tuple\":\n        meta_dict, preview = self._peek_list(list(data), peek_n)\n\n    # Handling for other data types...\n\n    if _print:\n        self._print_info(data_type, meta_dict, preview)\n\n    return {\"metadata\": meta_dict, \"preview\": preview}\n</code></pre>"},{"location":"reference/unibox/utils/","title":"unibox.utils","text":""},{"location":"reference/unibox/utils/#unibox.utils","title":"utils","text":"<p>Modules:</p> <ul> <li> <code>constants</code>           \u2013            <p>commonly used constant variables</p> </li> <li> <code>credentials_manager</code>           \u2013            <p>Utility functions for execlib, including AWS and Hugging Face credential management.</p> </li> <li> <code>df_utils</code>           \u2013            </li> <li> <code>globals</code>           \u2013            <p>create a global temporary directory that is cleaned up when the program exits.</p> </li> <li> <code>image_utils</code>           \u2013            </li> <li> <code>io_utils</code>           \u2013            </li> <li> <code>llm_api</code>           \u2013            </li> <li> <code>logger</code>           \u2013            </li> <li> <code>s3_client</code>           \u2013            </li> <li> <code>utils</code>           \u2013            </li> </ul>"},{"location":"reference/unibox/utils/constants/","title":"unibox.utils.constants","text":""},{"location":"reference/unibox/utils/constants/#unibox.utils.constants","title":"constants","text":"<p>commonly used constant variables</p>"},{"location":"reference/unibox/utils/credentials_manager/","title":"unibox.utils.credentials_manager","text":""},{"location":"reference/unibox/utils/credentials_manager/#unibox.utils.credentials_manager","title":"credentials_manager","text":"<p>Utility functions for execlib, including AWS and Hugging Face credential management.</p> <p>Functions:</p> <ul> <li> <code>apply_credentials</code>             \u2013              <p>Apply credentials for the specified services.</p> </li> </ul>"},{"location":"reference/unibox/utils/credentials_manager/#unibox.utils.credentials_manager.apply_credentials","title":"apply_credentials","text":"<pre><code>apply_credentials(*services: str) -&gt; None\n</code></pre> <p>Apply credentials for the specified services.</p> Source code in <code>src/unibox/utils/credentials_manager.py</code> <pre><code>def apply_credentials(*services: str) -&gt; None:\n    \"\"\"Apply credentials for the specified services.\"\"\"\n    manager = CredentialManager()\n    manager.apply_credentials(*services)\n</code></pre>"},{"location":"reference/unibox/utils/df_utils/","title":"unibox.utils.df_utils","text":""},{"location":"reference/unibox/utils/df_utils/#unibox.utils.df_utils","title":"df_utils","text":"<p>Functions:</p> <ul> <li> <code>column_memory_usage</code>             \u2013              <p>Function to display memory usage of each column</p> </li> <li> <code>convert_object_to_category</code>             \u2013              <p>Convert specified columns in the DataFrame to categorical type.</p> </li> <li> <code>dataframe_to_markdown_fallback</code>             \u2013              <p>Convert DataFrame to markdown without requiring optional tabulate dependency.</p> </li> <li> <code>downcast_numerical_columns</code>             \u2013              <p>Reduce precision of numerical columns using to_numeric with downcast</p> </li> <li> <code>generate_dataset_summary</code>             \u2013              <p>Generate a combined dataset summary markdown text that merges:</p> </li> <li> <code>get_random_df</code>             \u2013              <p>Generate a random DataFrame for testing purposes</p> </li> <li> <code>human_readable_size</code>             \u2013              <p>Convert bytes to human readable size</p> </li> <li> <code>truncate_text</code>             \u2013              <p>Truncate text if it exceeds max_length, adding an ellipsis.</p> </li> </ul>"},{"location":"reference/unibox/utils/df_utils/#unibox.utils.df_utils.column_memory_usage","title":"column_memory_usage","text":"<pre><code>column_memory_usage(dataframe: DataFrame) -&gt; DataFrame\n</code></pre> <p>Function to display memory usage of each column</p> Source code in <code>src/unibox/utils/df_utils.py</code> <pre><code>def column_memory_usage(dataframe: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Function to display memory usage of each column\"\"\"\n    memory_usage = dataframe.memory_usage(deep=True)\n    readable_memory_usage = memory_usage.apply(human_readable_size)\n    dtypes = dataframe.dtypes\n\n    mem_df = pd.DataFrame(\n        {\n            \"Column\": memory_usage.index[1:],  # Exclude Index\n            \"Memory Usage\": memory_usage.values[1:],  # Exclude Index\n            # Exclude Index\n            \"Readable Memory Usage\": readable_memory_usage.values[1:],\n            \"Dtype\": dtypes.values,\n        },\n    )\n    mem_df = mem_df.sort_values(by=\"Memory Usage\", ascending=False)\n    return mem_df\n</code></pre>"},{"location":"reference/unibox/utils/df_utils/#unibox.utils.df_utils.convert_object_to_category","title":"convert_object_to_category","text":"<pre><code>convert_object_to_category(\n    df: DataFrame, category_columns: list[str]\n) -&gt; DataFrame\n</code></pre> <p>Convert specified columns in the DataFrame to categorical type.</p> <p>:param df: DataFrame to convert columns. :param category_columns: List of column names to convert to categorical type. :return: DataFrame with specified columns converted to categorical type.</p> Source code in <code>src/unibox/utils/df_utils.py</code> <pre><code>def convert_object_to_category(df: pd.DataFrame, category_columns: list[str]) -&gt; pd.DataFrame:\n    \"\"\"Convert specified columns in the DataFrame to categorical type.\n\n    :param df: DataFrame to convert columns.\n    :param category_columns: List of column names to convert to categorical type.\n    :return: DataFrame with specified columns converted to categorical type.\n    \"\"\"\n    if not isinstance(category_columns, list):\n        raise ValueError(\"category_columns must be a list of column names\")\n\n    for col in category_columns:\n        if df[col].isna().sum() &gt; 0:  # Check for NaN values\n            df[col] = pd.Categorical(df[col], categories=df[col].dropna().unique())\n        df[col] = df[col].astype(\"category\")\n\n    return df\n</code></pre>"},{"location":"reference/unibox/utils/df_utils/#unibox.utils.df_utils.dataframe_to_markdown_fallback","title":"dataframe_to_markdown_fallback","text":"<pre><code>dataframe_to_markdown_fallback(\n    df: DataFrame, index: bool = False\n) -&gt; str\n</code></pre> <p>Convert DataFrame to markdown without requiring optional tabulate dependency.</p> Source code in <code>src/unibox/utils/df_utils.py</code> <pre><code>def dataframe_to_markdown_fallback(df: pd.DataFrame, index: bool = False) -&gt; str:\n    \"\"\"Convert DataFrame to markdown without requiring optional tabulate dependency.\"\"\"\n    try:\n        return df.to_markdown(index=index)\n    except Exception as e:\n        logger.debug(\"to_markdown failed; falling back to manual markdown: %s\", e)\n\n    table_df = df\n    if index:\n        table_df = df.copy()\n        table_df.insert(0, \"index\", df.index)\n\n    columns = [str(col) for col in table_df.columns]\n    if not columns:\n        return \"\"\n\n    rows = [[_markdown_cell(cell) for cell in row] for row in table_df.itertuples(index=False, name=None)]\n\n    widths = [len(col) for col in columns]\n    for row in rows:\n        for i, cell in enumerate(row):\n            if i &lt; len(widths):\n                widths[i] = max(widths[i], len(cell))\n\n    def format_row(values: list[str]) -&gt; str:\n        padded = [values[i].ljust(widths[i]) for i in range(len(widths))]\n        return \"| \" + \" | \".join(padded) + \" |\"\n\n    lines = [format_row(columns), \"| \" + \" | \".join(\"-\" * w for w in widths) + \" |\"]\n    for row in rows:\n        lines.append(format_row(row))\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"reference/unibox/utils/df_utils/#unibox.utils.df_utils.downcast_numerical_columns","title":"downcast_numerical_columns","text":"<pre><code>downcast_numerical_columns(df: DataFrame)\n</code></pre> <p>Reduce precision of numerical columns using to_numeric with downcast</p> Source code in <code>src/unibox/utils/df_utils.py</code> <pre><code>def downcast_numerical_columns(df: pd.DataFrame):\n    \"\"\"Reduce precision of numerical columns using to_numeric with downcast\"\"\"\n    numerical_columns = df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n    for col in numerical_columns:\n        df[col] = pd.to_numeric(df[col], downcast=\"integer\" if df[col].dtype == \"int64\" else \"float\")\n    return df\n</code></pre>"},{"location":"reference/unibox/utils/df_utils/#unibox.utils.df_utils.generate_dataset_summary","title":"generate_dataset_summary","text":"<pre><code>generate_dataset_summary(\n    df: DataFrame,\n    repo_id: str,\n    sample_rows: int = 3,\n    max_unique_for_freq: int = 20,\n    profile_rows: int = 200000,\n    max_rows_for_deep_memory: int = 1000000,\n    max_rows_for_duplicates: int = 200000,\n) -&gt; str\n</code></pre> <p>Generate a combined dataset summary markdown text that merges: - robust column-wise checks (numeric, datetime, object, bool) and missing values - memory usage, duplicates, and table displays - sample row previews</p> Source code in <code>src/unibox/utils/df_utils.py</code> <pre><code>def generate_dataset_summary(\n    df: pd.DataFrame,\n    repo_id: str,\n    sample_rows: int = 3,\n    max_unique_for_freq: int = 20,\n    profile_rows: int = 200_000,\n    max_rows_for_deep_memory: int = 1_000_000,\n    max_rows_for_duplicates: int = 200_000,\n) -&gt; str:\n    \"\"\"Generate a combined dataset summary markdown text that merges:\n    - robust column-wise checks (numeric, datetime, object, bool) and missing values\n    - memory usage, duplicates, and table displays\n    - sample row previews\n    \"\"\"\n\n    # -------------------------------------------------------------\n    # 1) Helper functions\n    # -------------------------------------------------------------\n    def human_readable_size(size_in_bytes: float) -&gt; str:\n        \"\"\"Convert bytes to a human-readable string.\"\"\"\n        units = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n        i = 0\n        while size_in_bytes &gt; 1024 and i &lt; len(units) - 1:\n            size_in_bytes /= 1024.0\n            i += 1\n        return f\"{size_in_bytes:.2f} {units[i]}\"\n\n    def column_memory_usage(df: pd.DataFrame, deep: bool) -&gt; pd.DataFrame:\n        \"\"\"Compute per-column memory usage. Returns a DataFrame with:\n        Column, Dtype, Memory Usage (bytes), and Readable Memory Usage.\n        \"\"\"\n        try:\n            usage = df.memory_usage(deep=deep, index=False)\n        except Exception:\n            usage = df.memory_usage(deep=False, index=False)\n        dtypes = df.dtypes.astype(str)\n        usage_values = usage.values if hasattr(usage, \"values\") else usage\n        return pd.DataFrame(\n            {\n                \"Column\": df.columns,\n                \"Dtype\": dtypes.values,\n                \"Memory Usage (bytes)\": usage_values,\n                \"Readable Memory Usage\": [human_readable_size(val) for val in usage_values],\n            },\n        )\n\n    def truncate_text(text, max_len=50):\n        \"\"\"Truncate text to a max length for preview.\"\"\"\n        if not isinstance(text, str):\n            text = str(text)\n        return text if len(text) &lt;= max_len else text[: max_len - 3] + \"...\"\n\n    def compute_duplicates_count(df: pd.DataFrame, max_rows: int) -&gt; tuple[Any, str, str]:\n        \"\"\"Compute duplicated row count and duplicate rate (%).\n        Uses a capped sample for large frames to avoid full-table scans.\n        \"\"\"\n        if len(df) == 0:\n            return 0, \"N/A\", \"\"\n\n        if max_rows and len(df) &gt; max_rows:\n            dup_df = df.head(max_rows)\n            note = f\"(sample of {len(dup_df)})\"\n        else:\n            dup_df = df\n            note = \"\"\n\n        try:\n            duplicate_count = dup_df.duplicated().sum()\n        except Exception:\n            try:\n                obj_cols = dup_df.select_dtypes(include=[\"object\", \"category\"]).columns\n                if len(obj_cols) &gt; 0:\n                    safe_df = dup_df.copy()\n                    safe_df[obj_cols] = safe_df[obj_cols].astype(str)\n                else:\n                    safe_df = dup_df\n                duplicate_count = safe_df.duplicated().sum()\n            except Exception:\n                return \"N/A\", \"N/A\", note\n\n        total = len(dup_df)\n        duplicate_rate = f\"{(duplicate_count / total * 100):.2f}%\" if total else \"N/A\"\n        return duplicate_count, duplicate_rate, note\n\n    def build_robust_column_summaries(\n        summary_df: pd.DataFrame,\n        max_unique_for_freq: int = 10,\n        sample_note: str | None = None,\n    ) -&gt; str:\n        lines = []\n        header = \"## Column Summaries:\"\n        if sample_note:\n            header = f\"## Column Summaries ({sample_note}):\"\n        lines.append(header)\n\n        if summary_df.empty:\n            lines.append(\"\\n(No rows available for summary.)\")\n            return \"\\n\".join(lines)\n\n        numeric_cols = [\n            col\n            for col in summary_df.columns\n            if pd.api.types.is_numeric_dtype(summary_df[col]) and not pd.api.types.is_bool_dtype(summary_df[col])\n        ]\n        bool_cols = [col for col in summary_df.columns if pd.api.types.is_bool_dtype(summary_df[col])]\n        datetime_cols = [col for col in summary_df.columns if pd.api.types.is_datetime64_any_dtype(summary_df[col])]\n        numeric_set = set(numeric_cols)\n        bool_set = set(bool_cols)\n        datetime_set = set(datetime_cols)\n\n        numeric_stats = None\n        if numeric_cols:\n            try:\n                numeric_stats = summary_df[numeric_cols].agg([\"min\", \"max\", \"mean\", \"std\"])\n            except Exception:\n                numeric_stats = None\n\n        datetime_stats = None\n        if datetime_cols:\n            try:\n                datetime_stats = summary_df[datetime_cols].agg([\"min\", \"max\"])\n            except Exception:\n                datetime_stats = None\n\n        total_rows = len(summary_df)\n        for col in summary_df.columns:\n            lines.append(f\"\\n\u2192 {col} ({summary_df[col].dtype})\")\n            try:\n                col_data = summary_df[col]\n\n                if col in numeric_set:\n                    if numeric_stats is not None and col in numeric_stats:\n                        desc = numeric_stats[col]\n                    else:\n                        desc = col_data.describe()\n                    parts = []\n                    for key in [\"min\", \"max\", \"mean\", \"std\"]:\n                        if key in desc and pd.notna(desc[key]):\n                            val = desc[key]\n                            parts.append(\n                                f\"{key.capitalize()}: {val:.3f}\"\n                                if isinstance(val, float)\n                                else f\"{key.capitalize()}: {val}\",\n                            )\n                    lines.append(\"  - \" + \", \".join(parts) if parts else \"  - No valid numeric summary available\")\n\n                elif col in bool_set:\n                    total = total_rows\n                    true_count = (col_data == True).sum()\n                    false_count = (col_data == False).sum()\n                    na_count = col_data.isna().sum()\n                    if total &gt; 0:\n                        lines.append(\n                            f\"  - True: {true_count} ({true_count / total:.2%}), False: {false_count} ({false_count / total:.2%})\",\n                        )\n                    else:\n                        lines.append(\"  - True: 0 (N/A), False: 0 (N/A)\")\n                    if na_count &gt; 0:\n                        lines.append(f\"  - Missing: {na_count} ({na_count / total:.2%})\")\n\n                elif col in datetime_set:\n                    if datetime_stats is not None and col in datetime_stats:\n                        min_date = datetime_stats[col].get(\"min\")\n                        max_date = datetime_stats[col].get(\"max\")\n                    else:\n                        min_date = col_data.min()\n                        max_date = col_data.max()\n                    if pd.notna(min_date) and pd.notna(max_date):\n                        lines.append(f\"  - Range: {min_date} \u2192 {max_date}\")\n                    else:\n                        lines.append(\"  - Date range unavailable (all NaT?)\")\n\n                else:\n                    non_null = col_data.dropna()\n                    sample_val = non_null.iloc[0] if not non_null.empty else None\n\n                    if isinstance(sample_val, set):\n                        lines.append(\"  - Contains unhashable type: set\")\n                        set_lengths = non_null.map(lambda x: len(x) if isinstance(x, set) else None)\n                        set_lengths = pd.to_numeric(set_lengths, errors=\"coerce\").dropna()\n                        if not set_lengths.empty:\n                            lines.append(\n                                f\"    - Typical set length: mean={set_lengths.mean():.2f}, min={set_lengths.min()}, max={set_lengths.max()}\",\n                            )\n                        else:\n                            lines.append(\"    - Could not determine typical set length\")\n\n                    elif isinstance(sample_val, (list, tuple, np.ndarray, collections.abc.Sequence)) and not isinstance(\n                        sample_val,\n                        str,\n                    ):\n                        val_type = type(sample_val).__name__\n                        lines.append(f\"  - Contains unhashable sequence: {val_type}\")\n                        lengths = non_null.map(lambda x: len(x) if isinstance(x, collections.abc.Sized) else None)\n                        lengths = pd.to_numeric(lengths, errors=\"coerce\").dropna()\n                        if not lengths.empty:\n                            lines.append(\n                                f\"    - Typical length: mean={lengths.mean():.2f}, min={lengths.min()}, max={lengths.max()}\",\n                            )\n                        else:\n                            lines.append(\"    - Could not determine typical length\")\n\n                    else:\n                        col_for_freq = col_data\n                        try:\n                            nunique = col_data.nunique(dropna=True)\n                        except Exception:\n                            try:\n                                col_for_freq = col_data.astype(str)\n                                nunique = col_for_freq.nunique(dropna=True)\n                            except Exception:\n                                lines.append(\"  - Unique values: N/A\")\n                                continue\n\n                        lines.append(f\"  - Unique values: {nunique}\")\n                        if nunique &lt;= max_unique_for_freq:\n                            freqs = col_for_freq.value_counts(dropna=False).head(max_unique_for_freq)\n                            for val, count in freqs.items():\n                                percent = count / total_rows * 100 if total_rows else 0\n                                lines.append(f\"    - {val!r}: {count} ({percent:.2f}%)\")\n\n            except Exception as e:\n                lines.append(f\"  - Summary failed: {type(e).__name__} \u2013 {e!s}\")\n\n        return \"\\n\".join(lines)\n\n    # -------------------------------------------------------------\n    # 3) Compute memory, duplicates, missing info\n    # -------------------------------------------------------------\n    row_count = len(df)\n    col_count = len(df.columns)\n\n    if profile_rows is None or profile_rows &lt;= 0:\n        profile_rows = row_count\n\n    if row_count &gt; profile_rows:\n        profile_df = df.head(profile_rows)\n        profile_note = f\"first {len(profile_df)} rows\"\n    else:\n        profile_df = df\n        profile_note = \"\"\n\n    object_cols_count = df.select_dtypes(include=[\"object\"]).shape[1]\n    use_deep_memory = object_cols_count &gt; 0 and row_count &lt;= max_rows_for_deep_memory\n    mem_df = column_memory_usage(df, deep=use_deep_memory)\n    total_mem = mem_df[\"Memory Usage (bytes)\"].sum()\n    total_mem_readable = human_readable_size(total_mem)\n    memory_note = \"\"\n    if not use_deep_memory and object_cols_count &gt; 0:\n        memory_note = \"Memory usage excludes deep object sizes for performance.\"\n\n    # Missing stats\n    missing_df = profile_df if profile_note else df\n    if len(missing_df) &gt; 0:\n        missing_count = missing_df.isnull().sum()\n        missing_rate = (missing_count / len(missing_df) * 100).round(2).astype(str) + \"%\"\n    else:\n        missing_count = pd.Series([0] * col_count, index=df.columns)\n        missing_rate = pd.Series([\"N/A\"] * col_count, index=df.columns)\n\n    missing_count_label = \"Missing Count\"\n    missing_rate_label = \"Missing Rate\"\n    if profile_note:\n        missing_count_label = f\"Missing Count (sample {len(missing_df)})\"\n        missing_rate_label = \"Missing Rate (sample)\"\n\n    # Duplicate stats\n    duplicate_count, duplicate_rate, duplicate_note = compute_duplicates_count(df, max_rows_for_duplicates)\n\n    # Combine memory, dtype, missing info into a single DataFrame\n    stats_df = pd.DataFrame(\n        {\n            \"Column\": df.columns,\n            \"Dtype\": mem_df[\"Dtype\"],\n            \"Memory Usage\": mem_df[\"Readable Memory Usage\"],\n            missing_count_label: missing_count.values,\n            missing_rate_label: missing_rate.values,\n        },\n    )\n\n    # Add a total row at the bottom\n    denom = len(missing_df) * col_count if col_count else 0\n    total_missing_rate = f\"{(missing_count.sum() / denom * 100):.2f}%\" if denom else \"N/A\"\n    total_row = pd.DataFrame(\n        [\n            {\n                \"Column\": \"\",\n                \"Dtype\": \"\",\n                \"Memory Usage\": \"\",\n                missing_count_label: \"\",\n                missing_rate_label: \"\",\n            },\n            {\n                \"Column\": \"**TOTAL**\",\n                \"Dtype\": \"N/A\",\n                \"Memory Usage\": total_mem_readable,\n                missing_count_label: missing_count.sum(),\n                missing_rate_label: total_missing_rate,\n            },\n        ],\n    )\n\n    stats_df = pd.concat([stats_df, total_row], ignore_index=True)\n\n    # Safely convert stats_df to a markdown table\n    try:\n        stats_table = dataframe_to_markdown_fallback(stats_df, index=False)\n    except Exception:\n        stats_table = \"Error generating stats table.\"\n\n    # -------------------------------------------------------------\n    # 4) Build column summaries\n    # -------------------------------------------------------------\n    try:\n        column_summaries_text = build_robust_column_summaries(\n            profile_df,\n            max_unique_for_freq=max_unique_for_freq,\n            sample_note=profile_note or None,\n        )\n    except Exception as e:\n        column_summaries_text = f\"Error generating column summaries: {e}\"\n\n    # -------------------------------------------------------------\n    # 5) Build a safe sample preview\n    # -------------------------------------------------------------\n    preview_df = df.head(sample_rows).copy()\n\n    # Truncate long object columns for preview\n    for col in preview_df.select_dtypes(include=[\"object\"]):\n        try:\n            preview_df[col] = preview_df[col].apply(\n                lambda x: truncate_text(x) if x is not None else None,\n            )\n        except Exception:\n            # If something fails, coerce to string and then truncate\n            preview_df[col] = (\n                preview_df[col]\n                .astype(str)\n                .apply(\n                    lambda x: truncate_text(x) if x else None,\n                )\n            )\n\n    # Identify columns that can be rendered\n    renderable_cols = []\n    for col in preview_df.columns:\n        try:\n            # Test if we can astype str on the first few rows\n            preview_df[col].head(sample_rows).astype(str)\n            renderable_cols.append(col)\n        except Exception:\n            pass\n\n    if len(renderable_cols) == 0:\n        sample_table = f\"No columns available for preview (data shape: {df.shape}).\"\n    else:\n        try:\n            sample_table = dataframe_to_markdown_fallback(\n                preview_df[renderable_cols].head(sample_rows),\n                index=False,\n            )\n        except Exception:\n            # Fallback: try .to_string if to_markdown fails\n            try:\n                sample_table = preview_df[renderable_cols].head(sample_rows).to_string(index=False)\n            except Exception as e:\n                sample_table = f\"Preview table failed. Error: {e}\"\n\n    # -------------------------------------------------------------\n    # 6) Construct final markdown output\n    # -------------------------------------------------------------\n    notes = []\n    if profile_note:\n        notes.append(f\"Profiling based on {profile_note}.\")\n    if memory_note:\n        notes.append(memory_note)\n    notes_text = f\"- Notes: {' '.join(notes)}\\n\" if notes else \"\"\n\n    readme_text = f\"\"\"# {repo_id}\n(Auto-generated summary)\n\n## Basic Info:\n- Shape: **{df.shape[0]}** rows \u00d7 **{df.shape[1]}** columns\n- Total Memory Usage: {total_mem_readable}\n- Duplicates: {duplicate_count} ({duplicate_rate}) {duplicate_note}\n{notes_text}\n\n## Column Stats:\n\n{stats_table}\n\n{column_summaries_text}\n\n## Sample Rows (first {sample_rows}):\n\n```\n{sample_table}\n```\n\n## Usage Example:\n\n```python\nimport unibox as ub\ndf = ub.loads(\"hf://{repo_id}\").to_pandas()\n```\n\n## Saving to dataset:\n\n```python\nimport unibox as ub\nub.saves(df, \"hf://{repo_id}\")\n```\n\n(last updated: {pd.Timestamp.now()})\n\"\"\"\n    return readme_text\n</code></pre>"},{"location":"reference/unibox/utils/df_utils/#unibox.utils.df_utils.get_random_df","title":"get_random_df","text":"<pre><code>get_random_df() -&gt; DataFrame\n</code></pre> <p>Generate a random DataFrame for testing purposes</p> Source code in <code>src/unibox/utils/df_utils.py</code> <pre><code>def get_random_df() -&gt; pd.DataFrame:\n    \"\"\"Generate a random DataFrame for testing purposes\"\"\"\n    import random\n    from datetime import datetime\n\n    df = pd.DataFrame(\n        {\n            \"name\": [f\"Name_{i}\" for i in range(1000)],\n            \"age\": [random.randint(18, 80) for _ in range(1000)],\n            \"height\": [random.randint(150, 200) for _ in range(1000)],\n            \"weight\": [random.randint(50, 150) for _ in range(1000)],\n            \"date\": [datetime.now() for _ in range(1000)],\n            \"is_student\": [random.choice([True, False]) for _ in range(1000)],\n        },\n    )\n    return df\n</code></pre>"},{"location":"reference/unibox/utils/df_utils/#unibox.utils.df_utils.human_readable_size","title":"human_readable_size","text":"<pre><code>human_readable_size(size_bytes)\n</code></pre> <p>Convert bytes to human readable size</p> Source code in <code>src/unibox/utils/df_utils.py</code> <pre><code>def human_readable_size(size_bytes):\n    \"\"\"Convert bytes to human readable size\"\"\"\n    for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\"]:\n        if size_bytes &lt; 1024:\n            return f\"{size_bytes:.2f} {unit}\"\n        size_bytes /= 1024\n</code></pre>"},{"location":"reference/unibox/utils/df_utils/#unibox.utils.df_utils.truncate_text","title":"truncate_text","text":"<pre><code>truncate_text(text, max_length=200)\n</code></pre> <p>Truncate text if it exceeds max_length, adding an ellipsis.</p> Source code in <code>src/unibox/utils/df_utils.py</code> <pre><code>def truncate_text(text, max_length=200):\n    \"\"\"Truncate text if it exceeds max_length, adding an ellipsis.\"\"\"\n    if isinstance(text, str):\n        if len(text) &gt; max_length:\n            return text[:max_length] + \"...\"\n        return text.replace(\"\\n\", \" \")  # Remove excessive newlines\n    return text\n</code></pre>"},{"location":"reference/unibox/utils/globals/","title":"unibox.utils.globals","text":""},{"location":"reference/unibox/utils/globals/#unibox.utils.globals","title":"globals","text":"<p>create a global temporary directory that is cleaned up when the program exits.</p> <p>When accessing files that's not local, they have to be downloaded first;</p> <p>having a global temporary dir ensures that the files can be properly cleaned up when the program exits.</p>"},{"location":"reference/unibox/utils/image_utils/","title":"unibox.utils.image_utils","text":""},{"location":"reference/unibox/utils/image_utils/#unibox.utils.image_utils","title":"image_utils","text":"<p>Functions:</p> <ul> <li> <code>add_annotation</code>             \u2013              <p>Adds a single annotation string as text to the top, left, right, or bottom of the image.</p> </li> <li> <code>add_annotations</code>             \u2013              <p>Adds multiple annotation strings as text to the top, left, right, or bottom of the image.</p> </li> <li> <code>concatenate_images_horizontally</code>             \u2013              <p>Concatenates a list of PIL.Image objects horizontally, ensuring no image exceeds a specified maximum height.</p> </li> </ul>"},{"location":"reference/unibox/utils/image_utils/#unibox.utils.image_utils.add_annotation","title":"add_annotation","text":"<pre><code>add_annotation(\n    pil_image: Image,\n    annotation: str,\n    position: str = \"top\",\n    alignment: str = \"center\",\n    size: str = \"default\",\n)\n</code></pre> <p>Adds a single annotation string as text to the top, left, right, or bottom of the image. Adjusts font size automatically according to actual image size to make it easy to look.</p> <p>Parameters: pil_image (Image.Image): The image to annotate. annotation (str): The annotation text. position (str): The position where the annotation should be added ('top', 'left', 'right', 'bottom'). alignment (str): The alignment of the text ('center', 'left', 'right'). 'left' and 'right' are only valid for top and bottom positions. text_size (str): The size of the text ('larger', 'default', 'smaller', 'smallest', 'largest').</p> Source code in <code>src/unibox/utils/image_utils.py</code> <pre><code>def add_annotation(\n    pil_image: Image.Image, annotation: str, position: str = \"top\", alignment: str = \"center\", size: str = \"default\"\n):\n    \"\"\"Adds a single annotation string as text to the top, left, right, or bottom of the image.\n    Adjusts font size automatically according to actual image size to make it easy to look.\n\n    Parameters:\n    pil_image (Image.Image): The image to annotate.\n    annotation (str): The annotation text.\n    position (str): The position where the annotation should be added ('top', 'left', 'right', 'bottom').\n    alignment (str): The alignment of the text ('center', 'left', 'right'). 'left' and 'right' are only valid for top and bottom positions.\n    text_size (str): The size of the text ('larger', 'default', 'smaller', 'smallest', 'largest').\n    \"\"\"\n    font = load_font()\n    img_width, img_height = pil_image.size\n\n    # Calculate the maximum width and height for the text\n    max_width = img_width if position in [\"top\", \"bottom\"] else img_width // 5\n    max_height = img_height // 10 if position in [\"top\", \"bottom\"] else img_height\n\n    font = get_font_size(annotation, max_width, max_height, font, size)\n    text_width, text_height = textsize(annotation, font)\n\n    padding = 10  # Add padding around the text\n    text_x, text_y = calculate_text_position(\n        img_width, img_height, text_width, text_height, position, alignment, padding\n    )\n\n    new_image = create_new_image_with_space(pil_image, max_width, max_height, padding, position, text_height)\n    draw = ImageDraw.Draw(new_image)\n    draw.text((text_x, text_y), annotation, fill=\"black\", font=font)\n\n    return new_image\n</code></pre>"},{"location":"reference/unibox/utils/image_utils/#unibox.utils.image_utils.add_annotations","title":"add_annotations","text":"<pre><code>add_annotations(\n    pil_image: Image,\n    annotations: list,\n    position: str = \"top\",\n)\n</code></pre> <p>Adds multiple annotation strings as text to the top, left, right, or bottom of the image. Adjusts font size automatically according to actual image size to make it easy to look.</p> <p>Parameters: pil_image (Image.Image): The image to annotate. annotations (list): List of tuples containing the annotation text, alignment, and text size. position (str): The position where the annotation should be added ('top', 'left', 'right', 'bottom').</p> Source code in <code>src/unibox/utils/image_utils.py</code> <pre><code>def add_annotations(pil_image: Image.Image, annotations: list, position: str = \"top\"):\n    \"\"\"Adds multiple annotation strings as text to the top, left, right, or bottom of the image.\n    Adjusts font size automatically according to actual image size to make it easy to look.\n\n    Parameters:\n    pil_image (Image.Image): The image to annotate.\n    annotations (list): List of tuples containing the annotation text, alignment, and text size.\n    position (str): The position where the annotation should be added ('top', 'left', 'right', 'bottom').\n    \"\"\"\n    font = load_font()\n    img_width, img_height = pil_image.size\n\n    # Calculate the maximum width and height for the text\n    max_width = img_width if position in [\"top\", \"bottom\"] else img_width // 5\n    max_height = img_height // 10 if position in [\"top\", \"bottom\"] else img_height\n\n    padding = 10  # Add padding around the text\n\n    # Calculate the maximum height of all annotations to align them properly\n    max_text_height = 0\n    for annotation, _, size in annotations:\n        font = get_font_size(annotation, max_width, max_height, font, size)\n        _, text_height = textsize(annotation, font)\n        max_text_height = max(max_text_height, text_height)\n\n    new_image = create_new_image_with_space(pil_image, max_width, max_height, padding, position, max_text_height)\n    draw = ImageDraw.Draw(new_image)\n\n    for annotation, align, size in annotations:\n        font = get_font_size(annotation, max_width, max_height, font, size)\n        text_width, text_height = textsize(annotation, font)\n        text_x, text_y = calculate_text_position(\n            img_width, img_height, text_width, max_text_height, position, align, padding\n        )\n        draw.text((text_x, text_y), annotation, fill=\"black\", font=font)\n\n    return new_image\n</code></pre>"},{"location":"reference/unibox/utils/image_utils/#unibox.utils.image_utils.concatenate_images_horizontally","title":"concatenate_images_horizontally","text":"<pre><code>concatenate_images_horizontally(\n    images, max_height=1024\n) -&gt; Image\n</code></pre> <p>Concatenates a list of PIL.Image objects horizontally, ensuring no image exceeds a specified maximum height.</p> <p>:param images: List of PIL.Image objects. :param max_height: Maximum height for any image in the list. Images taller than this will be resized proportionally. :return: A single PIL.Image object resulting from the horizontal concatenation.</p> Source code in <code>src/unibox/utils/image_utils.py</code> <pre><code>def concatenate_images_horizontally(images, max_height=1024) -&gt; Image.Image:\n    \"\"\"Concatenates a list of PIL.Image objects horizontally, ensuring no image exceeds a specified maximum height.\n\n    :param images: List of PIL.Image objects.\n    :param max_height: Maximum height for any image in the list. Images taller than this will be resized proportionally.\n    :return: A single PIL.Image object resulting from the horizontal concatenation.\n    \"\"\"\n    if not images:\n        print(\"No images to concatenate.\")\n        return None\n\n    resized_images = []\n\n    # Resize images if necessary to ensure no image exceeds the max height\n    for image in images:\n        if image.height &gt; max_height:\n            aspect_ratio = image.width / image.height\n            new_width = int(aspect_ratio * max_height)\n            resized_image = image.resize((new_width, max_height))\n            resized_images.append(resized_image)\n        else:\n            resized_images.append(image)\n\n    # Determine the total width and the maximum height of resized images\n    total_width = sum(image.width for image in resized_images)\n    max_height = max(image.height for image in resized_images)\n\n    # Create a new image with the appropriate dimensions\n    concatenated_image = Image.new(\"RGB\", (total_width, max_height))\n\n    # Paste each resized image into the new image\n    x_offset = 0\n    for image in resized_images:\n        concatenated_image.paste(image, (x_offset, 0))\n        x_offset += image.width\n\n    return concatenated_image\n</code></pre>"},{"location":"reference/unibox/utils/io_utils/","title":"unibox.utils.io_utils","text":""},{"location":"reference/unibox/utils/io_utils/#unibox.utils.io_utils","title":"io_utils","text":"<p>Functions:</p> <ul> <li> <code>fast_copy_files</code>             \u2013              <p>Copy many files into a single directory efficiently.</p> </li> </ul>"},{"location":"reference/unibox/utils/io_utils/#unibox.utils.io_utils.fast_copy_files","title":"fast_copy_files","text":"<pre><code>fast_copy_files(paths, target_dir, workers=None)\n</code></pre> <p>Copy many files into a single directory efficiently.</p>"},{"location":"reference/unibox/utils/io_utils/#unibox.utils.io_utils.fast_copy_files--parameters","title":"Parameters","text":"<p>paths : list[str | Path]     List of file paths to copy. target_dir : str | Path     Destination directory. workers : int | None     Number of worker threads. Default uses min(32, cpu_count*2).</p> Source code in <code>src/unibox/utils/io_utils.py</code> <pre><code>def fast_copy_files(paths, target_dir, workers=None):\n    \"\"\"\n    Copy many files into a single directory efficiently.\n\n    Parameters\n    ----------\n    paths : list[str | Path]\n        List of file paths to copy.\n    target_dir : str | Path\n        Destination directory.\n    workers : int | None\n        Number of worker threads. Default uses min(32, cpu_count*2).\n    \"\"\"\n\n    target_dir = Path(target_dir)\n    target_dir.mkdir(parents=True, exist_ok=True)\n\n    # Choose a good default worker count for I/O bound work\n    if workers is None:\n        workers = min(32, (os.cpu_count() or 4) * 2)\n\n    def _copy_one(src):\n        src = Path(src)\n        dst = target_dir / src.name\n        shutil.copy2(src, dst)   # Uses kernel copy when possible\n        return src\n\n    with ThreadPoolExecutor(max_workers=workers) as pool:\n        futures = [pool.submit(_copy_one, p) for p in paths]\n\n        # Force completion &amp; surface exceptions early\n        for f in as_completed(futures):\n            f.result()\n</code></pre>"},{"location":"reference/unibox/utils/llm_api/","title":"unibox.utils.llm_api","text":""},{"location":"reference/unibox/utils/llm_api/#unibox.utils.llm_api","title":"llm_api","text":"<p>Functions:</p> <ul> <li> <code>generate_gemini</code>             \u2013              <p>Generates content using Google's Generative Language API</p> </li> <li> <code>generate_openai</code>             \u2013              <p>Generates content using OpenAI's API (Chat Completions format)</p> </li> </ul>"},{"location":"reference/unibox/utils/llm_api/#unibox.utils.llm_api.generate_gemini","title":"generate_gemini","text":"<pre><code>generate_gemini(prompt, api_key, model='gemini-2.5-flash')\n</code></pre> <p>Generates content using Google's Generative Language API</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>           \u2013            <p>Input text/prompt for generation</p> </li> <li> <code>api_key</code>           \u2013            <p>Your API key</p> </li> <li> <code>model</code>           \u2013            <p>Model name (default: gemini-1.5-flash)</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Generated text as a string</p> </li> </ul> Source code in <code>src/unibox/utils/llm_api.py</code> <pre><code>def generate_gemini(prompt, api_key, model=\"gemini-2.5-flash\"):\n    \"\"\"Generates content using Google's Generative Language API\n\n    Args:\n        prompt: Input text/prompt for generation\n        api_key: Your API key\n        model: Model name (default: gemini-1.5-flash)\n\n    Returns:\n        Generated text as a string\n    \"\"\"\n    url = f\"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent\"\n    headers = {\"Content-Type\": \"application/json\"}\n    params = {\"key\": api_key}\n\n    data = {\n        \"contents\": [\n            {\n                \"parts\": [{\"text\": prompt}],\n            },\n        ],\n    }\n\n    response = requests.post(url, json=data, headers=headers, params=params)\n\n    # Check for HTTP errors\n    if response.status_code != 200:\n        raise Exception(f\"API request failed: {response.status_code} - {response.text}\")\n\n    try:\n        return response.json()[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n    except (KeyError, IndexError) as e:\n        raise ValueError(\"Unexpected API response format\") from e\n</code></pre>"},{"location":"reference/unibox/utils/llm_api/#unibox.utils.llm_api.generate_openai","title":"generate_openai","text":"<pre><code>generate_openai(\n    prompt,\n    api_key,\n    model=\"chatgpt-4o-latest\",\n    endpoint=\"https://api.openai.com/v1/chat/completions\",\n    system_message=\"You are a helpful assistant.\",\n)\n</code></pre> <p>Generates content using OpenAI's API (Chat Completions format)</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>           \u2013            <p>Input text/prompt for generation</p> </li> <li> <code>api_key</code>           \u2013            <p>Your API key</p> </li> <li> <code>model</code>           \u2013            <p>Model name (default: gpt-4o)</p> </li> <li> <code>endpoint</code>           \u2013            <p>API endpoint (default: OpenAI's chat completion endpoint)</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>Generated text as a string</p> </li> </ul> Source code in <code>src/unibox/utils/llm_api.py</code> <pre><code>def generate_openai(\n    prompt,\n    api_key,\n    model=\"chatgpt-4o-latest\",\n    endpoint=\"https://api.openai.com/v1/chat/completions\",\n    system_message=\"You are a helpful assistant.\",\n):\n    \"\"\"Generates content using OpenAI's API (Chat Completions format)\n\n    Args:\n        prompt: Input text/prompt for generation\n        api_key: Your API key\n        model: Model name (default: gpt-4o)\n        endpoint: API endpoint (default: OpenAI's chat completion endpoint)\n\n    Returns:\n        Generated text as a string\n    \"\"\"\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"Authorization\": f\"Bearer {api_key}\",\n    }\n    data = {\n        \"model\": model,\n        \"messages\": [\n            {\"role\": \"system\", \"content\": system_message},\n            {\"role\": \"user\", \"content\": prompt},\n        ],\n    }\n    response = requests.post(endpoint, headers=headers, data=json.dumps(data))\n    if response.status_code == 200:\n        return response.json().get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n    return f\"Error: {response.status_code} - {response.text}\"\n</code></pre>"},{"location":"reference/unibox/utils/logger/","title":"unibox.utils.logger","text":""},{"location":"reference/unibox/utils/logger/#unibox.utils.logger","title":"logger","text":"<p>Classes:</p> <ul> <li> <code>UniLogger</code>           \u2013            <p>A logger that:</p> </li> </ul>"},{"location":"reference/unibox/utils/logger/#unibox.utils.logger.UniLogger","title":"UniLogger","text":"<pre><code>UniLogger(\n    output_dir: str = \"logs\",\n    file_suffix: str = \"log\",\n    verbose: bool = False,\n    logger_name: str = None,\n    write_log: bool = True,\n    shorten_levels: int = 2,\n    use_color: bool = True,\n)\n</code></pre> <p>A logger that: 1) Uses colorlog for console color. 2) Writes an optional log file without color. 3) Shows the caller's class and method. 4) Conditionally includes file paths for specific log levels. 5) Detects if console supports color and allows disabling colors.</p> <p>Methods:</p> <ul> <li> <code>log</code>             \u2013              <p>Log with custom formatting based on log level.</p> </li> </ul> Source code in <code>src/unibox/utils/logger.py</code> <pre><code>def __init__(\n    self,\n    output_dir: str = \"logs\",\n    file_suffix: str = \"log\",\n    verbose: bool = False,\n    logger_name: str = None,\n    write_log: bool = True,\n    shorten_levels: int = 2,  # how many path parts to show for debug logs\n    use_color: bool = True,  # manually enable/disable color\n):\n    self.verbose = verbose\n    self.write_log = write_log\n    self.shorten_levels = shorten_levels\n\n    # Determine if console supports color\n    self.supports_color = self._detect_color_support() if use_color else False\n\n    self.logger = logging.getLogger(logger_name if logger_name else self.__class__.__name__)\n    self.logger.setLevel(logging.DEBUG if verbose else logging.INFO)\n\n    # Prepare handlers\n    self.handlers = []\n\n    # Optional file handler\n    if self.write_log:\n        # Use resolved log directory if default output_dir is used\n        if output_dir == \"logs\":\n            log_dir = _resolve_log_dir()\n        else:\n            log_dir = output_dir\n\n        os.makedirs(log_dir, exist_ok=True)\n        log_file = os.path.join(log_dir, f\"{file_suffix}_{datetime.now().strftime('%Y%m%d')}.log\")\n        fh = logging.FileHandler(log_file, mode=\"a\", encoding=\"utf-8\")\n        self.handlers.append(fh)\n\n    # Console handler\n    ch = logging.StreamHandler(sys.stdout)\n    self.handlers.append(ch)\n\n    # Add handlers to logger\n    if not self.logger.hasHandlers():\n        for h in self.handlers:\n            self.logger.addHandler(h)\n\n    self._setup_formatters()\n</code></pre>"},{"location":"reference/unibox/utils/logger/#unibox.utils.logger.UniLogger.log","title":"log","text":"<pre><code>log(level_name: str, message: str)\n</code></pre> <p>Log with custom formatting based on log level.</p> Source code in <code>src/unibox/utils/logger.py</code> <pre><code>def log(self, level_name: str, message: str):\n    \"\"\"Log with custom formatting based on log level.\"\"\"\n    level = getattr(logging, level_name.upper(), logging.INFO)\n\n    # Skip frames to find real caller\n    caller_frame = inspect.currentframe().f_back.f_back\n    method_name = caller_frame.f_code.co_name\n    class_name = None\n\n    if \"self\" in caller_frame.f_locals:\n        class_name = caller_frame.f_locals[\"self\"].__class__.__name__\n\n    if class_name:\n        full_func_name = f\"{class_name}.{method_name}\"\n    else:\n        full_func_name = method_name\n\n    full_path = os.path.abspath(caller_frame.f_code.co_filename)\n    short_path = self._shorten_path(full_path, self.shorten_levels)\n    lineno = caller_frame.f_lineno\n\n    # Conditionally include the path for debug, warning, error, critical levels\n    if level in [logging.DEBUG, logging.WARNING, logging.ERROR, logging.CRITICAL]:\n        extra_path = f\" {short_path}:{lineno}\"\n    else:\n        extra_path = \"\"\n\n    # Provide custom fields in `extra`\n    extra = {\n        \"my_func\": full_func_name,\n        \"my_lineno\": lineno,\n        \"extra_path\": extra_path,\n    }\n\n    # Log using extra fields\n    self.logger.log(level, message, extra=extra)\n</code></pre>"},{"location":"reference/unibox/utils/s3_client/","title":"unibox.utils.s3_client","text":""},{"location":"reference/unibox/utils/s3_client/#unibox.utils.s3_client","title":"s3_client","text":"<p>Classes:</p> <ul> <li> <code>S3Client</code>           \u2013            </li> </ul>"},{"location":"reference/unibox/utils/s3_client/#unibox.utils.s3_client.S3Client","title":"S3Client","text":"<pre><code>S3Client()\n</code></pre> <p>Methods:</p> <ul> <li> <code>download</code>             \u2013              <p>Download a file from S3 to a local directory.</p> </li> <li> <code>exists</code>             \u2013              <p>Check if a file exists in S3 at the given URI.</p> </li> <li> <code>generate_presigned_uri</code>             \u2013              <p>Generate a presigned URL from a given S3 URI with a default expiration of 7 days.</p> </li> <li> <code>traverse</code>             \u2013              <p>Traverse through an S3 \"directory\" and return entries under it.</p> </li> <li> <code>upload</code>             \u2013              <p>Upload a local file to S3.</p> </li> <li> <code>walk</code>             \u2013              <p>Generator that walks all objects under the given S3 URI.</p> </li> </ul> Source code in <code>src/unibox/utils/s3_client.py</code> <pre><code>def __init__(self) -&gt; None:\n    import boto3\n\n    # Simple S3 client init; if you need custom credentials or region,\n    # pass them directly via environment variables or create a custom session.\n    session = boto3.Session()\n    self.s3 = session.client(\"s3\")\n</code></pre>"},{"location":"reference/unibox/utils/s3_client/#unibox.utils.s3_client.S3Client.download","title":"download","text":"<pre><code>download(s3_uri: str, target_dir: str | Path) -&gt; str\n</code></pre> <p>Download a file from S3 to a local directory. :param s3_uri: S3 URI (e.g. s3://bucket/key) :param target_dir: Local directory path :return: Local file path</p> Source code in <code>src/unibox/utils/s3_client.py</code> <pre><code>def download(self, s3_uri: str, target_dir: str | Path) -&gt; str:\n    \"\"\"Download a file from S3 to a local directory.\n    :param s3_uri: S3 URI (e.g. s3://bucket/key)\n    :param target_dir: Local directory path\n    :return: Local file path\n    \"\"\"\n    bucket, key = parse_s3_url(s3_uri)\n    filename = os.path.basename(s3_uri)\n    path = os.path.join(target_dir, filename)\n    self.s3.download_file(bucket, key, path)\n    return path\n</code></pre>"},{"location":"reference/unibox/utils/s3_client/#unibox.utils.s3_client.S3Client.exists","title":"exists","text":"<pre><code>exists(s3_uri: str) -&gt; bool\n</code></pre> <p>Check if a file exists in S3 at the given URI. :param s3_uri: S3 URI :return: True if object exists, False otherwise.</p> Source code in <code>src/unibox/utils/s3_client.py</code> <pre><code>def exists(self, s3_uri: str) -&gt; bool:\n    \"\"\"Check if a file exists in S3 at the given URI.\n    :param s3_uri: S3 URI\n    :return: True if object exists, False otherwise.\n    \"\"\"\n    bucket, key = parse_s3_url(s3_uri)\n    try:\n        self.s3.head_object(Bucket=bucket, Key=key)\n        return True\n    except self.s3.exceptions.ClientError:\n        return False\n</code></pre>"},{"location":"reference/unibox/utils/s3_client/#unibox.utils.s3_client.S3Client.generate_presigned_uri","title":"generate_presigned_uri","text":"<pre><code>generate_presigned_uri(\n    s3_uri: str, expiration: int = 604800\n) -&gt; str\n</code></pre> <p>Generate a presigned URL from a given S3 URI with a default expiration of 7 days.</p> <p>:param s3_uri: S3 URI (e.g., 's3://bucket-name/object-key') :param expiration: Time in seconds for the presigned URL to remain valid (default 7 days). :return: Presigned URL as a string. If error, returns None.</p> Source code in <code>src/unibox/utils/s3_client.py</code> <pre><code>def generate_presigned_uri(self, s3_uri: str, expiration: int = 604800) -&gt; str:\n    \"\"\"Generate a presigned URL from a given S3 URI with a default expiration of 7 days.\n\n    :param s3_uri: S3 URI (e.g., 's3://bucket-name/object-key')\n    :param expiration: Time in seconds for the presigned URL to remain valid (default 7 days).\n    :return: Presigned URL as a string. If error, returns None.\n    \"\"\"\n    bucket, key = parse_s3_url(s3_uri)\n\n    # Constrain expiration to AWS max if needed.\n    expiration = min(expiration, 604800)\n\n    try:\n        response = self.s3.generate_presigned_url(\n            \"get_object\",\n            Params={\"Bucket\": bucket, \"Key\": key},\n            ExpiresIn=expiration,\n        )\n        return response\n    except ClientError as e:\n        logging.exception(f\"Failed to generate presigned URL for {s3_uri}: {e}\")\n        return None\n</code></pre>"},{"location":"reference/unibox/utils/s3_client/#unibox.utils.s3_client.S3Client.traverse","title":"traverse","text":"<pre><code>traverse(\n    s3_uri: str,\n    include_extensions=None,\n    exclude_extensions=None,\n    relative_unix=False,\n    debug_print=True,\n)\n</code></pre> <p>Traverse through an S3 \"directory\" and return entries under it.</p> <p>:param include_extensions: list of file extensions to include (e.g. ['.jpg', '.png']). :param exclude_extensions: list of file extensions to exclude (e.g. ['.txt', '.json']). :param relative_unix: return relative paths or full s3:// URIs. :param debug_print: whether to show a tqdm progress bar. :return: list of keys or URIs.</p> Source code in <code>src/unibox/utils/s3_client.py</code> <pre><code>def traverse(\n    self,\n    s3_uri: str,\n    include_extensions=None,\n    exclude_extensions=None,\n    relative_unix=False,\n    debug_print=True,\n):\n    \"\"\"Traverse through an S3 \"directory\" and return entries under it.\n\n    :param include_extensions: list of file extensions to include (e.g. ['.jpg', '.png']).\n    :param exclude_extensions: list of file extensions to exclude (e.g. ['.txt', '.json']).\n    :param relative_unix: return relative paths or full s3:// URIs.\n    :param debug_print: whether to show a tqdm progress bar.\n    :return: list of keys or URIs.\n    \"\"\"\n    bucket, prefix = parse_s3_url(s3_uri)\n\n    if not prefix.endswith(\"/\"):\n        prefix += \"/\"\n\n    paginator = self.s3.get_paginator(\"list_objects_v2\")\n    response_iterator = paginator.paginate(Bucket=bucket, Prefix=prefix, Delimiter=\"/\")\n\n    all_entries = []\n\n    if debug_print:\n        response_iterator = tqdm(response_iterator, desc=\"Traversing S3\", unit=\"page\")\n\n    for page in response_iterator:\n        # Subdirectories\n        for d in page.get(\"CommonPrefixes\", []):\n            dir_key = d[\"Prefix\"]\n            dir_entry = dir_key if relative_unix else f\"s3://{bucket}/{dir_key}\"\n            all_entries.append(dir_entry)\n\n        # Files\n        for obj in page.get(\"Contents\", []):\n            file_key = obj[\"Key\"]\n            if file_key == prefix:\n                continue  # skip the directory itself\n\n            # Check include/exclude\n            if (include_extensions is None or any(file_key.endswith(ext) for ext in include_extensions)) and (\n                exclude_extensions is None or not any(file_key.endswith(ext) for ext in exclude_extensions)\n            ):\n                file_entry = file_key[len(prefix) :] if relative_unix else f\"s3://{bucket}/{file_key}\"\n                all_entries.append(file_entry)\n\n    return all_entries\n</code></pre>"},{"location":"reference/unibox/utils/s3_client/#unibox.utils.s3_client.S3Client.upload","title":"upload","text":"<pre><code>upload(file_path: str, s3_uri: str) -&gt; None\n</code></pre> <p>Upload a local file to S3. :param file_path: Local file path :param s3_uri: S3 URI (e.g. s3://bucket/key)</p> Source code in <code>src/unibox/utils/s3_client.py</code> <pre><code>def upload(self, file_path: str, s3_uri: str) -&gt; None:\n    \"\"\"Upload a local file to S3.\n    :param file_path: Local file path\n    :param s3_uri: S3 URI (e.g. s3://bucket/key)\n    \"\"\"\n    bucket, key = parse_s3_url(s3_uri)\n    self.s3.upload_file(file_path, bucket, key)\n</code></pre>"},{"location":"reference/unibox/utils/s3_client/#unibox.utils.s3_client.S3Client.walk","title":"walk","text":"<pre><code>walk(s3_uri: str)\n</code></pre> <p>Generator that walks all objects under the given S3 URI. Yields metadata dictionaries for each object.</p> Source code in <code>src/unibox/utils/s3_client.py</code> <pre><code>def walk(self, s3_uri: str):\n    \"\"\"Generator that walks all objects under the given S3 URI.\n    Yields metadata dictionaries for each object.\n    \"\"\"\n    bucket, key = parse_s3_url(s3_uri)\n    paginator = self.s3.get_paginator(\"list_objects_v2\")\n    pages = paginator.paginate(Bucket=bucket, Prefix=key)\n    for page in pages:\n        for obj in page[\"Contents\"]:\n            yield {\n                \"key\": obj[\"Key\"],\n                \"size\": obj[\"Size\"],\n                \"last_modified\": obj[\"LastModified\"],\n                \"etag\": obj[\"ETag\"],\n                \"storage_class\": obj[\"StorageClass\"],\n            }\n</code></pre>"},{"location":"reference/unibox/utils/utils/","title":"unibox.utils.utils","text":""},{"location":"reference/unibox/utils/utils/#unibox.utils.utils","title":"utils","text":"<p>Functions:</p> <ul> <li> <code>is_hf_uri</code>             \u2013              <p>Check if the URI is a Hugging Face URI.</p> </li> <li> <code>is_s3_uri</code>             \u2013              <p>Check if the URI is an S3 URI.</p> </li> <li> <code>merge_dicts</code>             \u2013              <p>Merge dictionaries, raising warnings for overlapping keys and data type mismatches.</p> </li> <li> <code>parse_hf_uri</code>             \u2013              <p>Parse a Hugging Face URI into repo_id/path/revision/repo_type.</p> </li> </ul>"},{"location":"reference/unibox/utils/utils/#unibox.utils.utils.is_hf_uri","title":"is_hf_uri","text":"<pre><code>is_hf_uri(uri: str) -&gt; bool\n</code></pre> <p>Check if the URI is a Hugging Face URI.</p> Source code in <code>src/unibox/utils/utils.py</code> <pre><code>def is_hf_uri(uri: str) -&gt; bool:\n    \"\"\"Check if the URI is a Hugging Face URI.\"\"\"\n    return uri.startswith(\"hf://\")\n</code></pre>"},{"location":"reference/unibox/utils/utils/#unibox.utils.utils.is_s3_uri","title":"is_s3_uri","text":"<pre><code>is_s3_uri(uri: str) -&gt; bool\n</code></pre> <p>Check if the URI is an S3 URI.</p> Source code in <code>src/unibox/utils/utils.py</code> <pre><code>def is_s3_uri(uri: str) -&gt; bool:\n    \"\"\"Check if the URI is an S3 URI.\"\"\"\n    parsed = urlparse(uri)\n    return parsed.scheme == \"s3\"\n</code></pre>"},{"location":"reference/unibox/utils/utils/#unibox.utils.utils.merge_dicts","title":"merge_dicts","text":"<pre><code>merge_dicts(*dicts)\n</code></pre> <p>Merge dictionaries, raising warnings for overlapping keys and data type mismatches.</p> <p>Parameters:</p> <ul> <li> <code>*dicts</code>           \u2013            <p>Dictionaries to merge.</p> </li> <li> <code>logger</code>           \u2013            <p>Logger instance for warnings (default: None).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code>          \u2013            <p>Merged dictionary.</p> </li> </ul> Source code in <code>src/unibox/utils/utils.py</code> <pre><code>def merge_dicts(*dicts):\n    \"\"\"Merge dictionaries, raising warnings for overlapping keys and data type mismatches.\n\n    Args:\n        *dicts: Dictionaries to merge.\n        logger: Logger instance for warnings (default: None).\n\n    Returns:\n        dict: Merged dictionary.\n    \"\"\"\n    assert all(isinstance(d, dict) for d in dicts), \"All inputs must be dictionaries.\"\n\n    result = {}\n    for d in dicts:\n        for key, value in d.items():\n            if key in result:\n                logger.warning(f\"Overlapping key '{key}' detected. Existing value: {result[key]}, New value: {value}\")\n                if type(result[key]) != type(value):\n                    logger.warning(\n                        f\"Data type mismatch for key '{key}': {type(result[key])} vs {type(value)}.\",\n                    )\n            result[key] = value\n\n    return result\n</code></pre>"},{"location":"reference/unibox/utils/utils/#unibox.utils.utils.parse_hf_uri","title":"parse_hf_uri","text":"<pre><code>parse_hf_uri(uri: str) -&gt; HfUriParts\n</code></pre> <p>Parse a Hugging Face URI into repo_id/path/revision/repo_type.</p> <p>Parameters:</p> <ul> <li> <code>uri</code>               (<code>str</code>)           \u2013            <p>A URI in format 'hf://owner/repo/path/to/nested/file.ext'  or 'hf://owner/repo' for a dataset</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>HfUriParts</code> (              <code>HfUriParts</code> )          \u2013            <p>Parsed components where: - repo_id is 'owner/repo' - path_in_repo is the remaining path after repo (may contain multiple '/') - revision is optional (from hf://owner/repo@revision) - repo_type defaults to \"model\" unless prefixed (e.g., hf://datasets/owner/repo)</p> </li> </ul> Source code in <code>src/unibox/utils/utils.py</code> <pre><code>def parse_hf_uri(uri: str) -&gt; HfUriParts:\n    \"\"\"Parse a Hugging Face URI into repo_id/path/revision/repo_type.\n\n    Args:\n        uri: A URI in format 'hf://owner/repo/path/to/nested/file.ext'\n             or 'hf://owner/repo' for a dataset\n\n    Returns:\n        HfUriParts: Parsed components where:\n            - repo_id is 'owner/repo'\n            - path_in_repo is the remaining path after repo (may contain multiple '/')\n            - revision is optional (from hf://owner/repo@revision)\n            - repo_type defaults to \"model\" unless prefixed (e.g., hf://datasets/owner/repo)\n    \"\"\"\n    if not uri.startswith(\"hf://\"):\n        raise ValueError(\"URI must start with 'hf://'\")\n\n    # Remove prefix and normalize slashes; handles both hf://owner/repo and hf:///owner/repo\n    trimmed = uri[len(\"hf://\") :].strip(\"/\")\n    if not trimmed:\n        raise ValueError(f\"Invalid Hugging Face URI format: {uri}. Must contain at least owner/repo.\")\n\n    raw_parts = trimmed.split(\"/\")\n    repo_type = \"model\"\n    if raw_parts[0] in HF_REPO_TYPE_PREFIXES:\n        repo_type = HF_REPO_TYPE_PREFIXES[raw_parts[0]]\n        raw_parts = raw_parts[1:]\n\n    if len(raw_parts) &lt; 2:\n        raise ValueError(f\"Invalid Hugging Face URI format: {uri}. Must contain at least owner/repo.\")\n\n    owner = raw_parts[0]\n    repo_and_revision = raw_parts[1]\n    path_in_repo = \"/\".join(raw_parts[2:]) if len(raw_parts) &gt; 2 else \"\"\n\n    revision = None\n    if \"@\" in repo_and_revision:\n        repo_name, revision = repo_and_revision.split(\"@\", 1)\n        if not repo_name:\n            raise ValueError(f\"Invalid Hugging Face URI format: {uri}. Repo name is missing.\")\n        if revision == \"\":\n            revision = None\n    else:\n        repo_name = repo_and_revision\n\n    if not repo_name:\n        raise ValueError(f\"Invalid Hugging Face URI format: {uri}. Repo name is missing.\")\n\n    repo_id = f\"{owner}/{repo_name}\"\n    return HfUriParts(repo_id=repo_id, path_in_repo=path_in_repo, revision=revision, repo_type=repo_type)\n</code></pre>"},{"location":"coverage/","title":"Coverage report","text":""}]}